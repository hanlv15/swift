{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-06 14:55:45,936 - modelscope - INFO - PyTorch version 2.1.2 Found.\n",
      "2024-04-06 14:55:45,939 - modelscope - INFO - Loading ast index from /home/hanlv/.cache/modelscope/ast_indexer\n",
      "2024-04-06 14:55:45,974 - modelscope - INFO - Loading done! Current index file version is 1.13.1, with md5 ad87f4a589251d86333765b92ab59e41 and a total number of 972 components indexed\n",
      "[INFO:swift] Loading the model using model_dir: /home/css/models/Mixtral-8x7B-Instruct-v0.1-GPTQ-int4\n",
      "[INFO:swift] Setting torch_dtype: torch.bfloat16\n",
      "[INFO:swift] model_config: MixtralConfig {\n",
      "  \"_name_or_path\": \"/home/css/models/Mixtral-8x7B-Instruct-v0.1-GPTQ-int4\",\n",
      "  \"architectures\": [\n",
      "    \"MixtralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mixtral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_experts_per_tok\": 2,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_local_experts\": 8,\n",
      "  \"output_router_logits\": false,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"bits\": 4,\n",
      "    \"damp_percent\": 0.1,\n",
      "    \"desc_act\": true,\n",
      "    \"group_size\": 32,\n",
      "    \"model_file_base_name\": \"model\",\n",
      "    \"model_name_or_path\": null,\n",
      "    \"modules_in_block_to_quantize\": [\n",
      "      [\n",
      "        \"self_attn.k_proj\",\n",
      "        \"self_attn.v_proj\",\n",
      "        \"self_attn.q_proj\"\n",
      "      ],\n",
      "      [\n",
      "        \"self_attn.o_proj\"\n",
      "      ],\n",
      "      [\n",
      "        \"block_sparse_moe.experts.0.w1\",\n",
      "        \"block_sparse_moe.experts.0.w2\",\n",
      "        \"block_sparse_moe.experts.0.w3\"\n",
      "      ],\n",
      "      [\n",
      "        \"block_sparse_moe.experts.1.w1\",\n",
      "        \"block_sparse_moe.experts.1.w2\",\n",
      "        \"block_sparse_moe.experts.1.w3\"\n",
      "      ],\n",
      "      [\n",
      "        \"block_sparse_moe.experts.2.w1\",\n",
      "        \"block_sparse_moe.experts.2.w2\",\n",
      "        \"block_sparse_moe.experts.2.w3\"\n",
      "      ],\n",
      "      [\n",
      "        \"block_sparse_moe.experts.3.w1\",\n",
      "        \"block_sparse_moe.experts.3.w2\",\n",
      "        \"block_sparse_moe.experts.3.w3\"\n",
      "      ],\n",
      "      [\n",
      "        \"block_sparse_moe.experts.4.w1\",\n",
      "        \"block_sparse_moe.experts.4.w2\",\n",
      "        \"block_sparse_moe.experts.4.w3\"\n",
      "      ],\n",
      "      [\n",
      "        \"block_sparse_moe.experts.5.w1\",\n",
      "        \"block_sparse_moe.experts.5.w2\",\n",
      "        \"block_sparse_moe.experts.5.w3\"\n",
      "      ],\n",
      "      [\n",
      "        \"block_sparse_moe.experts.6.w1\",\n",
      "        \"block_sparse_moe.experts.6.w2\",\n",
      "        \"block_sparse_moe.experts.6.w3\"\n",
      "      ],\n",
      "      [\n",
      "        \"block_sparse_moe.experts.7.w1\",\n",
      "        \"block_sparse_moe.experts.7.w2\",\n",
      "        \"block_sparse_moe.experts.7.w3\"\n",
      "      ]\n",
      "    ],\n",
      "    \"quant_method\": \"gptq\",\n",
      "    \"sym\": true,\n",
      "    \"true_sequential\": true\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"router_aux_loss_coef\": 0.02,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "from swift.llm import get_model_tokenizer\n",
    "dirs = [\"/home/hanlv/workspace/code/research/infodemic/LLM/swift/examples/pytorch/llm\"]\n",
    "for _dir in dirs:\n",
    "    if _dir not in sys.path:\n",
    "        sys.path.append(_dir)\n",
    "from custom import CustomModelType, CustomTemplateType\n",
    "model_type = CustomModelType.mixtral_moe_7b_instruct_gptq_int4\n",
    "model, tokenizer = get_model_tokenizer(\n",
    "    model_type, \n",
    "    load_model=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d55777431f43b983eca02cc7d2e0d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14384 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jsonlines\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "sort = False\n",
    "prior_knowledge_list = []\n",
    "\n",
    "K = 5\n",
    "prior_knowledge_version = \"1\"\n",
    "model_name = \"mixtral\"\n",
    "\n",
    "\n",
    "with open(f\"train_search_llm_tmp.json\", \"r\") as f:\n",
    "    prior_knowledge_list = json.load(f)\n",
    "\n",
    "\n",
    "lens1 = []\n",
    "lens2 = []\n",
    "\n",
    "# for item in tqdm(prior_knowledge_list):\n",
    "#     pk=item[f\"prior_knowledge_{model_name}\"]\n",
    "#     lens1.append(len(tokenizer(pk)[\"input_ids\"]))\n",
    "\n",
    "for item in tqdm(prior_knowledge_list):\n",
    "    if item.get(f\"prior_knowledge_{model_name}3\") is None:\n",
    "        lens1.append(1000)\n",
    "        continue\n",
    "\n",
    "    pk=item[f\"prior_knowledge_{model_name}3\"]\n",
    "    lens1.append(len(tokenizer(pk)[\"input_ids\"]))\n",
    "    # lens2.append(len(tokenizer.tokenize(item[\"query\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[205, 207, 221, 224, 230, 234, 239, 255, 276, 279]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 300以下用llama template\n",
    "\n",
    "sorted(lens1, reverse=False)[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "修改较长的句子\n",
    "\n",
    "显卡 24GB\n",
    "\n",
    "7B模型，长度控制在3600以下 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9076]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del_ids = []\n",
    "for i, item in enumerate(lens1):\n",
    "    if item == 207:\n",
    "        del_ids.append(i)\n",
    "\n",
    "del_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Information'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(len(tokenizer(dict_list[4157][\"query\"])[\"input_ids\"]))\n",
    "prior_knowledge_list[7299][f\"prior_knowledge_{model_name}\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Summary:\n",
      "The three pieces of information provided all discuss a photo that is being shared on social media, claiming to show a protest in Berlin against coronavirus restrictions. However, the photo is actually of a music festival in Zurich, Switzerland that took place in 2019. The police estimate that 17,000 people attended the protest in Berlin on August 1, while the music festival in Zurich attracted 850,000 people. The mislabeled photo has been shared on social media platforms such as Facebook and Twitter.\n",
      "\n",
      "Claim Verification:\n",
      "The claim that the photo shows crowds protesting in Berlin against coronavirus restrictions is false, based on the information provided. The photo actually shows a music festival in Zurich, Switzerland that took place in 2019. This has been confirmed by multiple sources, including fact-checking organizations and the Zurich police. Therefore, the claim that the photo shows a protest in Berlin is not accurate.\n"
     ]
    }
   ],
   "source": [
    "print(prior_knowledge_list[9076][f\"prior_knowledge_{model_name}3\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
