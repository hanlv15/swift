{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "import sys\n",
    "import prompt_rag\n",
    "\n",
    "label_convert_liar2 = {'pants-fire': 0, 'false': 1, 'barely-true': 2, 'half-true': 3, 'mostly-true': 4, 'true':5}\n",
    "\n",
    "dirs = [\"..\"]\n",
    "for _dir in dirs:\n",
    "    if _dir not in sys.path:\n",
    "        sys.path.append(_dir)\n",
    "\n",
    "import covmis, liar2\n",
    "search_engine = \"brave\"\n",
    "\n",
    "dataset = \"liar2\" # liar2, covmis\n",
    "data_type = \"train\" # train, test, valid\n",
    "\n",
    "if dataset == \"covmis\":\n",
    "    data = covmis.load_train()\n",
    "    data_search =  covmis.load_train_search(search_engine=search_engine)\n",
    "    data_search_llm = covmis.load_train_llm(search_engine=search_engine)\n",
    "    claim_key = 'claim'\n",
    "    LABEL_TRUE = 2\n",
    "    LABEL_FALSE = 0\n",
    "\n",
    "    save_search = lambda data: covmis.save_train_search(\n",
    "        data, search_engine=search_engine)\n",
    "    save_search_llm = lambda data: covmis.save_train_llm(\n",
    "        data, search_engine=search_engine)\n",
    "elif dataset == \"liar2\":\n",
    "    data = liar2.load_data(data_type)\n",
    "    data_search = liar2.load_data_search(data_type, search_engine)\n",
    "    data_search_llm = liar2.load_data_llm(data_type, search_engine)\n",
    "    claim_key = 'statement'\n",
    "    LABEL_TRUE = label_convert_liar2['true']\n",
    "    LABEL_FALSE = label_convert_liar2['false']\n",
    "\n",
    "    save_search = lambda data: liar2.save_data_search(\n",
    "        data, data_type, search_engine)\n",
    "    save_search_llm = lambda data: liar2.save_data_llm(\n",
    "        data, data_type, search_engine)\n",
    "else:\n",
    "    raise Exception(\"数据集错误\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合并多个文件的先验知识"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 13847,\n",
       " 'prior_knowledge_llama3_v1_K=5': 'Here is a clear summary of the given INFORMATION:\\n\\nThe INFORMATION provided consists of five sources discussing the level of support among Americans for universal background checks on gun purchases. The sources include statements from Senator Chris Murphy, a poll from MPR News, and fact-checking articles from PolitiFact, Snopes, and Warriors coach Steve Kerr.\\n\\nThe main points from the INFORMATION are:\\n\\n* Senator Chris Murphy stated that 90% of Americans support universal background checks, citing a data point that 89% of Republicans and 89% of gun owners also support this measure.\\n* A poll from MPR News found that 80% of Republicans and 90% of Democrats support universal background checks, with 54% of Republicans also supporting gun licensing.\\n* PolitiFact and Snopes fact-checked similar claims and found them to be TRUE, citing multiple polls that consistently show majority support for universal background checks, often above 80% and sometimes above 90%.\\n* Experts suggest that the wording of questions in national surveys can vary, but the overall trend of strong support for universal background checks remains consistent.\\n\\nBased on this INFORMATION, there is reasonable evidence to judge the correctness of the <CLAIM> as TRUE. The <CLAIM> states that 90% of Americans support universal background checks for gun purchases, which is consistent with the findings from multiple polls and fact-checking articles. While the exact percentage of support may vary slightly depending on the poll or survey, the overall trend of strong majority support for universal background checks is clear.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bing search llm\n",
    "# v2: k = 5\n",
    "# v3: 不对时间排序\n",
    "\n",
    "# brave search llm\n",
    "\n",
    "# Solar\n",
    "# v1: k = 5\n",
    "# v2: k = 5 随机选取\n",
    "\n",
    "# Mixtral\n",
    "# v1: k = 5\n",
    "\n",
    "\n",
    "# llama3\n",
    "# v1: k = 5, vllm = 0.5.0\n",
    "\n",
    "sort = False\n",
    "prior_knowledge_list = []\n",
    "\n",
    "K = 5\n",
    "prior_knowledge_version = \"1\"\n",
    "model_name = \"llama3\"\n",
    "\n",
    "\n",
    "# with open(f\"data_search_llm_tmp.json\", \"r\") as f:\n",
    "#     prior_knowledge_list = json.load(f)\n",
    "\n",
    "# for i, item in enumerate(data_search_llm):\n",
    "\n",
    "#     if item[\"id\"] != prior_knowledge_list[i][\"id\"]:\n",
    "#         print(i)\n",
    "#         print(data[i][claim_key])\n",
    "#         raise Exception()\n",
    "#     else:\n",
    "#         # vv = \"\"\n",
    "#         # if prior_knowledge_list[i].get(f\"prior_knowledge_{model_name}3\") is not None:\n",
    "#         #     vv = \"3\"\n",
    "#         # elif prior_knowledge_list[i].get(f\"prior_knowledge_{model_name}2\") is not None:\n",
    "#         #     vv = \"2\"\n",
    "        \n",
    "#         item[f\"prior_knowledge_{model_name}_v{prior_knowledge_version}_K={K}\"] = prior_knowledge_list[i][f\"prior_knowledge_{model_name}\"]\n",
    "\n",
    "# data_search_llm[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_search_llm(data_search_llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建数据（带有先验知识的Prompt）以微调LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a CLAIM and the PRIOR KNOWLEDGE associated with it. Please classify the CLAIM as TRUE or FALSE based on the PRIOR KNOWLEDGE. If the content described by the CLAIM is correct, then classify it as TRUE; if the content described by the CLAIM is incorrect, then classify it as FALSE.\n",
      "\n",
      "CLAIM: 90 percent of Americans \"support universal background checks\" for gun purchases.\n",
      "Publication date: October 2, 2017\n",
      "\n",
      "PRIOR KNOWLEDGE:\n",
      "Information 1:\n",
      "Publication date: None\n",
      "Title: Murphy: We Know Background Checks Work and 90 Percent of Americans Support Them | U.S. Senator Chris Murphy of Connecticut\n",
      "Content:\n",
      "And for most of the people who are talking to me who aren't mentally ill and don't have criminal histories, that's their only experience – is that the background check is not a barrier to purchasing a gun. “And so it's just not surprising to me to hear the data that Senator Warnock is talking about. 90 percent of Americans supporting universal background checks, checks on every gun sale.\n",
      "He failed a background check when he tried to purchase a gun in 2014. But he went to a private seller online. He bought a gun, and then he used it to kill seven people and wound 25 others in a mass shooting in Odessa. This is not theoretical. This happens. How do you think all these guns get into our cities? It's because the criminal traffickers who have serious criminal records who can’t buy guns at a brick-and-mortar store go to a state that doesn't have universal checks.\n",
      "He failed a background check when he tried to purchase a gun in 2014. But he went to a private seller online. He bought a gun, and then he used it to kill seven people and wound 25 others in a mass shooting in Odessa. “This is not theoretical. This happens. How do you think all these guns get into our cities? It's because the criminal traffickers who have serious criminal records who can’t buy guns at a brick-and-mortar store go to a state that doesn't have universal checks.\n",
      "Murphy highlighted the popularity of background checks among Americans: “90 percent of Americans supporting universal background checks, checks on every gun sale. 89 percent of Republicans, 89 percent of gun owners, 70 percent of the NRA members. Because even the gun owners, even the people who feel so fired up about this issue that they want to come talk to me in the middle of a county fair, we're not disagreeing about that simple policy: just make sure that people who shouldn't have guns don't get their hands on them.”\n",
      "Information 2:\n",
      "Publication date: 2023-07-25\n",
      "Title: Poll: A majority of Americans support universal background checks, gun licensing and an assault weapons ban | MPR News\n",
      "Content:\n",
      "Support for universal background checks and gun licensing is bipartisan. Over 90 percent of Democrats support both provisions, as do a majority of Republicans. Eighty percent of Republicans support universal background checks and 54 percent support gun licensing.\n",
      "The other two provisions are split by party: An overwhelming majority of Democrats (90 percent) support a congressional ban on assault weapons, while a majority of Republicans (59 percent) oppose it. On the other hand, a majority of Republicans (54 percent) support eliminating most current gun laws to protect Second Amendment rights, while two-thirds of Democrats are opposed.\n",
      "Eighty percent of Republicans support universal background checks and 54 percent support gun licensing. Gifts from individuals keep MPR News accessible to all - free of paywalls and barriers. ... The other two provisions are split by party: An overwhelming majority of Democrats (90 percent) support a congressional ban on assault weapons, while a majority of Republicans (59 percent) oppose it.\n",
      "It is noteworthy that the three firearm-restricting provisions included in this poll are supported by a majority of those who support eliminating “most current gun laws in order to protect Second Amendment rights.” Among these Second Amendment advocates, 77 percent support universal background checks, 60 percent support a testing, licensing and registering process for guns similar to that of automobiles, and 52 percent support an assault weapons ban.\n",
      "Information 3:\n",
      "Publication date: None\n",
      "Title: Warriors coach Steve Kerr said “90% of Americans, regardless of political party, want universal background checks” for gun purchases. Polls show majority support for background checks, though lower numbers for Republicans.\n",
      "Content:\n",
      "Golden State Warriors Coach Steve Kerr gave an emotional speech hours after the mass shooting at a Texas elementary school, pleading with lawmakers to pass legislation to expand background checks for gun purchases. \"Do you realize that 90% of Americans, regardless of political party, want background checks, universal background checks?\n",
      "PolitiFact, \"Lee Leffingwell says polls show 90 percent of Americans and 74 percent of NRA members support criminal background checks before all gun buys,\" April 4, 2013 · Email interview, Raymond Ridder, Warriors spokesperson, May 25, 2022 · Email interview, Justin McCarthy, Gallup spokesperson, May 25, 2022 · Email interview, Steven S. Smith, professor of social sciences and political science at Washington University, May 25, 2022\n",
      "His father, Malcolm H. Kerr, president of the American University of Beirut, was assassinated by gunmen in 1984. At PolitiFact, we have been fact-checking statements at least since 2013 about majority support for gun background checks, and have found them to be generally accurate, although support is typically lower than 90% among Republicans.\n",
      "Polls consistently show majority support for gun background checks · Polling expert Steven Smith at Washington University in St. Louis told us that \"90% — or nearly 90% — support for background checks is a common finding in nationwide surveys.\" While polls show majority support among Republicans, it is lower than 90%.\n",
      "Information 4:\n",
      "Publication date: None\n",
      "Title: PolitiFact | Do 90% of Americans support background checks for all gun sales?\n",
      "Content:\n",
      "CNN/ORC (June 2016): \"Would you generally favor or oppose a background check on anyone attempting to purchase a gun in order to determine whether the prospective buyer has been convicted of a felony?\" Favor: 92 percent. We’ll also note that in July 2016, U.S. Sen. Chris Murphy, D-Conn., said: \"Ninety percent of Americans want our background check system strengthened and expanded to cover more gun sales.\" PolitiFact National’s rating was True. ... Abele says 90 percent of Americans \"support universal background checks\" for gun purchases.\n",
      "Abele says 90 percent of Americans \"support universal background checks\" for gun purchases. \"Universal\" is the term for background checks to be done on every gun sale. We found support for that policy at 94 percent in the latest national poll. Support ranged between 84 percent and 89 percent in the four other most recent polls.\n",
      "Experts say support at or near 90 percent has been consistent for years. For a statement that is accurate and has nothing significant missing, our rating is True. ... Twitter, Chris Abele tweet, Oct. 2, 2017 · Texts, Chris Abele spokeswoman Melissa Baldauff, Oct. 2, 2017 · PolitiFact National, \"At DNC, Sen. Chris Murphy says 90% of Americans want expanded background checks for gun purchases,\" July 27, 2016 · Quinnipiac University Poll, poll results, June 28, 2017\n",
      "Polling expert Steven Smith, a professor of social sciences and political science at Washington University in St. Louis, told us that the wording of questions in national surveys can vary and usually do not use the term \"universal.\" But well over 80 percent, and sometimes over 90 percent, of respondents favor background checks for all gun sales, he said.\n",
      "Information 5:\n",
      "Publication date: 2022-05-25\n",
      "Title: Do 90% of Americans Want Universal Background Checks for Gun Buys? | Snopes.com\n",
      "Content:\n",
      "As news circulated that 19 children under the age of 10 had been killed in another mass shooting, Steve Kerr, the head coach of the Golden State Warriors basketball team, expressed outrage at the lawmakers who refuse to vote on the federal bill H.R. 8, the \"Bipartisan Background Checks Act of 2021,\" and noted that 90% of Americans support universal background checks for gun purchases. Kerr said: \"Do you realize that 90% of Americans, regardless of political party, want background checks, universal background checks? 90% of us ... We are being held hostage by 50 Senators in Washington who refuse to even put it to a vote, despite what we the American people want. They won't vote on it because they want to hold on to their own power.\" A number of polls taken over the years show that the vast majority of Americans support universal background checks for those wanting to buy guns.\n",
      "Overwhelming majorities of Democrats and Democratic-leaning independents (91%) and Republicans and Republican-leaners (92%) say they strongly or somewhat favor barring people with mental illnesses from purchasing guns. These views are largely unchanged since the question was first asked in 2017. Similarly, large majorities in both parties continue to favor making private gun sales and sales at gun shows subject to background checks (93% of Democrats, 82% of Republicans). Kerr's claim that 90% of Americans support universal background checks falls within the scope of the available data.\n",
      "Some 90% of Americans support universal background checks for gun purchases.\n",
      "In 2015, for example, a national Public Policy Polling survey of gun owners found overwhelming support (83%) for background checks, a 2018 Gallup poll found that 95% of people supported \"requiring background checks for all gun sales,\" and in 2019, a Quinnipiac University poll from 2019 found that 94% of American voters supported universal background checks. The Pew Research Center wrote in 2019: When it comes to specific gun-related policies, the public finds broad agreement in certain areas. Overwhelming majorities of Democrats and Democratic-leaning independents (91%) and Republicans and Republican-leaners (92%) say they strongly or somewhat favor barring people with mental illnesses from purchasing guns.\n",
      "\n",
      "Here is a clear summary of the given INFORMATION:\n",
      "\n",
      "The INFORMATION provided consists of five sources discussing the level of support among Americans for universal background checks on gun purchases. The sources include statements from Senator Chris Murphy, a poll from MPR News, and fact-checking articles from PolitiFact, Snopes, and Warriors coach Steve Kerr.\n",
      "\n",
      "The main points from the INFORMATION are:\n",
      "\n",
      "* Senator Chris Murphy stated that 90% of Americans support universal background checks, citing a data point that 89% of Republicans and 89% of gun owners also support this measure.\n",
      "* A poll from MPR News found that 80% of Republicans and 90% of Democrats support universal background checks, with 54% of Republicans also supporting gun licensing.\n",
      "* PolitiFact and Snopes fact-checked similar claims and found them to be TRUE, citing multiple polls that consistently show majority support for universal background checks, often above 80% and sometimes above 90%.\n",
      "* Experts suggest that the wording of questions in national surveys can vary, but the overall trend of strong support for universal background checks remains consistent.\n",
      "\n",
      "Based on this INFORMATION, there is reasonable evidence to judge the correctness of the <CLAIM> as TRUE. The <CLAIM> states that 90% of Americans support universal background checks for gun purchases, which is consistent with the findings from multiple polls and fact-checking articles. While the exact percentage of support may vary slightly depending on the poll or survey, the overall trend of strong majority support for universal background checks is clear.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7352,\n",
       " [{'query': 'Below is a CLAIM and the PRIOR KNOWLEDGE associated with it. Please classify the CLAIM as TRUE or FALSE based on the PRIOR KNOWLEDGE. If the content described by the CLAIM is correct, then classify it as TRUE; if the content described by the CLAIM is incorrect, then classify it as FALSE.\\n\\nCLAIM: 90 percent of Americans \"support universal background checks\" for gun purchases.\\nPublication date: October 2, 2017\\n\\nPRIOR KNOWLEDGE:\\nInformation 1:\\nPublication date: None\\nTitle: Murphy: We Know Background Checks Work and 90 Percent of Americans Support Them | U.S. Senator Chris Murphy of Connecticut\\nContent:\\nAnd for most of the people who are talking to me who aren\\'t mentally ill and don\\'t have criminal histories, that\\'s their only experience – is that the background check is not a barrier to purchasing a gun. “And so it\\'s just not surprising to me to hear the data that Senator Warnock is talking about. 90 percent of Americans supporting universal background checks, checks on every gun sale.\\nHe failed a background check when he tried to purchase a gun in 2014. But he went to a private seller online. He bought a gun, and then he used it to kill seven people and wound 25 others in a mass shooting in Odessa. This is not theoretical. This happens. How do you think all these guns get into our cities? It\\'s because the criminal traffickers who have serious criminal records who can’t buy guns at a brick-and-mortar store go to a state that doesn\\'t have universal checks.\\nHe failed a background check when he tried to purchase a gun in 2014. But he went to a private seller online. He bought a gun, and then he used it to kill seven people and wound 25 others in a mass shooting in Odessa. “This is not theoretical. This happens. How do you think all these guns get into our cities? It\\'s because the criminal traffickers who have serious criminal records who can’t buy guns at a brick-and-mortar store go to a state that doesn\\'t have universal checks.\\nMurphy highlighted the popularity of background checks among Americans: “90 percent of Americans supporting universal background checks, checks on every gun sale. 89 percent of Republicans, 89 percent of gun owners, 70 percent of the NRA members. Because even the gun owners, even the people who feel so fired up about this issue that they want to come talk to me in the middle of a county fair, we\\'re not disagreeing about that simple policy: just make sure that people who shouldn\\'t have guns don\\'t get their hands on them.”\\nInformation 2:\\nPublication date: 2023-07-25\\nTitle: Poll: A majority of Americans support universal background checks, gun licensing and an assault weapons ban | MPR News\\nContent:\\nSupport for universal background checks and gun licensing is bipartisan. Over 90 percent of Democrats support both provisions, as do a majority of Republicans. Eighty percent of Republicans support universal background checks and 54 percent support gun licensing.\\nThe other two provisions are split by party: An overwhelming majority of Democrats (90 percent) support a congressional ban on assault weapons, while a majority of Republicans (59 percent) oppose it. On the other hand, a majority of Republicans (54 percent) support eliminating most current gun laws to protect Second Amendment rights, while two-thirds of Democrats are opposed.\\nEighty percent of Republicans support universal background checks and 54 percent support gun licensing. Gifts from individuals keep MPR News accessible to all - free of paywalls and barriers. ... The other two provisions are split by party: An overwhelming majority of Democrats (90 percent) support a congressional ban on assault weapons, while a majority of Republicans (59 percent) oppose it.\\nIt is noteworthy that the three firearm-restricting provisions included in this poll are supported by a majority of those who support eliminating “most current gun laws in order to protect Second Amendment rights.” Among these Second Amendment advocates, 77 percent support universal background checks, 60 percent support a testing, licensing and registering process for guns similar to that of automobiles, and 52 percent support an assault weapons ban.\\nInformation 3:\\nPublication date: None\\nTitle: Warriors coach Steve Kerr said “90% of Americans, regardless of political party, want universal background checks” for gun purchases. Polls show majority support for background checks, though lower numbers for Republicans.\\nContent:\\nGolden State Warriors Coach Steve Kerr gave an emotional speech hours after the mass shooting at a Texas elementary school, pleading with lawmakers to pass legislation to expand background checks for gun purchases. \"Do you realize that 90% of Americans, regardless of political party, want background checks, universal background checks?\\nPolitiFact, \"Lee Leffingwell says polls show 90 percent of Americans and 74 percent of NRA members support criminal background checks before all gun buys,\" April 4, 2013 · Email interview, Raymond Ridder, Warriors spokesperson, May 25, 2022 · Email interview, Justin McCarthy, Gallup spokesperson, May 25, 2022 · Email interview, Steven S. Smith, professor of social sciences and political science at Washington University, May 25, 2022\\nHis father, Malcolm H. Kerr, president of the American University of Beirut, was assassinated by gunmen in 1984. At PolitiFact, we have been fact-checking statements at least since 2013 about majority support for gun background checks, and have found them to be generally accurate, although support is typically lower than 90% among Republicans.\\nPolls consistently show majority support for gun background checks · Polling expert Steven Smith at Washington University in St. Louis told us that \"90% — or nearly 90% — support for background checks is a common finding in nationwide surveys.\" While polls show majority support among Republicans, it is lower than 90%.\\nInformation 4:\\nPublication date: None\\nTitle: PolitiFact | Do 90% of Americans support background checks for all gun sales?\\nContent:\\nCNN/ORC (June 2016): \"Would you generally favor or oppose a background check on anyone attempting to purchase a gun in order to determine whether the prospective buyer has been convicted of a felony?\" Favor: 92 percent. We’ll also note that in July 2016, U.S. Sen. Chris Murphy, D-Conn., said: \"Ninety percent of Americans want our background check system strengthened and expanded to cover more gun sales.\" PolitiFact National’s rating was True. ... Abele says 90 percent of Americans \"support universal background checks\" for gun purchases.\\nAbele says 90 percent of Americans \"support universal background checks\" for gun purchases. \"Universal\" is the term for background checks to be done on every gun sale. We found support for that policy at 94 percent in the latest national poll. Support ranged between 84 percent and 89 percent in the four other most recent polls.\\nExperts say support at or near 90 percent has been consistent for years. For a statement that is accurate and has nothing significant missing, our rating is True. ... Twitter, Chris Abele tweet, Oct. 2, 2017 · Texts, Chris Abele spokeswoman Melissa Baldauff, Oct. 2, 2017 · PolitiFact National, \"At DNC, Sen. Chris Murphy says 90% of Americans want expanded background checks for gun purchases,\" July 27, 2016 · Quinnipiac University Poll, poll results, June 28, 2017\\nPolling expert Steven Smith, a professor of social sciences and political science at Washington University in St. Louis, told us that the wording of questions in national surveys can vary and usually do not use the term \"universal.\" But well over 80 percent, and sometimes over 90 percent, of respondents favor background checks for all gun sales, he said.\\nInformation 5:\\nPublication date: 2022-05-25\\nTitle: Do 90% of Americans Want Universal Background Checks for Gun Buys? | Snopes.com\\nContent:\\nAs news circulated that 19 children under the age of 10 had been killed in another mass shooting, Steve Kerr, the head coach of the Golden State Warriors basketball team, expressed outrage at the lawmakers who refuse to vote on the federal bill H.R. 8, the \"Bipartisan Background Checks Act of 2021,\" and noted that 90% of Americans support universal background checks for gun purchases. Kerr said: \"Do you realize that 90% of Americans, regardless of political party, want background checks, universal background checks? 90% of us ... We are being held hostage by 50 Senators in Washington who refuse to even put it to a vote, despite what we the American people want. They won\\'t vote on it because they want to hold on to their own power.\" A number of polls taken over the years show that the vast majority of Americans support universal background checks for those wanting to buy guns.\\nOverwhelming majorities of Democrats and Democratic-leaning independents (91%) and Republicans and Republican-leaners (92%) say they strongly or somewhat favor barring people with mental illnesses from purchasing guns. These views are largely unchanged since the question was first asked in 2017. Similarly, large majorities in both parties continue to favor making private gun sales and sales at gun shows subject to background checks (93% of Democrats, 82% of Republicans). Kerr\\'s claim that 90% of Americans support universal background checks falls within the scope of the available data.\\nSome 90% of Americans support universal background checks for gun purchases.\\nIn 2015, for example, a national Public Policy Polling survey of gun owners found overwhelming support (83%) for background checks, a 2018 Gallup poll found that 95% of people supported \"requiring background checks for all gun sales,\" and in 2019, a Quinnipiac University poll from 2019 found that 94% of American voters supported universal background checks. The Pew Research Center wrote in 2019: When it comes to specific gun-related policies, the public finds broad agreement in certain areas. Overwhelming majorities of Democrats and Democratic-leaning independents (91%) and Republicans and Republican-leaners (92%) say they strongly or somewhat favor barring people with mental illnesses from purchasing guns.\\n\\nHere is a clear summary of the given INFORMATION:\\n\\nThe INFORMATION provided consists of five sources discussing the level of support among Americans for universal background checks on gun purchases. The sources include statements from Senator Chris Murphy, a poll from MPR News, and fact-checking articles from PolitiFact, Snopes, and Warriors coach Steve Kerr.\\n\\nThe main points from the INFORMATION are:\\n\\n* Senator Chris Murphy stated that 90% of Americans support universal background checks, citing a data point that 89% of Republicans and 89% of gun owners also support this measure.\\n* A poll from MPR News found that 80% of Republicans and 90% of Democrats support universal background checks, with 54% of Republicans also supporting gun licensing.\\n* PolitiFact and Snopes fact-checked similar claims and found them to be TRUE, citing multiple polls that consistently show majority support for universal background checks, often above 80% and sometimes above 90%.\\n* Experts suggest that the wording of questions in national surveys can vary, but the overall trend of strong support for universal background checks remains consistent.\\n\\nBased on this INFORMATION, there is reasonable evidence to judge the correctness of the <CLAIM> as TRUE. The <CLAIM> states that 90% of Americans support universal background checks for gun purchases, which is consistent with the findings from multiple polls and fact-checking articles. While the exact percentage of support may vary slightly depending on the poll or survey, the overall trend of strong majority support for universal background checks is clear.',\n",
       "   'response': 'TRUE.'},\n",
       "  {'query': 'Below is a CLAIM and the PRIOR KNOWLEDGE associated with it. Please classify the CLAIM as TRUE or FALSE based on the PRIOR KNOWLEDGE. If the content described by the CLAIM is correct, then classify it as TRUE; if the content described by the CLAIM is incorrect, then classify it as FALSE.\\n\\nCLAIM: Last year was one of the deadliest years ever for law enforcement officers.\\nPublication date: May 19, 2017\\n\\nPRIOR KNOWLEDGE:\\nInformation 1:\\nPublication date: 2022-01-31\\nTitle: 2021 Was Deadliest Year for Law Enforcement Officers in History - National Law Enforcement Officers Memorial Fund\\nContent:\\nNational Law Enforcement Memorial Fund reports fatalities increased 55% in 2021 over 2020, with Covid-19 fatalities the leading cause of officer deaths WASHINGTON, D.C. (January 11, 2022) – The number of law enforcement professionals nationwide who died in the line of duty in 2021 increased 55% over the previous year, according to preliminary data provided […]\\nThere are currently 22,611 names of officers killed in the line of duty inscribed on the National Law Enforcement Officers Memorial in Washington, DC, dating back to the first known death in 1786. The deadliest year on record for law enforcement was 1930 when 312 law enforcement officers were killed in the line of duty.\\nNational Law Enforcement Memorial Fund reports fatalities increased 55% in 2021 over 2020, with Covid-19 fatalities the leading cause of officer deaths · WASHINGTON, D.C. (January 11, 2022) – The number of law enforcement professionals nationwide who died in the line of duty in 2021 increased 55% over the previous year, according to preliminary data provided by the National Law Enforcement Officers Memorial Fund (NLEOMF), the leading authority on officer fatalities.\\nWASHINGTON, D.C. (January 11, 2022) – The number of law enforcement professionals nationwide who died in the line of duty in 2021 increased 55% over the previous year, according to preliminary data provided by the National Law Enforcement Officers Memorial Fund (NLEOMF), the leading authority on officer fatalities. NLEOMF announced in its official 2021 Law Enforcement Officers Fatalities Report that as of December 31, 2021, 458 federal, state, county, municipal, military, campus, tribal, and territorial officers died in the line of duty during the past year, representing a 55% increase over the 295 officers who died in the line of duty in 2020.\\nInformation 2:\\nPublication date: 2024-01-08\\nTitle: 2023 saw record killings by US police. Who is most affected? | US policing | The Guardian\\nContent:\\nPolice in the US killed at least 1,232 people last year, making 2023 the deadliest year for homicides committed by law enforcement in more than a decade, according to newly released data. Mapping Police Violence, a non-profit research group, catalogs deaths at the hands of police and last year recorded the highest number of killings since its national tracking began in 2013. The data suggests a systemic crisis and a remarkably consistent pattern, with an average of roughly three people killed by officers each day, with slight upticks in recent years.\\nAlbuquerque police killed six people in 2023, while many cities with substantially larger populations, including San Jose and Honolulu, each killed only one civilian last year. Some advocates have said gun culture in the state, particularly in rural areas, could be a factor in the high rates of police violence. Federal database will track misconduct records of US law enforcement officers\\nFederal database will track misconduct records of US law enforcement officers ... A spokesperson for the New Mexico governor, Michelle Lujan Grisham, said in an email that she was “committed to promoting professional and constitutional policing”, and noted the governor signed a bill into law last year “aimed at increased accountability for those in this critical profession”. SB19 established a duty to intervene when officers witness certain unlawful uses of force; prohibited neck restraints and firing at fleeing vehicles; and required the establishment of a public police misconduct database.\\nThis contributes to the steady rate of violence, said Joanna Schwartz, University of California, Los Angeles law professor and expert on how officers evade accountability for misconduct: “Even with public attention to police killings in recent years and unprecedented community engagement, it’s really business as usual.\\nInformation 3:\\nPublication date: 2022-01-11\\nTitle: 2021 was deadliest year ever for law enforcement, report says\\nContent:\\nThe state with the highest number of duty deaths was Texas at 84 deaths. Florida had the second-highest at 52 officer deaths, and Georgia was third with 39 deaths. Prior to 2021, the deadliest year on record for law enforcement was 1930 when 312 officers were killed in the line of duty, according to the report.\\nWASHINGTON – Line of duty deaths increased 55% in 2021 from 2020 among law enforcement officers nationwide, according to the 2021 Law Enforcement Officers Fatalities Report from the National Law Enforcement Officers Memorial Fund (NLEOMF). A total of 458 officers died in 2021, according to the preliminary data. That number includes officers at the federal, state, local, military, tribal and territorial levels. The leading cause of death, COVID-19, was responsible for 301 line of duty deaths. The largest category of death was “other” at 338 deaths – a 63% increase in that category from last year.\\nInformation 4:\\nPublication date: 2023-11-30\\nTitle: How many police officers die in the line of duty in the US?\\nContent:\\nIn 2022, 118 officers died in the line of duty, according to the FBI’s Law Enforcement Officers Killed and Assaulted database. Sixty of those officers were killed feloniously, primarily by firearms. However, law enforcement does not rank in the top 10 deadliest professions in the US, according to the Bureau of Labor Statistics (BLS).\\nHowever, the profession is still not considered among the deadliest. Updated on Thu, November 30, 2023 by the USAFacts Team · Home / Crime / Articles / How many police officers die in the line of duty? In 2022, 118 officers died in the line of duty, according to the FBI’s Law Enforcement Officers Killed and Assaulted database.\\nSixty of those officers were killed feloniously, primarily by firearms. However, law enforcement does not rank in the top 10 deadliest professions in the US, according to the Bureau of Labor Statistics (BLS). The FBI collects data from US law enforcement agencies on officer assaults and deaths.\\nIn 2022, felonious killings were the top cause of death for law enforcement officers, accounting for 60 deaths, compared to 58 accidental deaths. This is a decline from 2021, when there were 73 felonious officer deaths. This figure marked a 25-year record high. The majority of killings were committed using a firearm. Six of the felonious killings in 2022 were unprovoked attacks on law enforcement officers.\\nInformation 5:\\nPublication date: 2021-01-12\\nTitle: 2020 was one of the deadliest years for law enforcement officers on record | CNN\\nContent:\\nAs of Monday, the group’s website lists more than 300 officers who died in the line of duty last year, more than 180 of them from Covid-19. “An additional 200+ COVID line of duty deaths are still pending verification, so 2020 may eventually turn out to be the deadliest year for law enforcement in U.S. history due to the COVID pandemic,” the Officer Down Memorial Page wrote in a January 8 Facebook post.\\nAn American flag hangs over the funeral procession of Glen Ridge Police Officer Charles Roberts in New Jersey on May 14, 2020. Roberts was the first officer on the force to die of Covid-19. ... 2020 was one of the deadliest years for law enforcement officers in history, according to a group that tracks officer deaths in the line of duty.\\nRelated article Covid-19 has killed more law enforcement officers this year than all other causes combined · “It’s going to go down in history as one of the deadliest years for law enforcement,” said Marcia Ferranto, CEO of NLEOMF. “Of course, it’s due to Covid.\\n2020 was one of the deadliest years for law enforcement officers in history, according to a group that tracks officer deaths in the line of duty. A report released Monday by the National Law Enforcement Officers Memorial Fund (NLEOMF) found that 264 federal, state, military, tribal and local law enforcement officers to date died in the line of duty last year – the highest since 1974.\\n\\n**Summary of INFORMATION:**\\n\\nThe provided information consists of five articles related to law enforcement officer fatalities in the United States. Here\\'s a brief summary of each article:\\n\\n1. **Information 1 (2022-01-31)**: The National Law Enforcement Officers Memorial Fund reports that 2021 was the deadliest year for law enforcement officers in history, with 458 fatalities, a 55% increase from 2020. Covid-19 was the leading cause of death.\\n2. **Information 2 (2024-01-08)**: According to Mapping Police Violence, 2023 saw a record number of killings by US police, with at least 1,232 people killed. This data suggests a systemic crisis in police violence.\\n3. **Information 3 (2022-01-11)**: The National Law Enforcement Officers Memorial Fund reports that 2021 was the deadliest year ever for law enforcement, with 458 officer deaths, a 55% increase from 2020. Texas, Florida, and Georgia had the highest number of duty deaths.\\n4. **Information 4 (2023-11-30)**: According to the FBI\\'s Law Enforcement Officers Killed and Assaulted database, 118 officers died in the line of duty in 2022, with 60 of those deaths being felonious, primarily by firearms.\\n5. **Information 5 (2021-01-12)**: The Officer Down Memorial Page reports that 2020 was one of the deadliest years for law enforcement officers on record, with over 300 officers dying in the line of duty, more than 180 of them from Covid-19.\\n\\n**Evaluation of the CLAIM:**\\n\\nThe CLAIM states that \"Last year was one of the deadliest years ever for law enforcement officers.\" The publication date of the CLAIM is May 19, 2017. However, the information provided is from 2021, 2022, 2023, and 2024, which is after the CLAIM\\'s publication date.\\n\\nBased on the information provided, it is clear that the CLAIM is TRUE, but only in the context of the years 2020, 2021, and 2023. According to the information, 2020 was one of the deadliest years for law enforcement officers, and 2021 was the deadliest year ever for law enforcement officers. Additionally, 2023 saw a record number of killings by US police.\\n\\nHowever, it is essential to note that the CLAIM\\'s publication date is 2017, and the information provided is from later years. Therefore, the CLAIM\\'s accuracy cannot be determined based on the provided information, as it is out of context.',\n",
       "   'response': 'FALSE.'}])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_list = []\n",
    "\n",
    "data_version = \"1\"\n",
    "\n",
    "if data_version == \"1\":\n",
    "    known_info = True\n",
    "    rag_info = True\n",
    "elif data_version == \"2\":\n",
    "    known_info = False\n",
    "    rag_info = True\n",
    "elif data_version == \"3\":\n",
    "    known_info = True\n",
    "    rag_info = False\n",
    "else:\n",
    "    raise Exception(\"Error data_version!\")\n",
    "\n",
    "\n",
    "# for i, item in enumerate(data_search_llm):\n",
    "#     if data[i][\"id\"] != item[\"id\"]:\n",
    "#         raise Exception(\"data 与 data_search_llm 的 id 不匹配！\")\n",
    "    \n",
    "#     if int(data[i][\"label\"]) in [LABEL_TRUE, LABEL_FALSE]:\n",
    "#         prompt = prompt_rag.get_prompt_with_prior_knowledge(\n",
    "#             data[i][claim_key], \n",
    "#             search_engine,\n",
    "#             data_search[i][f\"{search_engine}_search_results\"], \n",
    "#             item[f\"prior_knowledge_{model_name}_v{prior_knowledge_version}_K={K}\"], \n",
    "#             K=K,\n",
    "#             claim_date=data[i][\"date\"],\n",
    "#             known_info=known_info, \n",
    "#             rag_info=rag_info,\n",
    "#             ids=None\n",
    "#         )\n",
    "#         if int(data[i][\"label\"]) == LABEL_TRUE:\n",
    "#             label = \"TRUE.\"\n",
    "#         elif int(data[i][\"label\"]) == LABEL_FALSE:\n",
    "#             label = \"FALSE.\"\n",
    "#         else:\n",
    "#             error_label = data[i][\"label\"]\n",
    "#             _id = data[i][\"id\"]\n",
    "#             raise Exception(f\"Error label: {error_label}; id: {_id}\")\n",
    "        \n",
    "#         dict_list.append({\"query\": prompt, \"response\": label})\n",
    "# print(dict_list[0][\"query\"])\n",
    "# len(dict_list), dict_list[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"/home/hanlv/workspace/code/research/infodemic/LLM/swift/examples/pytorch/llm/my_data/\" + \\\n",
    "#     f\"{dataset}/with_{model_name}_info/{search_engine}/data{data_version}.jsonl\"\n",
    "\n",
    "# if dataset == \"liar2\":\n",
    "#     data_dir = data_dir.replace(f\"data{data_version}\", f\"{data_type}_data{data_version}\")\n",
    "\n",
    "\n",
    "# with jsonlines.open(data_dir, mode=\"w\") as file_jsonl:\n",
    "#     for line in dict_list:\n",
    "#         file_jsonl.write(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试模型的先验知识生成效果：一次提问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Successfully registered `/home/hanlv/workspace/code/research/infodemic/LLM/swift/swift/llm/data/dataset_info.json`\n",
      "[INFO:swift] Loading the model using model_dir: /home/css/models/llama-3-70b-instruct-awq\n",
      "[INFO:swift] model_config: LlamaConfig {\n",
      "  \"_name_or_path\": \"/home/css/models/llama-3-70b-instruct-awq\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 8192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 28672,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_hidden_layers\": 80,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"bits\": 4,\n",
      "    \"group_size\": 128,\n",
      "    \"modules_to_not_convert\": null,\n",
      "    \"quant_method\": \"awq\",\n",
      "    \"version\": \"gemm\",\n",
      "    \"zero_point\": true\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-17 15:37:21 config.py:244] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 07-17 15:37:21 config.py:698] Defaulting to use mp for distributed inference\n",
      "INFO 07-17 15:37:21 llm_engine.py:169] Initializing an LLM engine (v0.5.1) with config: model='/home/css/models/llama-3-70b-instruct-awq', speculative_config=None, tokenizer='/home/css/models/llama-3-70b-instruct-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=42, served_model_name=/home/css/models/llama-3-70b-instruct-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m INFO 07-17 15:37:24 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 07-17 15:37:24 utils.py:741] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m INFO 07-17 15:37:24 utils.py:741] Found nccl from library libnccl.so.2\n",
      "INFO 07-17 15:37:24 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m INFO 07-17 15:37:24 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method load_model: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU \u0001 has a total capacity of 23.65 GiB of which 725.12 MiB is free. Process 1776804 has 4.78 GiB memory in use. Including non-PyTorch memory, this process has 18.15 GiB memory in use. Of the allocated memory 17.57 GiB is allocated by PyTorch, and 16.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables), Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]   File \"/home/hanlv/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]   File \"/home/hanlv/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/worker/worker.py\", line 133, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]     self.model_runner.load_model()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]   File \"/home/hanlv/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 243, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]     self.model = get_model(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]                  ^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]   File \"/home/hanlv/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py\", line 21, in get_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]     return loader.load_model(model_config=model_config,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]   File \"/home/hanlv/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 267, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]     model = _initialize_model(model_config, self.load_config,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]   File \"/home/hanlv/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 104, in _initialize_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]     return model_class(config=model_config.hf_config,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]   File \"/home/hanlv/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 375, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]     self.lm_head = ParallelLMHead(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]                    ^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]   File \"/home/hanlv/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\", line 386, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]     super().__init__(num_embeddings, embedding_dim, params_dtype,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]   File \"/home/hanlv/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\", line 218, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]     self.linear_method.create_weights(self,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]   File \"/home/hanlv/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 117, in create_weights\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]     weight = Parameter(torch.empty(sum(output_partition_sizes),\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]   File \"/home/hanlv/miniconda3/envs/swift/lib/python3.11/site-packages/torch/utils/_device.py\", line 78, in __torch_function__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226] torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU \u0001 has a total capacity of 23.65 GiB of which 725.12 MiB is free. Process 1776804 has 4.78 GiB memory in use. Including non-PyTorch memory, this process has 18.15 GiB memory in use. Of the allocated memory 17.57 GiB is allocated by PyTorch, and 16.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m model_type \u001b[38;5;241m=\u001b[39m CustomModelType\u001b[38;5;241m.\u001b[39mllama_3_70b_instruct_awq\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# model_type = CustomModelType.mixtral_moe_7b_instruct_awq\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# model_type = CustomModelType.solar_instruct_10_7b\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m llm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mget_vllm_engine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# torch_dtype=torch.float16,  # 检查正确的数据类型！！！！\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# gpu_memory_utilization=0.92,\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# model_id_or_path=\"/home/css/models/Mixtral-8x7B-Instruct-v0.1-GPTQ-int4\",\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# \"enforce_eager\": True,\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_num_seqs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 64\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m template_type \u001b[38;5;241m=\u001b[39m get_default_template_type(model_type)\n\u001b[1;32m     38\u001b[0m template \u001b[38;5;241m=\u001b[39m get_template(template_type, llm_engine\u001b[38;5;241m.\u001b[39mhf_tokenizer)\n",
      "File \u001b[0;32m~/workspace/code/research/infodemic/LLM/swift/swift/llm/utils/vllm_utils.py:121\u001b[0m, in \u001b[0;36mget_vllm_engine\u001b[0;34m(model_type, torch_dtype, model_id_or_path, revision, gpu_memory_utilization, tensor_parallel_size, max_model_len, disable_custom_all_reduce, enforce_eager, engine_kwargs, use_async, enable_lora, max_loras, max_lora_rank, image_input_shape, image_feature_size, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(vllm\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0.5.1\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    119\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVLLM_WORKER_MULTIPROC_METHOD\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 121\u001b[0m llm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mllm_engine_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m llm_engine\u001b[38;5;241m.\u001b[39mengine_args \u001b[38;5;241m=\u001b[39m engine_args\n\u001b[1;32m    123\u001b[0m llm_engine\u001b[38;5;241m.\u001b[39mmodel_dir \u001b[38;5;241m=\u001b[39m model_dir\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/engine/llm_engine.py:414\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context)\u001b[0m\n\u001b[1;32m    411\u001b[0m     executor_class \u001b[38;5;241m=\u001b[39m GPUExecutor\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 414\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/engine/llm_engine.py:243\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, multimodal_config, speculative_config, decoding_config, observability_config, executor_class, log_stats, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config_fields \u001b[38;5;241m=\u001b[39m _load_generation_config_dict(\n\u001b[1;32m    238\u001b[0m     model_config)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_processor \u001b[38;5;241m=\u001b[39m INPUT_REGISTRY\u001b[38;5;241m.\u001b[39mcreate_input_processor(\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config)\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultimodal_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultimodal_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeculative_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeculative_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39membedding_mode:\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_kv_caches()\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/executor/distributed_gpu_executor.py:25\u001b[0m, in \u001b[0;36mDistributedGPUExecutor.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Updated by implementations that require additional args to be passed\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# to the _run_workers execute_model call\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_execute_model_run_workers_kwargs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/executor/executor_base.py:42\u001b[0m, in \u001b[0;36mExecutorBase.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, multimodal_config, speculative_config)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultimodal_config \u001b[38;5;241m=\u001b[39m multimodal_config\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeculative_config \u001b[38;5;241m=\u001b[39m speculative_config\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/executor/multiproc_gpu_executor.py:79\u001b[0m, in \u001b[0;36mMultiprocessingGPUExecutor._init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_worker(\n\u001b[1;32m     77\u001b[0m     distributed_init_method\u001b[38;5;241m=\u001b[39mdistributed_init_method)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_workers(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_device\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mload_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmax_concurrent_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\n\u001b[1;32m     81\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmax_parallel_loading_workers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/executor/multiproc_gpu_executor.py:130\u001b[0m, in \u001b[0;36mMultiprocessingGPUExecutor._run_workers\u001b[0;34m(self, method, async_run_tensor_parallel_workers_only, max_concurrent_workers, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m worker_outputs\n\u001b[1;32m    129\u001b[0m driver_worker_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker, method)\n\u001b[0;32m--> 130\u001b[0m driver_worker_output \u001b[38;5;241m=\u001b[39m \u001b[43mdriver_worker_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Get the results of the workers.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [driver_worker_output\n\u001b[1;32m    134\u001b[0m         ] \u001b[38;5;241m+\u001b[39m [output\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m worker_outputs]\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/worker/worker.py:133\u001b[0m, in \u001b[0;36mWorker.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/worker/model_runner.py:243\u001b[0m, in \u001b[0;36mGPUModelRunnerBase.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m CudaMemoryProfiler() \u001b[38;5;28;01mas\u001b[39;00m m:\n\u001b[0;32m--> 243\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmultimodal_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultimodal_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mconsumed_memory\n\u001b[1;32m    255\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model weights took \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m GB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    256\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py:21\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(model_config, load_config, device_config, parallel_config, scheduler_config, lora_config, multimodal_config, cache_config)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model\u001b[39m(\u001b[38;5;241m*\u001b[39m, model_config: ModelConfig, load_config: LoadConfig,\n\u001b[1;32m     15\u001b[0m               device_config: DeviceConfig, parallel_config: ParallelConfig,\n\u001b[1;32m     16\u001b[0m               scheduler_config: SchedulerConfig,\n\u001b[1;32m     17\u001b[0m               lora_config: Optional[LoRAConfig],\n\u001b[1;32m     18\u001b[0m               multimodal_config: Optional[MultiModalConfig],\n\u001b[1;32m     19\u001b[0m               cache_config: CacheConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m nn\u001b[38;5;241m.\u001b[39mModule:\n\u001b[1;32m     20\u001b[0m     loader \u001b[38;5;241m=\u001b[39m get_model_loader(load_config)\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mmultimodal_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultimodal_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py:270\u001b[0m, in \u001b[0;36mDefaultModelLoader.load_model\u001b[0;34m(self, model_config, device_config, lora_config, multimodal_config, parallel_config, scheduler_config, cache_config)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(device_config\u001b[38;5;241m.\u001b[39mdevice):\n\u001b[1;32m    267\u001b[0m     model \u001b[38;5;241m=\u001b[39m _initialize_model(model_config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_config,\n\u001b[1;32m    268\u001b[0m                               lora_config, multimodal_config,\n\u001b[1;32m    269\u001b[0m                               cache_config)\n\u001b[0;32m--> 270\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_weights_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mfall_back_to_pt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfall_back_to_pt_during_load\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, module \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_modules():\n\u001b[1;32m    279\u001b[0m     quant_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquant_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/models/llama.py:461\u001b[0m, in \u001b[0;36mLlamaForCausalLM.load_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    459\u001b[0m     param \u001b[38;5;241m=\u001b[39m params_dict[name]\n\u001b[1;32m    460\u001b[0m     weight_loader \u001b[38;5;241m=\u001b[39m param\u001b[38;5;241m.\u001b[39mweight_loader\n\u001b[0;32m--> 461\u001b[0m     \u001b[43mweight_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py:657\u001b[0m, in \u001b[0;36mQKVParallelLinear.weight_loader\u001b[0;34m(self, param, loaded_weight, loaded_shard_id)\u001b[0m\n\u001b[1;32m    651\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading a weight without `output_dim` attribute in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    653\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQKVParallelLinear, assume the weight is the same \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    654\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor all partitions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m param_data\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m loaded_weight\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 657\u001b[0m \u001b[43mparam_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "dirs = [\"../..\"]\n",
    "for _dir in dirs:\n",
    "    if _dir not in sys.path:\n",
    "        sys.path.append(_dir)\n",
    "\n",
    "from swift.llm import (\n",
    "    ModelType, get_vllm_engine, get_default_template_type,\n",
    "    get_template, inference_vllm, VllmGenerationConfig\n",
    ")\n",
    "from custom import CustomModelType, CustomTemplateType\n",
    "\n",
    "\n",
    "# model_type = CustomModelType.phi_3_medium_4k_instruct\n",
    "model_type = CustomModelType.llama_3_70b_instruct_awq\n",
    "# model_type = CustomModelType.mixtral_moe_7b_instruct_awq\n",
    "# model_type = CustomModelType.solar_instruct_10_7b\n",
    "\n",
    "llm_engine = get_vllm_engine(\n",
    "    model_type, \n",
    "    # torch_dtype=torch.float16,  # 检查正确的数据类型！！！！\n",
    "    tensor_parallel_size=2,\n",
    "    max_model_len=4096,\n",
    "    # gpu_memory_utilization=0.92,\n",
    "    # model_id_or_path=\"/home/css/models/Mixtral-8x7B-Instruct-v0.1-GPTQ-int4\",\n",
    "    engine_kwargs = {\n",
    "        # \"enforce_eager\": True,\n",
    "        \"max_num_seqs\": 64, # 64\n",
    "        \"seed\": 42,\n",
    "    }\n",
    ")\n",
    "\n",
    "template_type = get_default_template_type(model_type)\n",
    "template = get_template(template_type, llm_engine.hf_tokenizer)\n",
    "\n",
    "generation_config = VllmGenerationConfig(\n",
    "    max_new_tokens=2048,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "get_resp_list = lambda request_list : inference_vllm(\n",
    "    llm_engine, template, request_list, \n",
    "    generation_config=generation_config, \n",
    "    # use_tqdm=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 1560.44it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  9.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI, I am a program designed to simulate understanding and provide helpful responses. I am functioning properly and ready to assist you with your needs. How about you, how are you doing today?\n",
      "As an AI, I am a program designed to simulate understanding and provide helpful responses. I am functioning properly and ready to assist you with your needs. How about you, how are you doing today?\n",
      "As an AI, I am a program designed to simulate understanding and provide helpful responses. I am functioning properly and ready to assist you with your needs. How about you, how are you doing today?\n",
      "As an AI, I am a program designed to simulate understanding and provide helpful responses. I am functioning properly and ready to assist you with your needs. How about you, how are you doing today?\n",
      "As an AI, I am a program designed to simulate understanding and provide helpful responses. I am functioning properly and ready to assist you with your needs. How about you, how are you doing today?\n",
      "As an AI, I am a program designed to simulate understanding and provide helpful responses. I am functioning properly and ready to assist you with your needs. How about you, how are you doing today?\n",
      "As an AI, I am a program designed to simulate understanding and provide helpful responses. I am functioning properly and ready to assist you with your needs. How about you, how are you doing today?\n",
      "As an AI, I am a program designed to simulate understanding and provide helpful responses. I am functioning properly and ready to assist you with your needs. How about you, how are you doing today?\n",
      "As an AI, I am a program designed to simulate understanding and provide helpful responses. I am functioning properly and ready to assist you with your needs. How about you, how are you doing today?\n",
      "As an AI, I am a program designed to simulate understanding and provide helpful responses. I am functioning properly and ready to assist you with your needs. How about you, how are you doing today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# get_resp_list = lambda request_list : inference_vllm(\n",
    "#     llm_engine, template, request_list, \n",
    "#     generation_config=generation_config, \n",
    "#     use_tqdm=False, \n",
    "#     verbose=True, prompt_prefix=\"\", output_prefix=\"\"\n",
    "# )\n",
    "\n",
    "s = \"Information Summary:\\n\\nInformation 1, published on 2020-12-01, discusses a false claim about a Covid-19 vaccine having the ability to \\\"transfer genetic material\\\" and manipulate human genes. The claim was shared hundreds of times on Facebook, but experts, such as Dr. Kirsty Short, have confirmed that the mRNA vaccine cannot enter the human genome. Information 2, published on 2020-05-19, mentions false claims about a future Covid-19 vaccine genetically modifying humans. Information 3, with no specific publication date, states that Covid-19 vaccines do not change a person's genes and use messenger RNA or modified adenovirus to trigger an immune response. Information 4, published on 2023-11-01, reiterates that Covid-19 vaccines cannot alter a person's genome and debunks false claims about DNA contamination leading to harmful effects. Information 5, with no specific publication date, highlights the importance of monitoring and addressing vaccine misinformation to prevent vaccine hesitancy.\\n\\nRestated Claim:\\nOn 2020-12-01, experts refuted the false claim that a Covid-19 vaccine can manipulate human genes.\\n\\nGiven the information available, the claim is TRUE. The Covid-19 vaccines developed by Pfizer-BioNTech, Moderna, and Johnson & Johnson do not have the ability to manipulate human genes, as confirmed by various experts and sources. These vaccines either use messenger RNA or modified adenovirus to trigger an immune response, but they cannot alter human DNA.\"\n",
    "\n",
    "prompt_list = [\"how are you\"] * 10\n",
    "\n",
    "resp_list = get_resp_list([{'query': prompt} for prompt in prompt_list])\n",
    "\n",
    "for resp in resp_list:\n",
    "    print(resp[\"response\"])\n",
    "\n",
    "# print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_prompt_for_generating_prior_knowledge(\n",
    "#         claim, claim_date, search_engine, search_results, model_name,\n",
    "#         K=5, sort=False, ids=None, without_info=False, without_claim_date=False):\n",
    "#     \"\"\"\n",
    "#     sort: 对search result 按时间进行排序\n",
    "#     \"\"\"\n",
    "\n",
    "#     claim = claim.strip()\n",
    "\n",
    "#     if model_name == \"solar\":\n",
    "#         pre = \"Below is some INFORMATION searched online and a CLAIM. These pieces of INFORMATION are relevant to the CLAIM. This CLAIM and all INFORMATION include their respective publication dates and contents. To classify the CLAIM more accurately (if the content described by the CLAIM is correct, it will be classified as TRUE; if the content described by the CLAIM is incorrect, it will be classified as FALSE), please first expand on the given INFORMATION and provide a detailed summary of it. Then analyze, reason, and provide reasonable evidence to judge the correctness of the CLAIM based on the available information and your knowledge, and finally generate prior knowledge that helps classify the CLAIM.\\n\\n\"\n",
    "#     elif model_name == \"mixtral\":\n",
    "#         # v1\n",
    "#         # pre = \"Below is a CLAIM and some INFORMATION searched online. These pieces of INFORMATION are relevant to the CLAIM. This CLAIM and all INFORMATION include their respective publication dates and contents. To classify the CLAIM more accurately (if the content described by the CLAIM is correct, it will be classified as TRUE; if the content described by the CLAIM is incorrect, it will be classified as FALSE), please first provide a detailed summary of the given INFORMATION and restate the CLAIM. Then reason, and provide reasonable evidence to judge the correctness of the CLAIM based on the available information and your knowledge. In reasoning, it is necessary to consider the sequential relationship between the date of publication of the CLAIM and the date of publication of the INFORMATION.\\n\\n\"\n",
    "#         # v2\n",
    "#         # pre = \"Below is some INFORMATION searched online and a CLAIM. These pieces of INFORMATION are relevant to the CLAIM. This CLAIM and all INFORMATION include their respective publication dates and contents. To classify the CLAIM more accurately (if the content described by the CLAIM is correct, it will be classified as TRUE; if the content described by the CLAIM is incorrect, it will be classified as FALSE), please first provide a detailed summary of the given INFORMATION and restate the CLAIM. Then reason, and provide reasonable evidence to judge the correctness of the CLAIM based on the available information and your knowledge.\\n\\n\"\n",
    "        \n",
    "#         pre = \"Below is some INFORMATION searched online and a CLAIM. These pieces of INFORMATION are relevant to the CLAIM. This CLAIM and all INFORMATION include their respective publication dates and contents. To classify the CLAIM more accurately (if the content described by the CLAIM is correct, it will be classified as TRUE; if the content described by the CLAIM is incorrect, it will be classified as FALSE), please first provide a clear summary of the given INFORMATION, restate the CLAIM, and provide reasonable evidence to judge the correctness of the CLAIM based on the available information and your knowledge.\\n\\n\"\n",
    "\n",
    "#     else:\n",
    "#         raise Exception(\"model_name 只能从solar，mixtral中选择\")\n",
    "    \n",
    "#     if without_claim_date:\n",
    "#         text = \"CLAIM: \" + claim\n",
    "#     else:\n",
    "#         text = \"CLAIM: \" + prompt_rag.get_claim_with_date(claim, claim_date)\n",
    "\n",
    "#     if search_engine == \"bing\":\n",
    "#         snippet = prompt_rag.get_bing_snippet_v2(search_results, K=K, claim_date=claim_date, sort=sort)\n",
    "#     elif search_engine == \"brave\":\n",
    "#         if ids is None:\n",
    "#             ids = slice(0, K)\n",
    "#         snippet = prompt_rag.get_brave_snippet(search_results, ids=ids)\n",
    "#     else:\n",
    "#         raise Exception(\"Select search engines in [\\\"bing\\\", \\\"brave\\\"].\")\n",
    "    \n",
    "#     info = \"INFORMATION:\\n\" + snippet + '\\n'\n",
    "\n",
    "#     if without_info:\n",
    "#         return (pre + text).strip()\n",
    "#     else:\n",
    "#         return pre + info + text\n",
    "\n",
    "# def get_claim_with_date(claim, claim_date=None):\n",
    "#     if claim_date is None:\n",
    "#         return \" \" + claim\n",
    "    \n",
    "#     # res = \"\\n\"\n",
    "#     res = claim + \"\\nPublication date: \" + claim_date\n",
    "#     return res\n",
    "\n",
    "# K = 5\n",
    "# def get_id(claim):\n",
    "#     for i in range(len(data_train)):\n",
    "#         if claim.strip() in data_train[i][\"claim\"].strip():\n",
    "#             return i\n",
    "\n",
    "# # i = 0\n",
    "# i = get_id(\"False claim circulates that Pakistani plane transported Sri Lankan students home after COVID-19 lockdown\")\n",
    "\n",
    "# # claim = data_search[i][\"claim\"]\n",
    "# search_results = data_search[i][f\"{search_engine}_search_results\"]\n",
    "\n",
    "# model_name = 'mixtral'\n",
    "# prompt_list1 = [\n",
    "#     get_prompt_for_generating_prior_knowledge(\n",
    "#         data_train[i][\"claim\"], data_train[i][\"date\"], search_engine, search_results, model_name,\n",
    "#         K=K, sort=False, \n",
    "#         # ids=data_search[i][\"random_ids\"],\n",
    "#         ids=None\n",
    "#     ),\n",
    "# ]\n",
    "\n",
    "# request_list1 = [{'query': prompt} for prompt in prompt_list1]\n",
    "\n",
    "# print(prompt_list1[0])\n",
    "# print()\n",
    "\n",
    "# resp_list1 = get_resp_list(request_list1)\n",
    "# print(resp_list1[0][\"response\"].strip())\n",
    "# print()\n",
    "# resp_list1[0][\"response\"].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:00<00:00, 181.34it/s]\n",
      " 19%|█▉        | 10/53 [00:44<03:05,  4.32s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 224\u001b[0m\n\u001b[1;32m    216\u001b[0m     prompt_list\u001b[38;5;241m.\u001b[39mappend(get_prompt_for_generating_prior_knowledge2(\n\u001b[1;32m    217\u001b[0m             data_train[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaim\u001b[39m\u001b[38;5;124m\"\u001b[39m], data_train[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m], search_engine, \n\u001b[1;32m    218\u001b[0m             search_results, model_name, K\u001b[38;5;241m=\u001b[39mK, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m    219\u001b[0m             \u001b[38;5;66;03m# ids=data_search[i][\"random_ids\"],\u001b[39;00m\n\u001b[1;32m    220\u001b[0m             ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    221\u001b[0m         ))\n\u001b[1;32m    222\u001b[0m request_list \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m: prompt} \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompt_list]\n\u001b[0;32m--> 224\u001b[0m resp_list \u001b[38;5;241m=\u001b[39m \u001b[43mget_resp_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# print(resp_list[0][\"response\"].strip())\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 46\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(request_list)\u001b[0m\n\u001b[1;32m     39\u001b[0m template \u001b[38;5;241m=\u001b[39m get_template(template_type, llm_engine\u001b[38;5;241m.\u001b[39mhf_tokenizer, model\u001b[38;5;241m=\u001b[39mllm_engine)\n\u001b[1;32m     41\u001b[0m generation_config \u001b[38;5;241m=\u001b[39m VllmGenerationConfig(\n\u001b[1;32m     42\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m,\n\u001b[1;32m     43\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     44\u001b[0m )\n\u001b[0;32m---> 46\u001b[0m get_resp_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m request_list : \u001b[43minference_vllm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_engine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     50\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/code/research/infodemic/LLM/swift/swift/llm/utils/vllm_utils.py:429\u001b[0m, in \u001b[0;36minference_vllm\u001b[0;34m(llm_engine, template, request_list, generation_config, lora_request, use_tqdm, verbose, prompt_prefix, output_prefix, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m step_outputs \u001b[38;5;241m=\u001b[39m llm_engine\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n\u001b[1;32m    430\u001b[0m         outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    431\u001b[0m         prog_bar\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/engine/llm_engine.py:776\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scheduler_outputs\u001b[38;5;241m.\u001b[39mis_empty():\n\u001b[1;32m    768\u001b[0m     execute_model_req \u001b[38;5;241m=\u001b[39m ExecuteModelRequest(\n\u001b[1;32m    769\u001b[0m         seq_group_metadata_list\u001b[38;5;241m=\u001b[39mseq_group_metadata_list,\n\u001b[1;32m    770\u001b[0m         blocks_to_swap_in\u001b[38;5;241m=\u001b[39mscheduler_outputs\u001b[38;5;241m.\u001b[39mblocks_to_swap_in,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    774\u001b[0m         running_queue_size\u001b[38;5;241m=\u001b[39mscheduler_outputs\u001b[38;5;241m.\u001b[39mrunning_queue_size,\n\u001b[1;32m    775\u001b[0m     )\n\u001b[0;32m--> 776\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    779\u001b[0m     output \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/executor/gpu_executor.py:91\u001b[0m, in \u001b[0;36mGPUExecutor.execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_model\u001b[39m(\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m, execute_model_req: ExecuteModelRequest\n\u001b[1;32m     90\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Union[SamplerOutput, PoolerOutput]]:\n\u001b[0;32m---> 91\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/worker/worker.py:280\u001b[0m, in \u001b[0;36mWorker.execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_seq_groups \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m--> 280\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# Worker only supports single-step execution. Wrap the output in a list\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# to conform to interface.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [output]\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/worker/model_runner.py:765\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, seq_group_metadata_list, kv_caches)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;66;03m# Sample the next token.\u001b[39;00m\n\u001b[0;32m--> 765\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/models/llama.py:386\u001b[0m, in \u001b[0;36mLlamaForCausalLM.sample\u001b[0;34m(self, logits, sampling_metadata)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    383\u001b[0m     logits: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    384\u001b[0m     sampling_metadata: SamplingMetadata,\n\u001b[1;32m    385\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[SamplerOutput]:\n\u001b[0;32m--> 386\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m next_tokens\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:96\u001b[0m, in \u001b[0;36mSampler.forward\u001b[0;34m(self, logits, sampling_metadata)\u001b[0m\n\u001b[1;32m     93\u001b[0m logprobs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Sample the next tokens.\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m sample_results, maybe_sampled_tokens_tensor \u001b[38;5;241m=\u001b[39m \u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodify_greedy_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_modify_greedy_probs_inplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_gpu_probs_tensor:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m maybe_sampled_tokens_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:655\u001b[0m, in \u001b[0;36m_sample\u001b[0;34m(probs, logprobs, sampling_metadata, sampling_tensors, include_gpu_probs_tensor, modify_greedy_probs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sample\u001b[39m(\n\u001b[1;32m    639\u001b[0m     probs: torch\u001b[38;5;241m.\u001b[39mTensor, logprobs: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    640\u001b[0m     sampling_metadata: SamplingMetadata, sampling_tensors: SamplingTensors,\n\u001b[1;32m    641\u001b[0m     include_gpu_probs_tensor: \u001b[38;5;28mbool\u001b[39m, modify_greedy_probs: \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m    642\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[SampleResultType, Optional[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m    643\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;124;03m        probs: (num_query_tokens_in_batch, num_vocab)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;124;03m        sampled_token_ids_tensor: A tensor of sampled token ids.\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sample_with_torch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodify_greedy_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodify_greedy_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:542\u001b[0m, in \u001b[0;36m_sample_with_torch\u001b[0;34m(probs, logprobs, sampling_metadata, include_gpu_probs_tensor, modify_greedy_probs)\u001b[0m\n\u001b[1;32m    540\u001b[0m (seq_group_id, seq_groups) \u001b[38;5;241m=\u001b[39m sample_metadata[sampling_type]\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sampling_type \u001b[38;5;241m==\u001b[39m SamplingType\u001b[38;5;241m.\u001b[39mGREEDY:\n\u001b[0;32m--> 542\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m \u001b[43m_greedy_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreedy_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;129;01min\u001b[39;00m (SamplingType\u001b[38;5;241m.\u001b[39mRANDOM, SamplingType\u001b[38;5;241m.\u001b[39mRANDOM_SEED):\n\u001b[1;32m    544\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m _random_sample(seq_groups,\n\u001b[1;32m    545\u001b[0m                                     multinomial_samples[sampling_type])\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:288\u001b[0m, in \u001b[0;36m_greedy_sample\u001b[0;34m(selected_seq_groups, samples)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_greedy_sample\u001b[39m(\n\u001b[1;32m    273\u001b[0m     selected_seq_groups: List[SequenceGroupToSample],\n\u001b[1;32m    274\u001b[0m     samples: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    275\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SampleResultType:\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run greedy sampling on a given samples.\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m        seq_group has do_sample=False, tuple contains ([], [])\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[43msamples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    290\u001b[0m     results: SampleResultType \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_prompt_for_generating_prior_knowledge2(\n",
    "        claim, claim_date, search_engine, search_results, model_name,\n",
    "        K=5, sort=False, ids=None, without_info=False, without_claim_date=False):\n",
    "    \"\"\"\n",
    "    pre + info + text\n",
    "    \"\"\"\n",
    "\n",
    "    claim = claim.strip()\n",
    "\n",
    "    if model_name == \"solar\":\n",
    "        pre = \"Below is some INFORMATION searched online and a CLAIM. These pieces of INFORMATION are relevant to the CLAIM. This CLAIM and all INFORMATION include their respective publication dates and contents. To classify the CLAIM more accurately (if the content described by the CLAIM is correct, it will be classified as TRUE; if the content described by the CLAIM is incorrect, it will be classified as FALSE), please first expand on the given INFORMATION and provide a detailed summary of it. Then analyze, reason, and provide reasonable evidence to judge the correctness of the CLAIM based on the available information and your knowledge, and finally generate prior knowledge that helps classify the CLAIM.\\n\\n\"\n",
    "        # pre = \"Below is some INFORMATION searched online and a CLAIM. These pieces of INFORMATION are relevant to the CLAIM. This CLAIM and all INFORMATION include their respective publication dates and contents. To classify the CLAIM more accurately (if the content described by the CLAIM is correct, it will be classified as TRUE; if the content described by the CLAIM is incorrect, it will be classified as FALSE), please first provide a clear summary of the given INFORMATION, and provide reasonable evidence to judge the correctness of the CLAIM based on the available information and your knowledge.\\n\\n\"\n",
    "    elif model_name == \"mixtral\":\n",
    "        # pre = \"Below is a CLAIM and some INFORMATION searched online. These pieces of INFORMATION are relevant to the CLAIM. This CLAIM and all INFORMATION include their respective publication dates and contents. To classify the CLAIM more accurately (if the content described by the CLAIM is correct, it will be classified as TRUE; if the content described by the CLAIM is incorrect, it will be classified as FALSE), please first provide a detailed summary of the given INFORMATION and restate the CLAIM. Then reason, and provide reasonable evidence to judge the correctness of the CLAIM based on the available information and your knowledge. In reasoning, it is necessary to consider the sequential relationship between the date of publication of the CLAIM and the date of publication of the INFORMATION.\\n\\n\"\n",
    "        pre = \"Below is some INFORMATION searched online and a CLAIM. These pieces of INFORMATION are relevant to the CLAIM. This CLAIM and all INFORMATION include their respective publication dates and contents. To classify the CLAIM more accurately (if the content described by the CLAIM is correct, it will be classified as TRUE; if the content described by the CLAIM is incorrect, it will be classified as FALSE), please first provide a clear summary of the given INFORMATION, and provide reasonable evidence to judge the correctness of the CLAIM based on the available information and your knowledge.\\n\\n\"\n",
    "        \n",
    "        # pre = \"Below is some INFORMATION searched online and a <CLAIM>. These pieces of INFORMATION are relevant to the <CLAIM>. This <CLAIM> and all INFORMATION include their respective publication dates and contents. To classify the <CLAIM> more accurately (if the content described by the <CLAIM> is correct, it will be classified as TRUE; if the content described by the <CLAIM> is incorrect, it will be classified as FALSE), please first provide a clear summary of the given INFORMATION, and provide reasonable evidence to judge the correctness of the <CLAIM> based on the available information and your knowledge.\\n\\n\"\n",
    "    \n",
    "    elif model_name == \"llama3\":\n",
    "        # pre = \"Below is some INFORMATION searched online and a <CLAIM>. These pieces of INFORMATION are relevant to the <CLAIM>. This <CLAIM> and all INFORMATION include their respective publication dates and contents. To classify the <CLAIM> more accurately (if the content described by the <CLAIM> is correct, it will be classified as TRUE; if the content described by the <CLAIM> is incorrect, it will be classified as FALSE), please first provide a clear summary of the given INFORMATION, restate the <CLAIM>, and provide reasonable evidence to judge the correctness of the <CLAIM> based on the available information and your knowledge.\\n\\n\"\n",
    "        pre = \"Below is some INFORMATION searched online and a <CLAIM>. These pieces of INFORMATION are relevant to the <CLAIM>. This <CLAIM> and all INFORMATION include their respective publication dates and contents. To classify the <CLAIM> more accurately (if the content described by the <CLAIM> is correct, it will be classified as TRUE; if the content described by the <CLAIM> is incorrect, it will be classified as FALSE), please first provide a clear summary of the given INFORMATION, and provide reasonable evidence to judge the correctness of the <CLAIM> based on the available information and your knowledge.\\n\\n\"\n",
    "    elif model_name == \"phi3\":\n",
    "        # CLAIM\n",
    "        pre = \"Below is some INFORMATION searched online and a CLAIM. These pieces of INFORMATION are relevant to the CLAIM. This CLAIM and all INFORMATION include their respective publication dates and contents. To classify the CLAIM more accurately (if the content described by the CLAIM is correct, it will be classified as TRUE; if the content described by the CLAIM is incorrect, it will be classified as FALSE), please first provide a clear summary of the given INFORMATION, and provide reasonable evidence to judge the correctness of the CLAIM based on the available information and your knowledge.\\n\\n\"\n",
    "        \n",
    "        # <CLAIM>\n",
    "        # pre = \"Below is some INFORMATION searched online and a <CLAIM>. These pieces of INFORMATION are relevant to the <CLAIM>. This <CLAIM> and all INFORMATION include their respective publication dates and contents. To classify the <CLAIM> more accurately (if the content described by the <CLAIM> is correct, it will be classified as TRUE; if the content described by the <CLAIM> is incorrect, it will be classified as FALSE), please first provide a clear summary of the given INFORMATION, and provide reasonable evidence to judge the correctness of the <CLAIM> based on the available information and your knowledge.\\n\\n\"\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"model_name 只能从solar，mixtral中选择\")\n",
    "    \n",
    "    if without_claim_date:\n",
    "        text = \"CLAIM: \" + claim\n",
    "    else:\n",
    "        if model_name == \"solar\":\n",
    "            text = \"CLAIM: \"\n",
    "        if model_name == \"mixtral\":\n",
    "            text = \"CLAIM: \"\n",
    "        elif model_name == \"llama3\":\n",
    "            text = \"<CLAIM>: \"\n",
    "        elif model_name == \"phi3\":\n",
    "            text = \"CLAIM: \"\n",
    "        text += get_claim_with_date(claim, claim_date)\n",
    "\n",
    "    if search_engine == \"bing\":\n",
    "        snippet = prompt_rag.get_bing_snippet_v2(search_results, K=K, claim_date=claim_date, sort=sort)\n",
    "    elif search_engine == \"brave\":\n",
    "        if ids is None:\n",
    "            ids = slice(0, K)\n",
    "        snippet = prompt_rag.get_brave_snippet(search_results, ids=ids)\n",
    "    else:\n",
    "        raise Exception(\"Select search engines in [\\\"bing\\\", \\\"brave\\\"].\")\n",
    "    \n",
    "    info = \"INFORMATION:\\n\" + snippet + '\\n'\n",
    "\n",
    "    if without_info:\n",
    "        return (pre + text).strip()\n",
    "    else:\n",
    "        return pre + info + text\n",
    "\n",
    "def get_prompt_for_generating_prior_knowledge1(\n",
    "        claim, claim_date, search_engine, search_results, model_name,\n",
    "        K=5, sort=False, ids=None, without_info=False, without_claim_date=False):\n",
    "    \"\"\"\n",
    "    pre + text + info\n",
    "    \"\"\"\n",
    "\n",
    "    claim = claim.strip()\n",
    "\n",
    "    if model_name == \"solar\":\n",
    "        pre = \"Below is some INFORMATION searched online and a CLAIM. These pieces of INFORMATION are relevant to the CLAIM. This CLAIM and all INFORMATION include their respective publication dates and contents. To classify the CLAIM more accurately (if the content described by the CLAIM is correct, it will be classified as TRUE; if the content described by the CLAIM is incorrect, it will be classified as FALSE), please first expand on the given INFORMATION and provide a detailed summary of it. Then analyze, reason, and provide reasonable evidence to judge the correctness of the CLAIM based on the available information and your knowledge, and finally generate prior knowledge that helps classify the CLAIM.\\n\\n\"\n",
    "    elif model_name == \"mixtral\":\n",
    "\n",
    "\n",
    "        pre = \"Below is a CLAIM and some INFORMATION searched online. These pieces of INFORMATION are relevant to the CLAIM. This CLAIM and all INFORMATION include their respective publication dates and contents. To classify the CLAIM more accurately (if the content described by the CLAIM is correct, it will be classified as TRUE; if the content described by the CLAIM is incorrect, it will be classified as FALSE), please first provide a clear summary of the given INFORMATION, restate the CLAIM, and provide reasonable evidence to judge the correctness of the CLAIM based on the available information and your knowledge.\\n\\n\"\n",
    "        pre += \"CLAIM: \"\n",
    "\n",
    "        # pre = \"Below is a <CLAIM> and some INFORMATION searched online. These pieces of INFORMATION are relevant to the <CLAIM>. This <CLAIM> and all INFORMATION include their respective publication dates and contents. To classify the <CLAIM> more accurately (if the content described by the <CLAIM> is correct, it will be classified as TRUE; if the content described by the <CLAIM> is incorrect, it will be classified as FALSE), please first provide a clear summary of the given INFORMATION, restate the <CLAIM>, and provide reasonable evidence to judge the correctness of the <CLAIM> based on the available information and your knowledge.\\n\\n\"\n",
    "        # pre += \"<CLAIM>: \"\n",
    "\n",
    "    elif model_name == \"llama3\":\n",
    "\n",
    "        pre = \"Below is a <CLAIM> and some INFORMATION searched online. These pieces of INFORMATION are relevant to the <CLAIM>. This <CLAIM> and all INFORMATION include their respective publication dates and contents. To classify the <CLAIM> more accurately (if the content described by the <CLAIM> is correct, it will be classified as TRUE; if the content described by the <CLAIM> is incorrect, it will be classified as FALSE), please first provide a clear summary of the given INFORMATION, restate the <CLAIM>, and provide reasonable evidence to judge the correctness of the <CLAIM> based on the available information and your knowledge.\\n\\n\"\n",
    "        pre += \"<CLAIM>: \"\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"model_name 只能从solar，mixtral中选择\")\n",
    "    \n",
    "    if without_claim_date:\n",
    "        text = \"CLAIM: \" + claim\n",
    "    else:\n",
    "        text = get_claim_with_date(claim, claim_date) + '\\n\\n'\n",
    "\n",
    "    if search_engine == \"bing\":\n",
    "        snippet = prompt_rag.get_bing_snippet_v2(search_results, K=K, claim_date=claim_date, sort=sort)\n",
    "    elif search_engine == \"brave\":\n",
    "        if ids is None:\n",
    "            ids = slice(0, K)\n",
    "        snippet = prompt_rag.get_brave_snippet(search_results, ids=ids)\n",
    "    else:\n",
    "        raise Exception(\"Select search engines in [\\\"bing\\\", \\\"brave\\\"].\")\n",
    "    \n",
    "    info = \"INFORMATION:\\n\" + snippet\n",
    "\n",
    "    if without_info:\n",
    "        return (pre + text).strip()\n",
    "    else:\n",
    "        return pre + text + info\n",
    "    \n",
    "def get_claim_with_date(claim, claim_date=None):\n",
    "    if claim_date is None:\n",
    "        return \" \" + claim\n",
    "    \n",
    "    # res = \"\\n\"\n",
    "    res = claim + \"\\nPublication date: \" + claim_date\n",
    "    return res\n",
    "\n",
    "K = 5\n",
    "def get_id(claim):\n",
    "    for i in range(len(data_train)):\n",
    "        if claim.strip() in data_train[i][\"claim\"].strip():\n",
    "            return i\n",
    "\n",
    "# i = 0\n",
    "i = get_id(\"Pfizer and Moderna do call their COVID-19 shot a ‘vaccine’\")\n",
    "\n",
    "# ids = [194]\n",
    "ids1 = [19, 36, 57, 85, 98, 107, 116, 117, 125, 129, 194, 195, 204, 208, 212, 235, 279, 324, 328]\n",
    "# ids1 = [\n",
    "#     get_id(\"Pfizer and Moderna do call their COVID-19 shot a ‘vaccine’\"),\n",
    "#     get_id(\"Myth spreads online that Australian supermarkets have banned Chinese nationals during COVID-19 pandemic\"),\n",
    "#     get_id(\"Trump makes false claims about COVID-19 testing\"),\n",
    "#     get_id(\"UNHCR condemns fake notice which claimed refugees in Malaysia are resisting COVID-19 tests\"),\n",
    "#     get_id(\"Hospitals in Ohio were not set on fire during protests\"),\n",
    "#     get_id(\"A 2010 study on vaccines did not show that one in forty were damaged by vaccination\"),\n",
    "#     get_id(\"The Philippine health department said it did not issue this 'checklist' for COVID-19 symptoms\"),\n",
    "#     get_id(\"Contaminated CDC COVID-19 test kits recalled and did not spread virus\"),\n",
    "#     get_id(\"Experts refute false claim that Covid-19 vaccine can 'manipulate' human genes\"),\n",
    "#     get_id(\"Countries were not buying Covid-19 test kits in 2018\"),\n",
    "#     get_id(\"False claim circulates that Pakistani plane transported Sri Lankan students home after COVID-19 lockdown\"),\n",
    "#     get_id(\"People will not have to be vaccinated against COVID-19 to receive food stamps and rent assistance\"),\n",
    "#     get_id(\"Britain has not awarded a contract to develop a vaccine passport\"),\n",
    "#     get_id(\"World Health Organization says COVID-19 means ‘coronavirus disease 2019’ – not 'China outbreak virus'\"),\n",
    "#     get_id(\"The common cold is not the same as COVID-19 and the NHS is not saying it is\"),\n",
    "#     get_id(\"Philippine authorities did not issue this warning after the novel coronavirus outbreak\"),\n",
    "#     get_id(\"No tourists have been allowed to visit New Zealand since March 2020 -- this photo has circulated online since 2016\"),\n",
    "#     get_id(\"This video does not show social distancing failure on an Air India flight during the coronavirus pandemic\"),\n",
    "#     get_id(\"This photo shows a Sri Lankan airline pilot who tested positive for the novel coronavirus.\"), # fALSE\n",
    "# ]\n",
    "\n",
    "ids2 = [\n",
    "    get_id(\"Gynecological Cancers Not Tied to Severe COVID-19\"),\n",
    "    get_id(\"In February 2021, the Centers for Disease Control and Prevention (CDC) advised U.S. travelers to \\u201cavoid all travel to Mexico.\\u201d\"),\n",
    "    get_id(\"The U.S. CDC encourages the use of a \\u201c[COVID-19] flu shot\\u201d on children.\"),\n",
    "    get_id(\"An announcement was made on March 9 that all classes in the Basque Country were canceled due to the coronavirus.\"),\n",
    "    get_id(\"Rita Wilson, Tom Hanks’ wife, stated in an interview at CBS that “she wouldn’t be alive if not for chloroquine”\"),\n",
    "    get_id(\"Japan’s Nobel Prize winning Professor of Medicine, Professor Dr Tasuku Honjo has claimed that the coronavirus is not natural and that China manufactured it.\"),\n",
    "    get_id(\"Lies spread by Serbian authorities about COVID-19. For example, the virus does not affect pregnant women, children and young people; no newborn is infected with the coronavirus; the virus does not last long on objects; the number of respiratory machines that Serbia possesses, etc.\"),\n",
    "    get_id(\"Chief Secretary of West Bengal was not following relaxing while the state was performing poorly as it fought COVID-19.\"),\n",
    "    get_id(\"Businessman Ratan Tata has said that 2020 is the year to survive and not care about profits and losses.\"),\n",
    "    get_id(\"Celebrities spreading misinformation about coronavirus and the Janata curfew in India.\"),\n",
    "    get_id(\"This 3 year old girl is fighting for her life after getting the coronavirus.\"),\n",
    "    get_id(\"President Vucic claims that no one said coronavirus is the “funniest” virus.\"),\n",
    "    get_id(\"News photo from stay-at-home protest was doctored to add Confederate flag.\"),\n",
    "    get_id(\"President Donald Trump referred to the coronavirus as a “hoax” or “political conspiracy.”\"),\n",
    "    get_id(\"President Trump refers to the coronavirus as a hoax in an audio clip.\"),\n",
    "    get_id(\"A false image says that President Duque will declare a shutdown on November 1st.\"),\n",
    "    get_id(\"Professor Perronne makes several false claims on PCR tests, HCQ and hospitals.\"),\n",
    "    get_id(\"Audio of Biden calling the coronavirus a hoax.\"),\n",
    "    get_id(\"President Trump’s claim that he inherited no ventilators from the Obama administration.\"),\n",
    "    get_id(\"Video shows President Donald Trump saying COVID-19 is Democrats’ “new hoax.”\"),\n",
    "    get_id(\"Fauci Wasn't Involved in New Testing Guidelines\"),\n",
    "    get_id(\"This movie “Songbird” has been made before Covid. We’ve seen a lot of similar movies, yes. But, it is interesting that in the movie the virus is called COVID-23. Stop the scene when there is news on TV and you will see. Coincidence?\"),\n",
    "    get_id(\"Images of the newspaper front pages sharing vaccine misinformation\"),\n",
    "    get_id(\"I don't believe illegal casinos are operating in Bangkok, but if the doctor knows about it, he can inform the authorities.\"),\n",
    "    get_id(\"Facebook posts promote false conspiracy that coronavirus testing patent was submitted in 2015\"),\n",
    "    get_id(\"Misleading description of Canada\\u2019s quarantine sites feeds Covid-19 conspiracy\"),\n",
    "    get_id(\"Video repeats COVID-19 conspiracy theories\"),\n",
    "    get_id(\"The post contains a COVID-19 conspiracy theory written by former South Carolina Rep. Trey Gowdy.\"),\n",
    "    get_id(\"A long conspiracy video claims that: vaccines can alter a person’s DNA, nanorobots are inserted with the vaccine to collect biometric data, Bill Gates already owns this data and the body can receive 5G signal after the vaccine is taken\"),\n",
    "    get_id(\"Jorge Luis Sonnante published a 16-minute video that went viral on networks. In the video, Sonnante (who describes himself as a \\u201cdeacon\\u201d, but gives falsified evidence of this charge) mixes several conspiracy theories, some meaningless, others already denied, about the pandemic caused by the SARS coronavirus -CoV-2.\"),\n",
    "    get_id(\"Trump touts testing as \\\"greatest capacity in the world,\\\" but says people \\\"shouldn't want to get tested\\\"\"),\n",
    "    get_id(\"Alberto Fernández, about the coronavirus: “Mortality in people over 65 is 80%”.\"),\n",
    "    get_id(\"An English man received a COVID-19 vaccine through his shirt\"),\n",
    "    get_id(\"The first recipient of the COVID-19 trial vaccine is dead.\"),\n",
    "    get_id(\"ICUs are not overwhelmed\"),\n",
    "    get_id(\"Coronavirus tests do not work to diagnose COVID-19. Red Bull tests positive.\"),\n",
    "    get_id(\"Rapidly developed pharmaceuticals such as the Covid-19 vaccines compared to a sedative from the 1950s that caused serious birth defects.\"),\n",
    "    get_id(\"The major cause of death in Covid-19 is thrombosis or blood clot and not pneumonia.\"),\n",
    "    get_id(\"The COVID-19 virus was artificially created by the U.S. military and placed in a capsule.\"),\n",
    "    get_id(\"A video of a girl collapsing in a store is being shared widely on social media with a claim that the girl died of Coronavirus.\"),\n",
    "    get_id(\"Italy\\u2019s prime minister cries and declares that his country \\u201clost the battle\\u201d against the coronavirus.\"),\n",
    "    get_id(\"Black people are not being targeted for UK coronavirus vaccine trials\"),\n",
    "    get_id(\"In September 2019, a train named COVID-19 appeared in the United States, carrying gases that cause COVID-19.\"),\n",
    "    get_id(\"If you take the vaccine, you'll be enrolled in a pharmacovigilance tracking system. It means that you've enrolled yourself in a medical trial.\"),\n",
    "    get_id(\"Henry Kissinger quote about mandatory vaccinations\"),\n",
    "    get_id(\"RECEIVE FOOD AID SOCIAL PLAN 2020\"),\n",
    "    get_id(\"Post claims the video clip is last message of deceased Pakistani doctor Osama Riyaz who had contracted Coronavirus while treating patients\"),\n",
    "    get_id(\"For a few days now, an image has been circulating on Facebook claiming that COVID-19 can be cured with Dolex Gripa, \\\"Noraver\\\" night, and gargling with warm water and lemon.\"),\n",
    "    get_id(\"A magic remedy to prevent COVID-19: mix brown sugar, ginger, garlic, and Chinese leek, boil them and drink it.\"),\n",
    "    get_id(\"Some statements by the American political scientist and activist Susan George in which she says that “Spaniards are laboratory rats: let’s see how much punishment they tolerate without rebelling.” The phrase is shared related to the current coronavirus pandemic.\"),\n",
    "    get_id(\"Saddam Hussein explains what the coronavirus is.\"),\n",
    "    get_id(\"A Facebook post is spreading the false claim that former President Barack Obama gave $3.8 million to a lab in Wuhan, China.\"),\n",
    "    get_id(\"\\u2019No discrimination\\u2019 against Africans amid pandemic.\")\n",
    "]\n",
    "\n",
    "ids_x = [ids2[3],]\n",
    "\n",
    "prompt_list = []\n",
    "model_name = 'solar'\n",
    "\n",
    "# [ids2[ii] for ii in ids_filtered]:\n",
    "for i in ids2:\n",
    "    # claim = data_search[i][\"claim\"]\n",
    "    search_results = data_search[i][f\"{search_engine}_search_results\"]\n",
    "    prompt_list.append(get_prompt_for_generating_prior_knowledge2(\n",
    "            data_train[i][\"claim\"], data_train[i][\"date\"], search_engine, \n",
    "            search_results, model_name, K=K, sort=False, \n",
    "            # ids=data_search[i][\"random_ids\"],\n",
    "            ids=None\n",
    "        ))\n",
    "request_list = [{'query': prompt} for prompt in prompt_list]\n",
    "\n",
    "resp_list = get_resp_list(request_list)\n",
    "# print(resp_list[0][\"response\"].strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "0\n",
      "To classify the claim accurately, we need to analyze the available information and determine if it provides evidence of a Sri Lankan airline pilot testing positive for the novel coronavirus (COVID-19).\n",
      "\n",
      "Information 1 discusses the financial difficulties of SriLankan Airlines and mentions that one of their pilots tested positive for COVID-19. This information aligns with the claim, as it confirms the presence of a Sri Lankan airline pilot who tested positive for the virus. However, it does not provide any visual evidence, such as a photo.\n",
      "\n",
      "Information 2 is a travel advice from the UK government regarding COVID-19 in Sri Lanka, but it does not provide any specific details about a pilot testing positive.\n",
      "\n",
      "Information 3 is a dashboard displaying COVID-19 statistics in Sri Lanka, which is relevant to the context but does not provide direct evidence about the claim.\n",
      "\n",
      "Information 4 discusses a misleading claim about Sri Lanka eradicating the coronavirus, and it mentions that Sri Lanka had one confirmed case of the virus on January 27, 2020, which later recovered. Although this information is related to COVID-19 in Sri Lanka, it does not provide any direct evidence about the claim.\n",
      "\n",
      "Information 5 is a synoptic analysis of the COVID-19 outbreak in Sri Lanka, focusing on the impact on Sri Lankan migrant workers. This information is not directly relevant to the claim.\n",
      "\n",
      "In conclusion, while Information 1 provides evidence that a Sri Lankan airline pilot tested positive for COVID-19, it does not include a photo. Since there is no visual evidence in the provided information, we cannot definitively confirm or deny the claim based on the available data.\n"
     ]
    }
   ],
   "source": [
    "# llama-3-70b\n",
    "# CLAIM + info wrong: 1, 18\n",
    "# info + CLAIM wrong: 18\n",
    "\n",
    "# CLAIM + info wrong: 1, 2, 3, 7, 11, 15, 20, 30, 45, 51\n",
    "# info + CLAIM wrong: 1, 2, $3, 7, 11, 20, 45  √√√√√√√√√√√\n",
    "\n",
    "\n",
    "# mixtral-8x7b\n",
    "# claim wrong: 0, 1, 8, 10, 12\n",
    "# <claim>:1, 3\n",
    "\n",
    "# claim: 0, 1, 15, 16, 20, 24, 25, 30, 51\n",
    "# <claim>: 0, 1, 2, 7, 15, 16, 24, 25, 29, 51\n",
    "\n",
    "# phi-3-medium\n",
    "# claim wrong: \n",
    "# <claim>: 1, 8, 13, 18\n",
    "\n",
    "# claim: 1, 3, 7, 13, 15, 20, 22, 24, 25, 39, 51\n",
    "# <claim>: \n",
    "\n",
    "# solar\n",
    "# old: 0, 1, 10\n",
    "# new: 0, 1, 3, 8, 9, 10, 11, 13,\n",
    "print(len(ids1))\n",
    "\n",
    "nn = 18\n",
    "# print(prompt_list[nn])\n",
    "# print('*'*50)\n",
    "print(data_train[ids1[nn]][\"label\"])\n",
    "print(resp_list[nn][\"response\"].strip())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
