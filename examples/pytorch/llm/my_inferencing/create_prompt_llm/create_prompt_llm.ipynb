{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "import sys\n",
    "import prompt_rag\n",
    "\n",
    "dirs = [\"..\"]\n",
    "for _dir in dirs:\n",
    "    if _dir not in sys.path:\n",
    "        sys.path.append(_dir)\n",
    "\n",
    "import covmis, liar2\n",
    "\n",
    "search_engine = \"brave\"\n",
    "\n",
    "data_train = covmis.load_train()\n",
    "data_search = covmis.load_train_search()\n",
    "data_search_llm = covmis.load_train_llm()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合并多个文件的先验知识"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bing search llm\n",
    "# v2: k = 5\n",
    "# v3: 不对时间排序\n",
    "\n",
    "# brave search llm\n",
    "\n",
    "# Solar\n",
    "# v1: k = 5\n",
    "# v2: k = 5 随机选取\n",
    "\n",
    "# Mixtral\n",
    "# v1: k = 5\n",
    "\n",
    "\n",
    "# llama3\n",
    "# v1: k = 5, vllm = 0.5.0\n",
    "\n",
    "sort = False\n",
    "prior_knowledge_list = []\n",
    "\n",
    "K = 5\n",
    "prior_knowledge_version = \"1\"\n",
    "model_name = \"llama3\"\n",
    "\n",
    "# with open(f\"train_search_llm_tmp.json\", \"r\") as f:\n",
    "#     prior_knowledge_list = json.load(f)\n",
    "\n",
    "# for i, item in enumerate(data_search_llm):\n",
    "\n",
    "#     if item[\"id\"] != prior_knowledge_list[i][\"id\"]:\n",
    "#         print(i)\n",
    "#         print(data_train[i][\"claim\"])\n",
    "#         print(prior_knowledge_list[i][\"claim\"])\n",
    "#         raise Exception()\n",
    "#     else:\n",
    "#         # vv = \"\"\n",
    "#         # if prior_knowledge_list[i].get(f\"prior_knowledge_{model_name}3\") is not None:\n",
    "#         #     vv = \"3\"\n",
    "#         # elif prior_knowledge_list[i].get(f\"prior_knowledge_{model_name}2\") is not None:\n",
    "#         #     vv = \"2\"\n",
    "\n",
    "#         item[f\"prior_knowledge_{model_name}_v{prior_knowledge_version}_K={K}\"] = prior_knowledge_list[i][f\"prior_knowledge_{model_name}\"]\n",
    "# # There was a false study done where they gave it to very sick people -- extremely sick people, people that were ready to die. It was given by, obviously, not friends of the administration. And the study came out. The people were ready to die. Everybody was old, had bad problems with hearts, diabetes, and everything else you can imagine. So they gave it. So, immediately, when it came out, they gave a lot of false information, just so you understand. Great studies came out of Italy on hydroxy. \n",
    "# # There was a false study done where they gave it to very sick people -- extremely sick people, people that were ready to die. It was given by, obviously, not friends of the administration. And the study came out. The people were ready to die. Everybody was old, had bad problems with hearts, diabetes, and everything else you can imagine. So they gave it. So, immediately, when it came out, they gave a lot of false information, just so you understand. Great studies came out of Italy on hydroxy. \n",
    "\n",
    "# data_search_llm[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covmis.save_train_llm(data_search_llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建数据（带有先验知识的Prompt）以微调LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a CLAIM and the PRIOR KNOWLEDGE associated with it. Please classify the CLAIM as TRUE or FALSE based on the PRIOR KNOWLEDGE. If the content described by the CLAIM is correct, then classify it as TRUE; if the content described by the CLAIM is incorrect, then classify it as FALSE.\n",
      "\n",
      "CLAIM: Illinois Strengthens Face Mask Rules in Businesses\n",
      "Publication date: 2020-08-13\n",
      "\n",
      "PRIOR KNOWLEDGE:\n",
      "Information 1:\n",
      "Publication date: 2013-08-06\n",
      "Title: Illinois Strengthens Face Mask Rules in Businesses - WebMD\n",
      "Content:\n",
      "COVID-19 is a new type of coronavirus that causes mild to severe cases. Here’s a quick guide on how to spot symptoms, risk factors, prevent spread of the disease, and find out what to do if you think you have it.\n",
      "Information 2:\n",
      "Publication date: None\n",
      "Title: press-release\n",
      "Content:\n",
      "Illinoisans can resume activities without wearing a mask indoors on February 28th except where required by federal, state, local, tribal, or territorial laws, rules, and regulations, including local business and workplace guidance. Federal requirements, in effect through at least March 18, include all transportation systems such as airports, planes, trains, and buses. \"Preparing to repeal statewide masking mandates at the end of the month is aggressive and optimistic but reasonable,\" said Dr.\n",
      "\"Broad mandates are not about individuals. They are put in place to help protect communities, businesses, and healthcare access. Repealing the mask mandate allows people to choose the mitigation layers that are best for them and I have no doubt that many should and will choose to keep mask rules.\"\n",
      "If these trends continue — and we expect them to —then on Monday, February 28th, we will lift the indoor mask requirement for the State of Illinois,\" said Governor JB Pritzker. \"I want to be clear: Many local jurisdictions, businesses and organizations have their own mask requirements and other mitigations that must be respected.\n",
      "Seth Trueger, a Northwestern emergency physician who is also immunocompromised. \"Masking has helped slow the spread even in the face of omicron's transmissibility. We can and must use this time to further increase vaccination uptake & outreach, especially among children and other populations with low vaccination rates, so when the next wave comes, we will be even better prepared.\"\n",
      "Information 3:\n",
      "Publication date: None\n",
      "Title: Mask and Vaccine Requirements FAQ's\n",
      "Content:\n",
      "On September 3, 2021, the Governor signed Executive Order 21-22 which requires all individuals over the age of 2 and who can medically tolerate a face covering to wear a face covering when in indoor public places. The Executive Order also requires health care workers, school personnel, higher education personnel and students, and employees and contractors of state-owned or operated congregate facilities to be fully vaccinated, as described in the Order\n",
      "Information 4:\n",
      "Publication date: None\n",
      "Title: FAQ for Businesses Concerning Use of Face-Coverings During COVID-19\n",
      "Content:\n",
      "A: Businesses reserve the right to refuse service to persons unable to comply with the requirement to wear a face covering, but they are required to provide a reasonable accommodation if it does not cause an undue hardship. Businesses are encouraged to inform customers there are exceptions to the requirement that all individuals must wear a mask.\n",
      "These frequently asked questions are to provide guidance regarding the application of the face covering requirement in Executive Order 2021-10 for businesses and other places of public accommodation subject to Article 5 of the Illinois Human Rights Act, 775 ILCS 5/. A: A face covering is a mask or cloth face covering that covers your nose and mouth.\n",
      "Exceptions may be made for individuals with medical conditions or disabilities that prevent them from safely wearing a face covering. For more information, refer to the questions on reasonable accommodations. A: Masks still must be worn by everyone on planes, buses, trains, and other forms of public transportation; in transportation hubs, such as airports and train and bus stations; in health care settings; and in congregate facilities, such as correctional facilities and homeless shelters.\n",
      "These frequently asked questions are to provide guidance regarding the application of the face covering requirement in Executive Order 2021-10 for businesses and other places of public accommodation subject to Article 5 of the Illinois Human Rights Act, 775 ILCS 5/. When Face Coverings are Required Q: What does it mean to wear a face covering?\n",
      "Information 5:\n",
      "Publication date: 2020-04-22\n",
      "Title: Facing Your Face Mask Duties – A List of Statewide Orders | Littler ...\n",
      "Content:\n",
      "********************************************************NOTE: Given the reduction in activity on this topic,THIS POST WILL NO LONGER BE UPDATED, as of November 10, 2022.********************************************************Governors and public health officials across the country implemented stringent mitigation measures to help contain the spread of COVID-19.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12192,\n",
       " [{'query': 'Below is a CLAIM and the PRIOR KNOWLEDGE associated with it. Please classify the CLAIM as TRUE or FALSE based on the PRIOR KNOWLEDGE. If the content described by the CLAIM is correct, then classify it as TRUE; if the content described by the CLAIM is incorrect, then classify it as FALSE.\\n\\nCLAIM: Illinois Strengthens Face Mask Rules in Businesses\\nPublication date: 2020-08-13\\n\\nPRIOR KNOWLEDGE:\\nInformation 1:\\nPublication date: 2013-08-06\\nTitle: Illinois Strengthens Face Mask Rules in Businesses - WebMD\\nContent:\\nCOVID-19 is a new type of coronavirus that causes mild to severe cases. Here’s a quick guide on how to spot symptoms, risk factors, prevent spread of the disease, and find out what to do if you think you have it.\\nInformation 2:\\nPublication date: None\\nTitle: press-release\\nContent:\\nIllinoisans can resume activities without wearing a mask indoors on February 28th except where required by federal, state, local, tribal, or territorial laws, rules, and regulations, including local business and workplace guidance. Federal requirements, in effect through at least March 18, include all transportation systems such as airports, planes, trains, and buses. \"Preparing to repeal statewide masking mandates at the end of the month is aggressive and optimistic but reasonable,\" said Dr.\\n\"Broad mandates are not about individuals. They are put in place to help protect communities, businesses, and healthcare access. Repealing the mask mandate allows people to choose the mitigation layers that are best for them and I have no doubt that many should and will choose to keep mask rules.\"\\nIf these trends continue — and we expect them to —then on Monday, February 28th, we will lift the indoor mask requirement for the State of Illinois,\" said Governor JB Pritzker. \"I want to be clear: Many local jurisdictions, businesses and organizations have their own mask requirements and other mitigations that must be respected.\\nSeth Trueger, a Northwestern emergency physician who is also immunocompromised. \"Masking has helped slow the spread even in the face of omicron\\'s transmissibility. We can and must use this time to further increase vaccination uptake & outreach, especially among children and other populations with low vaccination rates, so when the next wave comes, we will be even better prepared.\"\\nInformation 3:\\nPublication date: None\\nTitle: Mask and Vaccine Requirements FAQ\\'s\\nContent:\\nOn September 3, 2021, the Governor signed Executive Order 21-22 which requires all individuals over the age of 2 and who can medically tolerate a face covering to wear a face covering when in indoor public places. The Executive Order also requires health care workers, school personnel, higher education personnel and students, and employees and contractors of state-owned or operated congregate facilities to be fully vaccinated, as described in the Order\\nInformation 4:\\nPublication date: None\\nTitle: FAQ for Businesses Concerning Use of Face-Coverings During COVID-19\\nContent:\\nA: Businesses reserve the right to refuse service to persons unable to comply with the requirement to wear a face covering, but they are required to provide a reasonable accommodation if it does not cause an undue hardship. Businesses are encouraged to inform customers there are exceptions to the requirement that all individuals must wear a mask.\\nThese frequently asked questions are to provide guidance regarding the application of the face covering requirement in Executive Order 2021-10 for businesses and other places of public accommodation subject to Article 5 of the Illinois Human Rights Act, 775 ILCS 5/. A: A face covering is a mask or cloth face covering that covers your nose and mouth.\\nExceptions may be made for individuals with medical conditions or disabilities that prevent them from safely wearing a face covering. For more information, refer to the questions on reasonable accommodations. A: Masks still must be worn by everyone on planes, buses, trains, and other forms of public transportation; in transportation hubs, such as airports and train and bus stations; in health care settings; and in congregate facilities, such as correctional facilities and homeless shelters.\\nThese frequently asked questions are to provide guidance regarding the application of the face covering requirement in Executive Order 2021-10 for businesses and other places of public accommodation subject to Article 5 of the Illinois Human Rights Act, 775 ILCS 5/. When Face Coverings are Required Q: What does it mean to wear a face covering?\\nInformation 5:\\nPublication date: 2020-04-22\\nTitle: Facing Your Face Mask Duties – A List of Statewide Orders | Littler ...\\nContent:\\n********************************************************NOTE: Given the reduction in activity on this topic,THIS POST WILL NO LONGER BE UPDATED, as of November 10, 2022.********************************************************Governors and public health officials across the country implemented stringent mitigation measures to help contain the spread of COVID-19.\\n',\n",
       "   'response': 'TRUE.'},\n",
       "  {'query': \"Below is a CLAIM and the PRIOR KNOWLEDGE associated with it. Please classify the CLAIM as TRUE or FALSE based on the PRIOR KNOWLEDGE. If the content described by the CLAIM is correct, then classify it as TRUE; if the content described by the CLAIM is incorrect, then classify it as FALSE.\\n\\nCLAIM: The government of Paraíba, a state in Brazil, received 11.6 million Brazilian reais from the federal government to spend on healthcare during the COVID-19 crisis. Instead, this money is being spent on artistic performances.\\nPublication date: 2020-04-03\\n\\nPRIOR KNOWLEDGE:\\nInformation 1:\\nPublication date: None\\nTitle: Brazil Overview: Development news, research, data | World Bank\\nContent:\\n<strong>Brazil</strong> <strong>is</strong> projected to fall into its deepest recession on record. However, macroeconomic framework is expected to remain broadly adequate, albeit with substantial downside risks, calling for strong fiscal consolidation and adoption of structural reforms.\\nInformation 2:\\nPublication date: 2023-12-30\\nTitle: Brazilian real - Wikipedia\\nContent:\\nLula started his government in 01/01/2003 with an exchange rate of US$1=R$3.52 and finished it in 12/31/2010 with an exchange rate of US$1=R$1.66. The exchange rate as of September 2015 was US$1=R$4.05. After a period of gradual recovery, it reached US$1=R$3 by February 2017. Jair Bolsonaro's tenure, initially welcomed with enthusiasm by the financial markets, started with US$1=R$3.86. Fueled by meager results of the economy, quick disenchantment followed, resulting in lack of foreign investments and real's strong depreciation. On 13 May 2020, during the COVID-19 pandemic, which deeply affected Brazil, the real reached a historical low against the US dollar, being negotiated at US$1=R$5.90.\\nIn the following years, the currency's value against the dollar followed an erratic but mostly downwards path from 1999 until late 2002, when the prospect of the election of leftist candidate Luiz Inácio Lula da Silva, considered a radical populist by sectors of the financial markets, prompted another currency crisis and a spike in inflation. Many Brazilians feared another default on government debts or a resumption of heterodox economic policies, and rushed to exchange their reais into tangible assets or foreign currencies.\\nThe Central Bank of Brazil is the central bank and the issuing authority. The real replaced the cruzeiro real in 1994. As of April 2019, the real was the twentieth most traded currency. Currencies in use before the current real include: The Portuguese real from the 16th to 18th centuries, with 1,000 réis called the milréis. The old Brazilian real from 1747 to 1942, with 1,000 réis also called the milréis.\\nInformation 3:\\nPublication date: 2023-08-23\\nTitle: Brazil - United States Department of State\\nContent:\\nU.S. foreign direct investment in Brazil totaled $191.6 billion in 2021, by far the most of any country. The United States and Brazil conduct regular government-to-government exchanges on topics including trade facilitation, good regulatory practices, and environmental and labor standards.\\nSince June, 2021 the United States has donated 5,187,300 safe and effective COVID-19 vaccine doses with the people of Brazil. This includes 3,000,000 J&J and 2,187,300 AstraZeneca doses. Of the 5,187,300 vaccine doses, 100% were donated through bilateral agreements. The United States is committed to leading an international and coordinated effort to accelerate access to safe and effective COVID-19 vaccines to meet global needs.\\nThe United States is committed to leading an international and coordinated effort to accelerate access to safe and effective COVID-19 vaccines to meet global needs. The United States is working with other governments and partners including COVAX, Caricom, and the African Vaccine Acquisition Trust (AVAT) to protect communities from COVID-19 and apply lessons from this pandemic to enhance health security now and in the future.\\nThe United States is working with other governments and partners including COVAX, Caricom, and the African Vaccine Acquisition Trust (AVAT) to protect communities from COVID-19 and apply lessons from this pandemic to enhance health security now and in the future. Learn more about our work Delivering Vaccines and on COVID-19 Recovery.\\nInformation 4:\\nPublication date: None\\nTitle: 1 BRL to USD - Brazilian Reais to US Dollars Exchange Rate\\nContent:\\nGet the latest 1 <strong>Brazilian</strong> Real to US Dollar rate for FREE with the original Universal Currency Converter. Set rate alerts for BRL to USD and learn more about <strong>Brazilian</strong> <strong>Reais</strong> and US Dollars from XE - the Currency Authority.\\nInformation 5:\\nPublication date: None\\nTitle: Brazil’s Open Budget Transparency Portal\\nContent:\\nRead More ... As with many of the countries included in this series, Brazil has long suffered from corruption. The country is ranked 69 among 175 countries in Transparency International’s 2014 Corruption Perceptions Index, which also reports that the country’s legal system is “plagued with inefficiencies and corrupt judges.”1 Brazilian elections are also believed to be infiltrated by corruption and, despite public outrage, politicians with prior convictions are often voted back to office.\\nThe country is ranked 69 among 175 countries in Transparency International’s 2014 Corruption Perceptions Index, which also reports that the country’s legal system is “plagued with inefficiencies and corrupt judges.”1 Brazilian elections are also believed to be infiltrated by corruption and, despite public outrage, politicians with prior convictions are often voted back to office. In Rio de Janeiro, for example, Brazil’s third-largest state with a population of 11.9 million as of 20122, “only one single politician in the Brazilian state of Rio de Janeiro is said to have never fallen foul of the law,” according to one news report.3\\nIn Rio de Janeiro, for example, Brazil’s third-largest state with a population of 11.9 million as of 20122, “only one single politician in the Brazilian state of Rio de Janeiro is said to have never fallen foul of the law,” according to one news report.3 · Additionally, a 2009 World Bank and IFC1 Enterprise Survey found that 70 percent of global and domestic companies viewed corruption as a “major constraint to doing business” in Brazil, while a Federation of the Industries of the state of São Paulo investigation found that corruption cost Brazil almost $40 billion (about 2.3 percent of GDP) in 2008 alone.4\\n6Savarese, Mauricio. “No End in Sight for Brazil’s Petrobras Scandal.” Americas Society / Council of the Americans. August 5, 2015. http://www.as-coa.org/articles/no-end-sight-brazil’s-petrobras-scandal. ... 8Brazil: New Access to Information Law becomes effective today.” Article 19. May 16, 2012. https://www.article19.org/resources.php/resource/3208/en/brazil:-new-access-to-information-law-becomes-effective-today. 9New Brazilian Data Portal Dados.Gov.Br – Powered by CKAN.” Open Knowledge Blog.\\n\",\n",
       "   'response': 'FALSE.'}])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_list = []\n",
    "\n",
    "data_version = \"3\"\n",
    "for i, item in enumerate(data_search_llm):\n",
    "    \n",
    "    if int(data_train[i][\"label\"]) != 1:\n",
    "        prompt = prompt_rag.get_prompt_with_prior_knowledge(\n",
    "            data_train[i][\"claim\"], \n",
    "            search_engine,\n",
    "            data_search[i][f\"{search_engine}_search_results\"], \n",
    "            item[f\"prior_knowledge_{model_name}_v{prior_knowledge_version}_K={K}\"], \n",
    "            K=K,\n",
    "            claim_date=data_train[i][\"date\"],\n",
    "            known_info=True, \n",
    "            rag_info=False,\n",
    "            ids=None\n",
    "        )\n",
    "        label = \"TRUE.\" if int(data_train[i][\"label\"]) == 2 else \"FALSE.\"\n",
    "        dict_list.append({\"query\": prompt, \"response\": label})\n",
    "print(dict_list[0][\"query\"])\n",
    "len(dict_list), dict_list[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM_dir = \"/home/hanlv/workspace/code/research/infodemic/LLM/\"\n",
    "# with jsonlines.open(\n",
    "#     LLM_dir + \\\n",
    "#     f\"swift/examples/pytorch/llm/my_data/with_{model_name}_info/{search_engine}/data{data_version}.jsonl\", mode=\"w\") as file_jsonl:\n",
    "#     for line in dict_list:\n",
    "#         file_jsonl.write(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试模型的先验知识生成效果：一次提问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Successfully registered `/home/hanlv/workspace/code/research/infodemic/LLM/swift/swift/llm/data/dataset_info.json`\n",
      "[INFO:swift] Loading the model using model_dir: /home/css/models/llama-3-70b-instruct-awq\n",
      "[INFO:swift] model_config: LlamaConfig {\n",
      "  \"_name_or_path\": \"/home/css/models/llama-3-70b-instruct-awq\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 8192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 28672,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_hidden_layers\": 80,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"bits\": 4,\n",
      "    \"group_size\": 128,\n",
      "    \"modules_to_not_convert\": null,\n",
      "    \"quant_method\": \"awq\",\n",
      "    \"version\": \"gemm\",\n",
      "    \"zero_point\": true\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-17 15:37:21 config.py:244] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 07-17 15:37:21 config.py:698] Defaulting to use mp for distributed inference\n",
      "INFO 07-17 15:37:21 llm_engine.py:169] Initializing an LLM engine (v0.5.1) with config: model='/home/css/models/llama-3-70b-instruct-awq', speculative_config=None, tokenizer='/home/css/models/llama-3-70b-instruct-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=42, served_model_name=/home/css/models/llama-3-70b-instruct-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m INFO 07-17 15:37:24 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 07-17 15:37:24 utils.py:741] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m INFO 07-17 15:37:24 utils.py:741] Found nccl from library libnccl.so.2\n",
      "INFO 07-17 15:37:24 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m INFO 07-17 15:37:24 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method load_model: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU \u0001 has a total capacity of 23.65 GiB of which 725.12 MiB is free. Process 1776804 has 4.78 GiB memory in use. Including non-PyTorch memory, this process has 18.15 GiB memory in use. Of the allocated memory 17.57 GiB is allocated by PyTorch, and 16.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables), Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]   File \"/home/hanlv/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]   File \"/home/hanlv/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/worker/worker.py\", line 133, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]     self.model_runner.load_model()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]   File \"/home/hanlv/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 243, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]     self.model = get_model(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]                  ^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]   File \"/home/hanlv/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py\", line 21, in get_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]     return loader.load_model(model_config=model_config,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]   File \"/home/hanlv/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 267, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]     model = _initialize_model(model_config, self.load_config,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]   File \"/home/hanlv/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 104, in _initialize_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]     return model_class(config=model_config.hf_config,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]   File \"/home/hanlv/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 375, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]     self.lm_head = ParallelLMHead(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]                    ^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]   File \"/home/hanlv/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\", line 386, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]     super().__init__(num_embeddings, embedding_dim, params_dtype,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]   File \"/home/hanlv/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\", line 218, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]     self.linear_method.create_weights(self,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]   File \"/home/hanlv/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 117, in create_weights\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]     weight = Parameter(torch.empty(sum(output_partition_sizes),\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]   File \"/home/hanlv/miniconda3/envs/swift/lib/python3.11/site-packages/torch/utils/_device.py\", line 78, in __torch_function__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226] torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU \u0001 has a total capacity of 23.65 GiB of which 725.12 MiB is free. Process 1776804 has 4.78 GiB memory in use. Including non-PyTorch memory, this process has 18.15 GiB memory in use. Of the allocated memory 17.57 GiB is allocated by PyTorch, and 16.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1926441)\u001b[0;0m ERROR 07-17 15:37:24 multiproc_worker_utils.py:226] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m model_type \u001b[38;5;241m=\u001b[39m CustomModelType\u001b[38;5;241m.\u001b[39mllama_3_70b_instruct_awq\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# model_type = CustomModelType.mixtral_moe_7b_instruct_awq\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# model_type = CustomModelType.solar_instruct_10_7b\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m llm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mget_vllm_engine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# torch_dtype=torch.float16,  # 检查正确的数据类型！！！！\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# gpu_memory_utilization=0.92,\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# model_id_or_path=\"/home/css/models/Mixtral-8x7B-Instruct-v0.1-GPTQ-int4\",\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# \"enforce_eager\": True,\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_num_seqs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 64\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m template_type \u001b[38;5;241m=\u001b[39m get_default_template_type(model_type)\n\u001b[1;32m     38\u001b[0m template \u001b[38;5;241m=\u001b[39m get_template(template_type, llm_engine\u001b[38;5;241m.\u001b[39mhf_tokenizer)\n",
      "File \u001b[0;32m~/workspace/code/research/infodemic/LLM/swift/swift/llm/utils/vllm_utils.py:121\u001b[0m, in \u001b[0;36mget_vllm_engine\u001b[0;34m(model_type, torch_dtype, model_id_or_path, revision, gpu_memory_utilization, tensor_parallel_size, max_model_len, disable_custom_all_reduce, enforce_eager, engine_kwargs, use_async, enable_lora, max_loras, max_lora_rank, image_input_shape, image_feature_size, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(vllm\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0.5.1\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    119\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVLLM_WORKER_MULTIPROC_METHOD\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 121\u001b[0m llm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mllm_engine_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m llm_engine\u001b[38;5;241m.\u001b[39mengine_args \u001b[38;5;241m=\u001b[39m engine_args\n\u001b[1;32m    123\u001b[0m llm_engine\u001b[38;5;241m.\u001b[39mmodel_dir \u001b[38;5;241m=\u001b[39m model_dir\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/engine/llm_engine.py:414\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context)\u001b[0m\n\u001b[1;32m    411\u001b[0m     executor_class \u001b[38;5;241m=\u001b[39m GPUExecutor\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 414\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/engine/llm_engine.py:243\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, multimodal_config, speculative_config, decoding_config, observability_config, executor_class, log_stats, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config_fields \u001b[38;5;241m=\u001b[39m _load_generation_config_dict(\n\u001b[1;32m    238\u001b[0m     model_config)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_processor \u001b[38;5;241m=\u001b[39m INPUT_REGISTRY\u001b[38;5;241m.\u001b[39mcreate_input_processor(\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config)\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultimodal_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultimodal_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeculative_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeculative_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39membedding_mode:\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_kv_caches()\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/executor/distributed_gpu_executor.py:25\u001b[0m, in \u001b[0;36mDistributedGPUExecutor.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Updated by implementations that require additional args to be passed\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# to the _run_workers execute_model call\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_execute_model_run_workers_kwargs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/executor/executor_base.py:42\u001b[0m, in \u001b[0;36mExecutorBase.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, multimodal_config, speculative_config)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultimodal_config \u001b[38;5;241m=\u001b[39m multimodal_config\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeculative_config \u001b[38;5;241m=\u001b[39m speculative_config\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/executor/multiproc_gpu_executor.py:79\u001b[0m, in \u001b[0;36mMultiprocessingGPUExecutor._init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_worker(\n\u001b[1;32m     77\u001b[0m     distributed_init_method\u001b[38;5;241m=\u001b[39mdistributed_init_method)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_workers(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_device\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mload_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmax_concurrent_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\n\u001b[1;32m     81\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmax_parallel_loading_workers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/executor/multiproc_gpu_executor.py:130\u001b[0m, in \u001b[0;36mMultiprocessingGPUExecutor._run_workers\u001b[0;34m(self, method, async_run_tensor_parallel_workers_only, max_concurrent_workers, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m worker_outputs\n\u001b[1;32m    129\u001b[0m driver_worker_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker, method)\n\u001b[0;32m--> 130\u001b[0m driver_worker_output \u001b[38;5;241m=\u001b[39m \u001b[43mdriver_worker_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Get the results of the workers.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [driver_worker_output\n\u001b[1;32m    134\u001b[0m         ] \u001b[38;5;241m+\u001b[39m [output\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m worker_outputs]\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/worker/worker.py:133\u001b[0m, in \u001b[0;36mWorker.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/worker/model_runner.py:243\u001b[0m, in \u001b[0;36mGPUModelRunnerBase.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m CudaMemoryProfiler() \u001b[38;5;28;01mas\u001b[39;00m m:\n\u001b[0;32m--> 243\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmultimodal_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultimodal_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mconsumed_memory\n\u001b[1;32m    255\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model weights took \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m GB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    256\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py:21\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(model_config, load_config, device_config, parallel_config, scheduler_config, lora_config, multimodal_config, cache_config)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model\u001b[39m(\u001b[38;5;241m*\u001b[39m, model_config: ModelConfig, load_config: LoadConfig,\n\u001b[1;32m     15\u001b[0m               device_config: DeviceConfig, parallel_config: ParallelConfig,\n\u001b[1;32m     16\u001b[0m               scheduler_config: SchedulerConfig,\n\u001b[1;32m     17\u001b[0m               lora_config: Optional[LoRAConfig],\n\u001b[1;32m     18\u001b[0m               multimodal_config: Optional[MultiModalConfig],\n\u001b[1;32m     19\u001b[0m               cache_config: CacheConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m nn\u001b[38;5;241m.\u001b[39mModule:\n\u001b[1;32m     20\u001b[0m     loader \u001b[38;5;241m=\u001b[39m get_model_loader(load_config)\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mmultimodal_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultimodal_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py:270\u001b[0m, in \u001b[0;36mDefaultModelLoader.load_model\u001b[0;34m(self, model_config, device_config, lora_config, multimodal_config, parallel_config, scheduler_config, cache_config)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(device_config\u001b[38;5;241m.\u001b[39mdevice):\n\u001b[1;32m    267\u001b[0m     model \u001b[38;5;241m=\u001b[39m _initialize_model(model_config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_config,\n\u001b[1;32m    268\u001b[0m                               lora_config, multimodal_config,\n\u001b[1;32m    269\u001b[0m                               cache_config)\n\u001b[0;32m--> 270\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_weights_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mfall_back_to_pt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfall_back_to_pt_during_load\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, module \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_modules():\n\u001b[1;32m    279\u001b[0m     quant_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquant_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/models/llama.py:461\u001b[0m, in \u001b[0;36mLlamaForCausalLM.load_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    459\u001b[0m     param \u001b[38;5;241m=\u001b[39m params_dict[name]\n\u001b[1;32m    460\u001b[0m     weight_loader \u001b[38;5;241m=\u001b[39m param\u001b[38;5;241m.\u001b[39mweight_loader\n\u001b[0;32m--> 461\u001b[0m     \u001b[43mweight_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py:657\u001b[0m, in \u001b[0;36mQKVParallelLinear.weight_loader\u001b[0;34m(self, param, loaded_weight, loaded_shard_id)\u001b[0m\n\u001b[1;32m    651\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading a weight without `output_dim` attribute in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    653\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQKVParallelLinear, assume the weight is the same \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    654\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor all partitions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m param_data\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m loaded_weight\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 657\u001b[0m \u001b[43mparam_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "dirs = [\"../..\"]\n",
    "for _dir in dirs:\n",
    "    if _dir not in sys.path:\n",
    "        sys.path.append(_dir)\n",
    "\n",
    "from swift.llm import (\n",
    "    ModelType, get_vllm_engine, get_default_template_type,\n",
    "    get_template, inference_vllm, VllmGenerationConfig\n",
    ")\n",
    "from custom import CustomModelType, CustomTemplateType\n",
    "\n",
    "\n",
    "# model_type = CustomModelType.phi_3_medium_4k_instruct\n",
    "model_type = CustomModelType.llama_3_70b_instruct_awq\n",
    "# model_type = CustomModelType.mixtral_moe_7b_instruct_awq\n",
    "# model_type = CustomModelType.solar_instruct_10_7b\n",
    "\n",
    "llm_engine = get_vllm_engine(\n",
    "    model_type, \n",
    "    # torch_dtype=torch.float16,  # 检查正确的数据类型！！！！\n",
    "    tensor_parallel_size=2,\n",
    "    max_model_len=4096,\n",
    "    # gpu_memory_utilization=0.92,\n",
    "    # model_id_or_path=\"/home/css/models/Mixtral-8x7B-Instruct-v0.1-GPTQ-int4\",\n",
    "    engine_kwargs = {\n",
    "        # \"enforce_eager\": True,\n",
    "        \"max_num_seqs\": 64, # 64\n",
    "        \"seed\": 42,\n",
    "    }\n",
    ")\n",
    "\n",
    "template_type = get_default_template_type(model_type)\n",
    "template = get_template(template_type, llm_engine.hf_tokenizer)\n",
    "\n",
    "generation_config = VllmGenerationConfig(\n",
    "    max_new_tokens=2048,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "get_resp_list = lambda request_list : inference_vllm(\n",
    "    llm_engine, template, request_list, \n",
    "    generation_config=generation_config, \n",
    "    # use_tqdm=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 1560.44it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  9.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI, I am a program designed to simulate understanding and provide helpful responses. I am functioning properly and ready to assist you with your needs. How about you, how are you doing today?\n",
      "As an AI, I am a program designed to simulate understanding and provide helpful responses. I am functioning properly and ready to assist you with your needs. How about you, how are you doing today?\n",
      "As an AI, I am a program designed to simulate understanding and provide helpful responses. I am functioning properly and ready to assist you with your needs. How about you, how are you doing today?\n",
      "As an AI, I am a program designed to simulate understanding and provide helpful responses. I am functioning properly and ready to assist you with your needs. How about you, how are you doing today?\n",
      "As an AI, I am a program designed to simulate understanding and provide helpful responses. I am functioning properly and ready to assist you with your needs. How about you, how are you doing today?\n",
      "As an AI, I am a program designed to simulate understanding and provide helpful responses. I am functioning properly and ready to assist you with your needs. How about you, how are you doing today?\n",
      "As an AI, I am a program designed to simulate understanding and provide helpful responses. I am functioning properly and ready to assist you with your needs. How about you, how are you doing today?\n",
      "As an AI, I am a program designed to simulate understanding and provide helpful responses. I am functioning properly and ready to assist you with your needs. How about you, how are you doing today?\n",
      "As an AI, I am a program designed to simulate understanding and provide helpful responses. I am functioning properly and ready to assist you with your needs. How about you, how are you doing today?\n",
      "As an AI, I am a program designed to simulate understanding and provide helpful responses. I am functioning properly and ready to assist you with your needs. How about you, how are you doing today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# get_resp_list = lambda request_list : inference_vllm(\n",
    "#     llm_engine, template, request_list, \n",
    "#     generation_config=generation_config, \n",
    "#     use_tqdm=False, \n",
    "#     verbose=True, prompt_prefix=\"\", output_prefix=\"\"\n",
    "# )\n",
    "\n",
    "s = \"Information Summary:\\n\\nInformation 1, published on 2020-12-01, discusses a false claim about a Covid-19 vaccine having the ability to \\\"transfer genetic material\\\" and manipulate human genes. The claim was shared hundreds of times on Facebook, but experts, such as Dr. Kirsty Short, have confirmed that the mRNA vaccine cannot enter the human genome. Information 2, published on 2020-05-19, mentions false claims about a future Covid-19 vaccine genetically modifying humans. Information 3, with no specific publication date, states that Covid-19 vaccines do not change a person's genes and use messenger RNA or modified adenovirus to trigger an immune response. Information 4, published on 2023-11-01, reiterates that Covid-19 vaccines cannot alter a person's genome and debunks false claims about DNA contamination leading to harmful effects. Information 5, with no specific publication date, highlights the importance of monitoring and addressing vaccine misinformation to prevent vaccine hesitancy.\\n\\nRestated Claim:\\nOn 2020-12-01, experts refuted the false claim that a Covid-19 vaccine can manipulate human genes.\\n\\nGiven the information available, the claim is TRUE. The Covid-19 vaccines developed by Pfizer-BioNTech, Moderna, and Johnson & Johnson do not have the ability to manipulate human genes, as confirmed by various experts and sources. These vaccines either use messenger RNA or modified adenovirus to trigger an immune response, but they cannot alter human DNA.\"\n",
    "\n",
    "prompt_list = [\"how are you\"] * 10\n",
    "\n",
    "resp_list = get_resp_list([{'query': prompt} for prompt in prompt_list])\n",
    "\n",
    "for resp in resp_list:\n",
    "    print(resp[\"response\"])\n",
    "\n",
    "# print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_prompt_for_generating_prior_knowledge(\n",
    "#         claim, claim_date, search_engine, search_results, model_name,\n",
    "#         K=5, sort=False, ids=None, without_info=False, without_claim_date=False):\n",
    "#     \"\"\"\n",
    "#     sort: 对search result 按时间进行排序\n",
    "#     \"\"\"\n",
    "\n",
    "#     claim = claim.strip()\n",
    "\n",
    "#     if model_name == \"solar\":\n",
    "#         pre = \"Below is some INFORMATION searched online and a CLAIM. These pieces of INFORMATION are relevant to the CLAIM. This CLAIM and all INFORMATION include their respective publication dates and contents. To classify the CLAIM more accurately (if the content described by the CLAIM is correct, it will be classified as TRUE; if the content described by the CLAIM is incorrect, it will be classified as FALSE), please first expand on the given INFORMATION and provide a detailed summary of it. Then analyze, reason, and provide reasonable evidence to judge the correctness of the CLAIM based on the available information and your knowledge, and finally generate prior knowledge that helps classify the CLAIM.\\n\\n\"\n",
    "#     elif model_name == \"mixtral\":\n",
    "#         # v1\n",
    "#         # pre = \"Below is a CLAIM and some INFORMATION searched online. These pieces of INFORMATION are relevant to the CLAIM. This CLAIM and all INFORMATION include their respective publication dates and contents. To classify the CLAIM more accurately (if the content described by the CLAIM is correct, it will be classified as TRUE; if the content described by the CLAIM is incorrect, it will be classified as FALSE), please first provide a detailed summary of the given INFORMATION and restate the CLAIM. Then reason, and provide reasonable evidence to judge the correctness of the CLAIM based on the available information and your knowledge. In reasoning, it is necessary to consider the sequential relationship between the date of publication of the CLAIM and the date of publication of the INFORMATION.\\n\\n\"\n",
    "#         # v2\n",
    "#         # pre = \"Below is some INFORMATION searched online and a CLAIM. These pieces of INFORMATION are relevant to the CLAIM. This CLAIM and all INFORMATION include their respective publication dates and contents. To classify the CLAIM more accurately (if the content described by the CLAIM is correct, it will be classified as TRUE; if the content described by the CLAIM is incorrect, it will be classified as FALSE), please first provide a detailed summary of the given INFORMATION and restate the CLAIM. Then reason, and provide reasonable evidence to judge the correctness of the CLAIM based on the available information and your knowledge.\\n\\n\"\n",
    "        \n",
    "#         pre = \"Below is some INFORMATION searched online and a CLAIM. These pieces of INFORMATION are relevant to the CLAIM. This CLAIM and all INFORMATION include their respective publication dates and contents. To classify the CLAIM more accurately (if the content described by the CLAIM is correct, it will be classified as TRUE; if the content described by the CLAIM is incorrect, it will be classified as FALSE), please first provide a clear summary of the given INFORMATION, restate the CLAIM, and provide reasonable evidence to judge the correctness of the CLAIM based on the available information and your knowledge.\\n\\n\"\n",
    "\n",
    "#     else:\n",
    "#         raise Exception(\"model_name 只能从solar，mixtral中选择\")\n",
    "    \n",
    "#     if without_claim_date:\n",
    "#         text = \"CLAIM: \" + claim\n",
    "#     else:\n",
    "#         text = \"CLAIM: \" + prompt_rag.get_claim_with_date(claim, claim_date)\n",
    "\n",
    "#     if search_engine == \"bing\":\n",
    "#         snippet = prompt_rag.get_bing_snippet_v2(search_results, K=K, claim_date=claim_date, sort=sort)\n",
    "#     elif search_engine == \"brave\":\n",
    "#         if ids is None:\n",
    "#             ids = slice(0, K)\n",
    "#         snippet = prompt_rag.get_brave_snippet(search_results, ids=ids)\n",
    "#     else:\n",
    "#         raise Exception(\"Select search engines in [\\\"bing\\\", \\\"brave\\\"].\")\n",
    "    \n",
    "#     info = \"INFORMATION:\\n\" + snippet + '\\n'\n",
    "\n",
    "#     if without_info:\n",
    "#         return (pre + text).strip()\n",
    "#     else:\n",
    "#         return pre + info + text\n",
    "\n",
    "# def get_claim_with_date(claim, claim_date=None):\n",
    "#     if claim_date is None:\n",
    "#         return \" \" + claim\n",
    "    \n",
    "#     # res = \"\\n\"\n",
    "#     res = claim + \"\\nPublication date: \" + claim_date\n",
    "#     return res\n",
    "\n",
    "# K = 5\n",
    "# def get_id(claim):\n",
    "#     for i in range(len(data_train)):\n",
    "#         if claim.strip() in data_train[i][\"claim\"].strip():\n",
    "#             return i\n",
    "\n",
    "# # i = 0\n",
    "# i = get_id(\"False claim circulates that Pakistani plane transported Sri Lankan students home after COVID-19 lockdown\")\n",
    "\n",
    "# # claim = data_search[i][\"claim\"]\n",
    "# search_results = data_search[i][f\"{search_engine}_search_results\"]\n",
    "\n",
    "# model_name = 'mixtral'\n",
    "# prompt_list1 = [\n",
    "#     get_prompt_for_generating_prior_knowledge(\n",
    "#         data_train[i][\"claim\"], data_train[i][\"date\"], search_engine, search_results, model_name,\n",
    "#         K=K, sort=False, \n",
    "#         # ids=data_search[i][\"random_ids\"],\n",
    "#         ids=None\n",
    "#     ),\n",
    "# ]\n",
    "\n",
    "# request_list1 = [{'query': prompt} for prompt in prompt_list1]\n",
    "\n",
    "# print(prompt_list1[0])\n",
    "# print()\n",
    "\n",
    "# resp_list1 = get_resp_list(request_list1)\n",
    "# print(resp_list1[0][\"response\"].strip())\n",
    "# print()\n",
    "# resp_list1[0][\"response\"].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:00<00:00, 181.34it/s]\n",
      " 19%|█▉        | 10/53 [00:44<03:05,  4.32s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 224\u001b[0m\n\u001b[1;32m    216\u001b[0m     prompt_list\u001b[38;5;241m.\u001b[39mappend(get_prompt_for_generating_prior_knowledge2(\n\u001b[1;32m    217\u001b[0m             data_train[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaim\u001b[39m\u001b[38;5;124m\"\u001b[39m], data_train[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m], search_engine, \n\u001b[1;32m    218\u001b[0m             search_results, model_name, K\u001b[38;5;241m=\u001b[39mK, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m    219\u001b[0m             \u001b[38;5;66;03m# ids=data_search[i][\"random_ids\"],\u001b[39;00m\n\u001b[1;32m    220\u001b[0m             ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    221\u001b[0m         ))\n\u001b[1;32m    222\u001b[0m request_list \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m: prompt} \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompt_list]\n\u001b[0;32m--> 224\u001b[0m resp_list \u001b[38;5;241m=\u001b[39m \u001b[43mget_resp_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# print(resp_list[0][\"response\"].strip())\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 46\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(request_list)\u001b[0m\n\u001b[1;32m     39\u001b[0m template \u001b[38;5;241m=\u001b[39m get_template(template_type, llm_engine\u001b[38;5;241m.\u001b[39mhf_tokenizer, model\u001b[38;5;241m=\u001b[39mllm_engine)\n\u001b[1;32m     41\u001b[0m generation_config \u001b[38;5;241m=\u001b[39m VllmGenerationConfig(\n\u001b[1;32m     42\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m,\n\u001b[1;32m     43\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     44\u001b[0m )\n\u001b[0;32m---> 46\u001b[0m get_resp_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m request_list : \u001b[43minference_vllm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_engine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     50\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/code/research/infodemic/LLM/swift/swift/llm/utils/vllm_utils.py:429\u001b[0m, in \u001b[0;36minference_vllm\u001b[0;34m(llm_engine, template, request_list, generation_config, lora_request, use_tqdm, verbose, prompt_prefix, output_prefix, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m step_outputs \u001b[38;5;241m=\u001b[39m llm_engine\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n\u001b[1;32m    430\u001b[0m         outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    431\u001b[0m         prog_bar\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/engine/llm_engine.py:776\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scheduler_outputs\u001b[38;5;241m.\u001b[39mis_empty():\n\u001b[1;32m    768\u001b[0m     execute_model_req \u001b[38;5;241m=\u001b[39m ExecuteModelRequest(\n\u001b[1;32m    769\u001b[0m         seq_group_metadata_list\u001b[38;5;241m=\u001b[39mseq_group_metadata_list,\n\u001b[1;32m    770\u001b[0m         blocks_to_swap_in\u001b[38;5;241m=\u001b[39mscheduler_outputs\u001b[38;5;241m.\u001b[39mblocks_to_swap_in,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    774\u001b[0m         running_queue_size\u001b[38;5;241m=\u001b[39mscheduler_outputs\u001b[38;5;241m.\u001b[39mrunning_queue_size,\n\u001b[1;32m    775\u001b[0m     )\n\u001b[0;32m--> 776\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    779\u001b[0m     output \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/executor/gpu_executor.py:91\u001b[0m, in \u001b[0;36mGPUExecutor.execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_model\u001b[39m(\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m, execute_model_req: ExecuteModelRequest\n\u001b[1;32m     90\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Union[SamplerOutput, PoolerOutput]]:\n\u001b[0;32m---> 91\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/worker/worker.py:280\u001b[0m, in \u001b[0;36mWorker.execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_seq_groups \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m--> 280\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# Worker only supports single-step execution. Wrap the output in a list\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# to conform to interface.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [output]\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/worker/model_runner.py:765\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, seq_group_metadata_list, kv_caches)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;66;03m# Sample the next token.\u001b[39;00m\n\u001b[0;32m--> 765\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/models/llama.py:386\u001b[0m, in \u001b[0;36mLlamaForCausalLM.sample\u001b[0;34m(self, logits, sampling_metadata)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    383\u001b[0m     logits: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    384\u001b[0m     sampling_metadata: SamplingMetadata,\n\u001b[1;32m    385\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[SamplerOutput]:\n\u001b[0;32m--> 386\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m next_tokens\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:96\u001b[0m, in \u001b[0;36mSampler.forward\u001b[0;34m(self, logits, sampling_metadata)\u001b[0m\n\u001b[1;32m     93\u001b[0m logprobs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Sample the next tokens.\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m sample_results, maybe_sampled_tokens_tensor \u001b[38;5;241m=\u001b[39m \u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodify_greedy_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_modify_greedy_probs_inplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_gpu_probs_tensor:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m maybe_sampled_tokens_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:655\u001b[0m, in \u001b[0;36m_sample\u001b[0;34m(probs, logprobs, sampling_metadata, sampling_tensors, include_gpu_probs_tensor, modify_greedy_probs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sample\u001b[39m(\n\u001b[1;32m    639\u001b[0m     probs: torch\u001b[38;5;241m.\u001b[39mTensor, logprobs: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    640\u001b[0m     sampling_metadata: SamplingMetadata, sampling_tensors: SamplingTensors,\n\u001b[1;32m    641\u001b[0m     include_gpu_probs_tensor: \u001b[38;5;28mbool\u001b[39m, modify_greedy_probs: \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m    642\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[SampleResultType, Optional[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m    643\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;124;03m        probs: (num_query_tokens_in_batch, num_vocab)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;124;03m        sampled_token_ids_tensor: A tensor of sampled token ids.\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sample_with_torch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodify_greedy_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodify_greedy_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:542\u001b[0m, in \u001b[0;36m_sample_with_torch\u001b[0;34m(probs, logprobs, sampling_metadata, include_gpu_probs_tensor, modify_greedy_probs)\u001b[0m\n\u001b[1;32m    540\u001b[0m (seq_group_id, seq_groups) \u001b[38;5;241m=\u001b[39m sample_metadata[sampling_type]\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sampling_type \u001b[38;5;241m==\u001b[39m SamplingType\u001b[38;5;241m.\u001b[39mGREEDY:\n\u001b[0;32m--> 542\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m \u001b[43m_greedy_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreedy_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;129;01min\u001b[39;00m (SamplingType\u001b[38;5;241m.\u001b[39mRANDOM, SamplingType\u001b[38;5;241m.\u001b[39mRANDOM_SEED):\n\u001b[1;32m    544\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m _random_sample(seq_groups,\n\u001b[1;32m    545\u001b[0m                                     multinomial_samples[sampling_type])\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:288\u001b[0m, in \u001b[0;36m_greedy_sample\u001b[0;34m(selected_seq_groups, samples)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_greedy_sample\u001b[39m(\n\u001b[1;32m    273\u001b[0m     selected_seq_groups: List[SequenceGroupToSample],\n\u001b[1;32m    274\u001b[0m     samples: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    275\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SampleResultType:\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run greedy sampling on a given samples.\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m        seq_group has do_sample=False, tuple contains ([], [])\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[43msamples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    290\u001b[0m     results: SampleResultType \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_prompt_for_generating_prior_knowledge2(\n",
    "        claim, claim_date, search_engine, search_results, model_name,\n",
    "        K=5, sort=False, ids=None, without_info=False, without_claim_date=False):\n",
    "    \"\"\"\n",
    "    pre + info + text\n",
    "    \"\"\"\n",
    "\n",
    "    claim = claim.strip()\n",
    "\n",
    "    if model_name == \"solar\":\n",
    "        pre = \"Below is some INFORMATION searched online and a CLAIM. These pieces of INFORMATION are relevant to the CLAIM. This CLAIM and all INFORMATION include their respective publication dates and contents. To classify the CLAIM more accurately (if the content described by the CLAIM is correct, it will be classified as TRUE; if the content described by the CLAIM is incorrect, it will be classified as FALSE), please first expand on the given INFORMATION and provide a detailed summary of it. Then analyze, reason, and provide reasonable evidence to judge the correctness of the CLAIM based on the available information and your knowledge, and finally generate prior knowledge that helps classify the CLAIM.\\n\\n\"\n",
    "        # pre = \"Below is some INFORMATION searched online and a CLAIM. These pieces of INFORMATION are relevant to the CLAIM. This CLAIM and all INFORMATION include their respective publication dates and contents. To classify the CLAIM more accurately (if the content described by the CLAIM is correct, it will be classified as TRUE; if the content described by the CLAIM is incorrect, it will be classified as FALSE), please first provide a clear summary of the given INFORMATION, and provide reasonable evidence to judge the correctness of the CLAIM based on the available information and your knowledge.\\n\\n\"\n",
    "    elif model_name == \"mixtral\":\n",
    "        # pre = \"Below is a CLAIM and some INFORMATION searched online. These pieces of INFORMATION are relevant to the CLAIM. This CLAIM and all INFORMATION include their respective publication dates and contents. To classify the CLAIM more accurately (if the content described by the CLAIM is correct, it will be classified as TRUE; if the content described by the CLAIM is incorrect, it will be classified as FALSE), please first provide a detailed summary of the given INFORMATION and restate the CLAIM. Then reason, and provide reasonable evidence to judge the correctness of the CLAIM based on the available information and your knowledge. In reasoning, it is necessary to consider the sequential relationship between the date of publication of the CLAIM and the date of publication of the INFORMATION.\\n\\n\"\n",
    "        pre = \"Below is some INFORMATION searched online and a CLAIM. These pieces of INFORMATION are relevant to the CLAIM. This CLAIM and all INFORMATION include their respective publication dates and contents. To classify the CLAIM more accurately (if the content described by the CLAIM is correct, it will be classified as TRUE; if the content described by the CLAIM is incorrect, it will be classified as FALSE), please first provide a clear summary of the given INFORMATION, and provide reasonable evidence to judge the correctness of the CLAIM based on the available information and your knowledge.\\n\\n\"\n",
    "        \n",
    "        # pre = \"Below is some INFORMATION searched online and a <CLAIM>. These pieces of INFORMATION are relevant to the <CLAIM>. This <CLAIM> and all INFORMATION include their respective publication dates and contents. To classify the <CLAIM> more accurately (if the content described by the <CLAIM> is correct, it will be classified as TRUE; if the content described by the <CLAIM> is incorrect, it will be classified as FALSE), please first provide a clear summary of the given INFORMATION, and provide reasonable evidence to judge the correctness of the <CLAIM> based on the available information and your knowledge.\\n\\n\"\n",
    "    \n",
    "    elif model_name == \"llama3\":\n",
    "        # pre = \"Below is some INFORMATION searched online and a <CLAIM>. These pieces of INFORMATION are relevant to the <CLAIM>. This <CLAIM> and all INFORMATION include their respective publication dates and contents. To classify the <CLAIM> more accurately (if the content described by the <CLAIM> is correct, it will be classified as TRUE; if the content described by the <CLAIM> is incorrect, it will be classified as FALSE), please first provide a clear summary of the given INFORMATION, restate the <CLAIM>, and provide reasonable evidence to judge the correctness of the <CLAIM> based on the available information and your knowledge.\\n\\n\"\n",
    "        pre = \"Below is some INFORMATION searched online and a <CLAIM>. These pieces of INFORMATION are relevant to the <CLAIM>. This <CLAIM> and all INFORMATION include their respective publication dates and contents. To classify the <CLAIM> more accurately (if the content described by the <CLAIM> is correct, it will be classified as TRUE; if the content described by the <CLAIM> is incorrect, it will be classified as FALSE), please first provide a clear summary of the given INFORMATION, and provide reasonable evidence to judge the correctness of the <CLAIM> based on the available information and your knowledge.\\n\\n\"\n",
    "    elif model_name == \"phi3\":\n",
    "        # CLAIM\n",
    "        pre = \"Below is some INFORMATION searched online and a CLAIM. These pieces of INFORMATION are relevant to the CLAIM. This CLAIM and all INFORMATION include their respective publication dates and contents. To classify the CLAIM more accurately (if the content described by the CLAIM is correct, it will be classified as TRUE; if the content described by the CLAIM is incorrect, it will be classified as FALSE), please first provide a clear summary of the given INFORMATION, and provide reasonable evidence to judge the correctness of the CLAIM based on the available information and your knowledge.\\n\\n\"\n",
    "        \n",
    "        # <CLAIM>\n",
    "        # pre = \"Below is some INFORMATION searched online and a <CLAIM>. These pieces of INFORMATION are relevant to the <CLAIM>. This <CLAIM> and all INFORMATION include their respective publication dates and contents. To classify the <CLAIM> more accurately (if the content described by the <CLAIM> is correct, it will be classified as TRUE; if the content described by the <CLAIM> is incorrect, it will be classified as FALSE), please first provide a clear summary of the given INFORMATION, and provide reasonable evidence to judge the correctness of the <CLAIM> based on the available information and your knowledge.\\n\\n\"\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"model_name 只能从solar，mixtral中选择\")\n",
    "    \n",
    "    if without_claim_date:\n",
    "        text = \"CLAIM: \" + claim\n",
    "    else:\n",
    "        if model_name == \"solar\":\n",
    "            text = \"CLAIM: \"\n",
    "        if model_name == \"mixtral\":\n",
    "            text = \"CLAIM: \"\n",
    "        elif model_name == \"llama3\":\n",
    "            text = \"<CLAIM>: \"\n",
    "        elif model_name == \"phi3\":\n",
    "            text = \"CLAIM: \"\n",
    "        text += get_claim_with_date(claim, claim_date)\n",
    "\n",
    "    if search_engine == \"bing\":\n",
    "        snippet = prompt_rag.get_bing_snippet_v2(search_results, K=K, claim_date=claim_date, sort=sort)\n",
    "    elif search_engine == \"brave\":\n",
    "        if ids is None:\n",
    "            ids = slice(0, K)\n",
    "        snippet = prompt_rag.get_brave_snippet(search_results, ids=ids)\n",
    "    else:\n",
    "        raise Exception(\"Select search engines in [\\\"bing\\\", \\\"brave\\\"].\")\n",
    "    \n",
    "    info = \"INFORMATION:\\n\" + snippet + '\\n'\n",
    "\n",
    "    if without_info:\n",
    "        return (pre + text).strip()\n",
    "    else:\n",
    "        return pre + info + text\n",
    "\n",
    "def get_prompt_for_generating_prior_knowledge1(\n",
    "        claim, claim_date, search_engine, search_results, model_name,\n",
    "        K=5, sort=False, ids=None, without_info=False, without_claim_date=False):\n",
    "    \"\"\"\n",
    "    pre + text + info\n",
    "    \"\"\"\n",
    "\n",
    "    claim = claim.strip()\n",
    "\n",
    "    if model_name == \"solar\":\n",
    "        pre = \"Below is some INFORMATION searched online and a CLAIM. These pieces of INFORMATION are relevant to the CLAIM. This CLAIM and all INFORMATION include their respective publication dates and contents. To classify the CLAIM more accurately (if the content described by the CLAIM is correct, it will be classified as TRUE; if the content described by the CLAIM is incorrect, it will be classified as FALSE), please first expand on the given INFORMATION and provide a detailed summary of it. Then analyze, reason, and provide reasonable evidence to judge the correctness of the CLAIM based on the available information and your knowledge, and finally generate prior knowledge that helps classify the CLAIM.\\n\\n\"\n",
    "    elif model_name == \"mixtral\":\n",
    "\n",
    "\n",
    "        pre = \"Below is a CLAIM and some INFORMATION searched online. These pieces of INFORMATION are relevant to the CLAIM. This CLAIM and all INFORMATION include their respective publication dates and contents. To classify the CLAIM more accurately (if the content described by the CLAIM is correct, it will be classified as TRUE; if the content described by the CLAIM is incorrect, it will be classified as FALSE), please first provide a clear summary of the given INFORMATION, restate the CLAIM, and provide reasonable evidence to judge the correctness of the CLAIM based on the available information and your knowledge.\\n\\n\"\n",
    "        pre += \"CLAIM: \"\n",
    "\n",
    "        # pre = \"Below is a <CLAIM> and some INFORMATION searched online. These pieces of INFORMATION are relevant to the <CLAIM>. This <CLAIM> and all INFORMATION include their respective publication dates and contents. To classify the <CLAIM> more accurately (if the content described by the <CLAIM> is correct, it will be classified as TRUE; if the content described by the <CLAIM> is incorrect, it will be classified as FALSE), please first provide a clear summary of the given INFORMATION, restate the <CLAIM>, and provide reasonable evidence to judge the correctness of the <CLAIM> based on the available information and your knowledge.\\n\\n\"\n",
    "        # pre += \"<CLAIM>: \"\n",
    "\n",
    "    elif model_name == \"llama3\":\n",
    "\n",
    "        pre = \"Below is a <CLAIM> and some INFORMATION searched online. These pieces of INFORMATION are relevant to the <CLAIM>. This <CLAIM> and all INFORMATION include their respective publication dates and contents. To classify the <CLAIM> more accurately (if the content described by the <CLAIM> is correct, it will be classified as TRUE; if the content described by the <CLAIM> is incorrect, it will be classified as FALSE), please first provide a clear summary of the given INFORMATION, restate the <CLAIM>, and provide reasonable evidence to judge the correctness of the <CLAIM> based on the available information and your knowledge.\\n\\n\"\n",
    "        pre += \"<CLAIM>: \"\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"model_name 只能从solar，mixtral中选择\")\n",
    "    \n",
    "    if without_claim_date:\n",
    "        text = \"CLAIM: \" + claim\n",
    "    else:\n",
    "        text = get_claim_with_date(claim, claim_date) + '\\n\\n'\n",
    "\n",
    "    if search_engine == \"bing\":\n",
    "        snippet = prompt_rag.get_bing_snippet_v2(search_results, K=K, claim_date=claim_date, sort=sort)\n",
    "    elif search_engine == \"brave\":\n",
    "        if ids is None:\n",
    "            ids = slice(0, K)\n",
    "        snippet = prompt_rag.get_brave_snippet(search_results, ids=ids)\n",
    "    else:\n",
    "        raise Exception(\"Select search engines in [\\\"bing\\\", \\\"brave\\\"].\")\n",
    "    \n",
    "    info = \"INFORMATION:\\n\" + snippet\n",
    "\n",
    "    if without_info:\n",
    "        return (pre + text).strip()\n",
    "    else:\n",
    "        return pre + text + info\n",
    "    \n",
    "def get_claim_with_date(claim, claim_date=None):\n",
    "    if claim_date is None:\n",
    "        return \" \" + claim\n",
    "    \n",
    "    # res = \"\\n\"\n",
    "    res = claim + \"\\nPublication date: \" + claim_date\n",
    "    return res\n",
    "\n",
    "K = 5\n",
    "def get_id(claim):\n",
    "    for i in range(len(data_train)):\n",
    "        if claim.strip() in data_train[i][\"claim\"].strip():\n",
    "            return i\n",
    "\n",
    "# i = 0\n",
    "i = get_id(\"Pfizer and Moderna do call their COVID-19 shot a ‘vaccine’\")\n",
    "\n",
    "# ids = [194]\n",
    "ids1 = [19, 36, 57, 85, 98, 107, 116, 117, 125, 129, 194, 195, 204, 208, 212, 235, 279, 324, 328]\n",
    "# ids1 = [\n",
    "#     get_id(\"Pfizer and Moderna do call their COVID-19 shot a ‘vaccine’\"),\n",
    "#     get_id(\"Myth spreads online that Australian supermarkets have banned Chinese nationals during COVID-19 pandemic\"),\n",
    "#     get_id(\"Trump makes false claims about COVID-19 testing\"),\n",
    "#     get_id(\"UNHCR condemns fake notice which claimed refugees in Malaysia are resisting COVID-19 tests\"),\n",
    "#     get_id(\"Hospitals in Ohio were not set on fire during protests\"),\n",
    "#     get_id(\"A 2010 study on vaccines did not show that one in forty were damaged by vaccination\"),\n",
    "#     get_id(\"The Philippine health department said it did not issue this 'checklist' for COVID-19 symptoms\"),\n",
    "#     get_id(\"Contaminated CDC COVID-19 test kits recalled and did not spread virus\"),\n",
    "#     get_id(\"Experts refute false claim that Covid-19 vaccine can 'manipulate' human genes\"),\n",
    "#     get_id(\"Countries were not buying Covid-19 test kits in 2018\"),\n",
    "#     get_id(\"False claim circulates that Pakistani plane transported Sri Lankan students home after COVID-19 lockdown\"),\n",
    "#     get_id(\"People will not have to be vaccinated against COVID-19 to receive food stamps and rent assistance\"),\n",
    "#     get_id(\"Britain has not awarded a contract to develop a vaccine passport\"),\n",
    "#     get_id(\"World Health Organization says COVID-19 means ‘coronavirus disease 2019’ – not 'China outbreak virus'\"),\n",
    "#     get_id(\"The common cold is not the same as COVID-19 and the NHS is not saying it is\"),\n",
    "#     get_id(\"Philippine authorities did not issue this warning after the novel coronavirus outbreak\"),\n",
    "#     get_id(\"No tourists have been allowed to visit New Zealand since March 2020 -- this photo has circulated online since 2016\"),\n",
    "#     get_id(\"This video does not show social distancing failure on an Air India flight during the coronavirus pandemic\"),\n",
    "#     get_id(\"This photo shows a Sri Lankan airline pilot who tested positive for the novel coronavirus.\"), # fALSE\n",
    "# ]\n",
    "\n",
    "ids2 = [\n",
    "    get_id(\"Gynecological Cancers Not Tied to Severe COVID-19\"),\n",
    "    get_id(\"In February 2021, the Centers for Disease Control and Prevention (CDC) advised U.S. travelers to \\u201cavoid all travel to Mexico.\\u201d\"),\n",
    "    get_id(\"The U.S. CDC encourages the use of a \\u201c[COVID-19] flu shot\\u201d on children.\"),\n",
    "    get_id(\"An announcement was made on March 9 that all classes in the Basque Country were canceled due to the coronavirus.\"),\n",
    "    get_id(\"Rita Wilson, Tom Hanks’ wife, stated in an interview at CBS that “she wouldn’t be alive if not for chloroquine”\"),\n",
    "    get_id(\"Japan’s Nobel Prize winning Professor of Medicine, Professor Dr Tasuku Honjo has claimed that the coronavirus is not natural and that China manufactured it.\"),\n",
    "    get_id(\"Lies spread by Serbian authorities about COVID-19. For example, the virus does not affect pregnant women, children and young people; no newborn is infected with the coronavirus; the virus does not last long on objects; the number of respiratory machines that Serbia possesses, etc.\"),\n",
    "    get_id(\"Chief Secretary of West Bengal was not following relaxing while the state was performing poorly as it fought COVID-19.\"),\n",
    "    get_id(\"Businessman Ratan Tata has said that 2020 is the year to survive and not care about profits and losses.\"),\n",
    "    get_id(\"Celebrities spreading misinformation about coronavirus and the Janata curfew in India.\"),\n",
    "    get_id(\"This 3 year old girl is fighting for her life after getting the coronavirus.\"),\n",
    "    get_id(\"President Vucic claims that no one said coronavirus is the “funniest” virus.\"),\n",
    "    get_id(\"News photo from stay-at-home protest was doctored to add Confederate flag.\"),\n",
    "    get_id(\"President Donald Trump referred to the coronavirus as a “hoax” or “political conspiracy.”\"),\n",
    "    get_id(\"President Trump refers to the coronavirus as a hoax in an audio clip.\"),\n",
    "    get_id(\"A false image says that President Duque will declare a shutdown on November 1st.\"),\n",
    "    get_id(\"Professor Perronne makes several false claims on PCR tests, HCQ and hospitals.\"),\n",
    "    get_id(\"Audio of Biden calling the coronavirus a hoax.\"),\n",
    "    get_id(\"President Trump’s claim that he inherited no ventilators from the Obama administration.\"),\n",
    "    get_id(\"Video shows President Donald Trump saying COVID-19 is Democrats’ “new hoax.”\"),\n",
    "    get_id(\"Fauci Wasn't Involved in New Testing Guidelines\"),\n",
    "    get_id(\"This movie “Songbird” has been made before Covid. We’ve seen a lot of similar movies, yes. But, it is interesting that in the movie the virus is called COVID-23. Stop the scene when there is news on TV and you will see. Coincidence?\"),\n",
    "    get_id(\"Images of the newspaper front pages sharing vaccine misinformation\"),\n",
    "    get_id(\"I don't believe illegal casinos are operating in Bangkok, but if the doctor knows about it, he can inform the authorities.\"),\n",
    "    get_id(\"Facebook posts promote false conspiracy that coronavirus testing patent was submitted in 2015\"),\n",
    "    get_id(\"Misleading description of Canada\\u2019s quarantine sites feeds Covid-19 conspiracy\"),\n",
    "    get_id(\"Video repeats COVID-19 conspiracy theories\"),\n",
    "    get_id(\"The post contains a COVID-19 conspiracy theory written by former South Carolina Rep. Trey Gowdy.\"),\n",
    "    get_id(\"A long conspiracy video claims that: vaccines can alter a person’s DNA, nanorobots are inserted with the vaccine to collect biometric data, Bill Gates already owns this data and the body can receive 5G signal after the vaccine is taken\"),\n",
    "    get_id(\"Jorge Luis Sonnante published a 16-minute video that went viral on networks. In the video, Sonnante (who describes himself as a \\u201cdeacon\\u201d, but gives falsified evidence of this charge) mixes several conspiracy theories, some meaningless, others already denied, about the pandemic caused by the SARS coronavirus -CoV-2.\"),\n",
    "    get_id(\"Trump touts testing as \\\"greatest capacity in the world,\\\" but says people \\\"shouldn't want to get tested\\\"\"),\n",
    "    get_id(\"Alberto Fernández, about the coronavirus: “Mortality in people over 65 is 80%”.\"),\n",
    "    get_id(\"An English man received a COVID-19 vaccine through his shirt\"),\n",
    "    get_id(\"The first recipient of the COVID-19 trial vaccine is dead.\"),\n",
    "    get_id(\"ICUs are not overwhelmed\"),\n",
    "    get_id(\"Coronavirus tests do not work to diagnose COVID-19. Red Bull tests positive.\"),\n",
    "    get_id(\"Rapidly developed pharmaceuticals such as the Covid-19 vaccines compared to a sedative from the 1950s that caused serious birth defects.\"),\n",
    "    get_id(\"The major cause of death in Covid-19 is thrombosis or blood clot and not pneumonia.\"),\n",
    "    get_id(\"The COVID-19 virus was artificially created by the U.S. military and placed in a capsule.\"),\n",
    "    get_id(\"A video of a girl collapsing in a store is being shared widely on social media with a claim that the girl died of Coronavirus.\"),\n",
    "    get_id(\"Italy\\u2019s prime minister cries and declares that his country \\u201clost the battle\\u201d against the coronavirus.\"),\n",
    "    get_id(\"Black people are not being targeted for UK coronavirus vaccine trials\"),\n",
    "    get_id(\"In September 2019, a train named COVID-19 appeared in the United States, carrying gases that cause COVID-19.\"),\n",
    "    get_id(\"If you take the vaccine, you'll be enrolled in a pharmacovigilance tracking system. It means that you've enrolled yourself in a medical trial.\"),\n",
    "    get_id(\"Henry Kissinger quote about mandatory vaccinations\"),\n",
    "    get_id(\"RECEIVE FOOD AID SOCIAL PLAN 2020\"),\n",
    "    get_id(\"Post claims the video clip is last message of deceased Pakistani doctor Osama Riyaz who had contracted Coronavirus while treating patients\"),\n",
    "    get_id(\"For a few days now, an image has been circulating on Facebook claiming that COVID-19 can be cured with Dolex Gripa, \\\"Noraver\\\" night, and gargling with warm water and lemon.\"),\n",
    "    get_id(\"A magic remedy to prevent COVID-19: mix brown sugar, ginger, garlic, and Chinese leek, boil them and drink it.\"),\n",
    "    get_id(\"Some statements by the American political scientist and activist Susan George in which she says that “Spaniards are laboratory rats: let’s see how much punishment they tolerate without rebelling.” The phrase is shared related to the current coronavirus pandemic.\"),\n",
    "    get_id(\"Saddam Hussein explains what the coronavirus is.\"),\n",
    "    get_id(\"A Facebook post is spreading the false claim that former President Barack Obama gave $3.8 million to a lab in Wuhan, China.\"),\n",
    "    get_id(\"\\u2019No discrimination\\u2019 against Africans amid pandemic.\")\n",
    "]\n",
    "\n",
    "ids_x = [ids2[3],]\n",
    "\n",
    "prompt_list = []\n",
    "model_name = 'solar'\n",
    "\n",
    "# [ids2[ii] for ii in ids_filtered]:\n",
    "for i in ids2:\n",
    "    # claim = data_search[i][\"claim\"]\n",
    "    search_results = data_search[i][f\"{search_engine}_search_results\"]\n",
    "    prompt_list.append(get_prompt_for_generating_prior_knowledge2(\n",
    "            data_train[i][\"claim\"], data_train[i][\"date\"], search_engine, \n",
    "            search_results, model_name, K=K, sort=False, \n",
    "            # ids=data_search[i][\"random_ids\"],\n",
    "            ids=None\n",
    "        ))\n",
    "request_list = [{'query': prompt} for prompt in prompt_list]\n",
    "\n",
    "resp_list = get_resp_list(request_list)\n",
    "# print(resp_list[0][\"response\"].strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "0\n",
      "To classify the claim accurately, we need to analyze the available information and determine if it provides evidence of a Sri Lankan airline pilot testing positive for the novel coronavirus (COVID-19).\n",
      "\n",
      "Information 1 discusses the financial difficulties of SriLankan Airlines and mentions that one of their pilots tested positive for COVID-19. This information aligns with the claim, as it confirms the presence of a Sri Lankan airline pilot who tested positive for the virus. However, it does not provide any visual evidence, such as a photo.\n",
      "\n",
      "Information 2 is a travel advice from the UK government regarding COVID-19 in Sri Lanka, but it does not provide any specific details about a pilot testing positive.\n",
      "\n",
      "Information 3 is a dashboard displaying COVID-19 statistics in Sri Lanka, which is relevant to the context but does not provide direct evidence about the claim.\n",
      "\n",
      "Information 4 discusses a misleading claim about Sri Lanka eradicating the coronavirus, and it mentions that Sri Lanka had one confirmed case of the virus on January 27, 2020, which later recovered. Although this information is related to COVID-19 in Sri Lanka, it does not provide any direct evidence about the claim.\n",
      "\n",
      "Information 5 is a synoptic analysis of the COVID-19 outbreak in Sri Lanka, focusing on the impact on Sri Lankan migrant workers. This information is not directly relevant to the claim.\n",
      "\n",
      "In conclusion, while Information 1 provides evidence that a Sri Lankan airline pilot tested positive for COVID-19, it does not include a photo. Since there is no visual evidence in the provided information, we cannot definitively confirm or deny the claim based on the available data.\n"
     ]
    }
   ],
   "source": [
    "# llama-3-70b\n",
    "# CLAIM + info wrong: 1, 18\n",
    "# info + CLAIM wrong: 18\n",
    "\n",
    "# CLAIM + info wrong: 1, 2, 3, 7, 11, 15, 20, 30, 45, 51\n",
    "# info + CLAIM wrong: 1, 2, $3, 7, 11, 20, 45  √√√√√√√√√√√\n",
    "\n",
    "\n",
    "# mixtral-8x7b\n",
    "# claim wrong: 0, 1, 8, 10, 12\n",
    "# <claim>:1, 3\n",
    "\n",
    "# claim: 0, 1, 15, 16, 20, 24, 25, 30, 51\n",
    "# <claim>: 0, 1, 2, 7, 15, 16, 24, 25, 29, 51\n",
    "\n",
    "# phi-3-medium\n",
    "# claim wrong: \n",
    "# <claim>: 1, 8, 13, 18\n",
    "\n",
    "# claim: 1, 3, 7, 13, 15, 20, 22, 24, 25, 39, 51\n",
    "# <claim>: \n",
    "\n",
    "# solar\n",
    "# old: 0, 1, 10\n",
    "# new: 0, 1, 3, 8, 9, 10, 11, 13,\n",
    "print(len(ids1))\n",
    "\n",
    "nn = 18\n",
    "# print(prompt_list[nn])\n",
    "# print('*'*50)\n",
    "print(data_train[ids1[nn]][\"label\"])\n",
    "print(resp_list[nn][\"response\"].strip())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
