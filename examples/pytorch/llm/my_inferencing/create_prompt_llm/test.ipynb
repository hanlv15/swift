{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "dirs = [\"..\"]\n",
    "for _dir in dirs:\n",
    "    if _dir not in sys.path:\n",
    "        sys.path.append(_dir)\n",
    "\n",
    "import covmis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化 data_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train = covmis.load_train()\n",
    "# data_train_llm = [{\n",
    "#     \"id\": i[\"id\"],\n",
    "# } for i in data_train]\n",
    "\n",
    "# covmis.save_train_llm(data_train_llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There was a false study done where they gave it to very sick people -- extremely sick people, people that were ready to die. It was given by, obviously, not friends of the administration. And the study came out. The people were ready to die. Everybody was old, had bad problems with hearts, diabetes, and everything else you can imagine. So they gave it. So, immediately, when it came out, they gave a lot of false information, just so you understand. Great studies came out of Italy on hydroxy.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = covmis.load_train()\n",
    "data_train_bak = covmis.load_train(version=\"original\")\n",
    "data_search = covmis.load_train_search()\n",
    "\n",
    "\n",
    "for i, item in enumerate(data_train):\n",
    "    item[\"claim\"] = item[\"claim\"].strip()\n",
    "\n",
    "data_train[7630][\"claim\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covmis.save_train(data_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-15 13:13:35,866 - modelscope - INFO - PyTorch version 2.3.0 Found.\n",
      "2024-06-15 13:13:35,868 - modelscope - INFO - Loading ast index from /home/hanlv/.cache/modelscope/ast_indexer\n",
      "2024-06-15 13:13:35,902 - modelscope - INFO - Loading done! Current index file version is 1.15.0, with md5 21bc0b9ccf26bcf3f4ca2e675ec8875d and a total number of 980 components indexed\n",
      "[INFO:swift] Successfully registered `/home/hanlv/workspace/code/research/infodemic/LLM/swift/swift/llm/data/dataset_info.json`\n",
      "[INFO:swift] Setting template_type: phi3\n",
      "[INFO:swift] Setting args.lazy_tokenize: False\n",
      "[INFO:swift] Setting args.dataloader_num_workers: 1\n",
      "[INFO:swift] output_dir: /home/hanlv/workspace/code/research/infodemic/LLM/swift/examples/pytorch/llm/my_inferencing/create_prompt_llm/output/phi3-4b-4k-instruct/v1-20240615-131337\n",
      "[INFO:swift] Start time of running main: 2024-06-15 13:13:37.175955\n",
      "[INFO:swift] args: SftArguments(model_type='phi3-4b-4k-instruct', model_id_or_path='/home/css/models/Phi-3-mini-4k-instruct', model_revision='master', sft_type='lora', freeze_parameters=0.0, additional_trainable_parameters=[], tuner_backend='peft', template_type='phi3', output_dir='/home/hanlv/workspace/code/research/infodemic/LLM/swift/examples/pytorch/llm/my_inferencing/create_prompt_llm/output/phi3-4b-4k-instruct/v1-20240615-131337', add_output_dir_suffix=True, ddp_backend=None, ddp_find_unused_parameters=None, ddp_broadcast_buffers=None, seed=42, resume_from_checkpoint=None, resume_only_model=False, ignore_data_skip=False, dtype='bf16', packing=False, dataset=['/home/hanlv/workspace/code/research/infodemic/LLM/swift/examples/pytorch/llm/my_data/with_llama3_info/brave/train_test_split/8:2/train_data1.jsonl'], val_dataset=[], dataset_seed=42, dataset_test_ratio=0, use_loss_scale=False, loss_scale_config_path='/home/hanlv/workspace/code/research/infodemic/LLM/swift/swift/llm/agent/default_loss_scale_config.json', system=None, tools_prompt='react_en', max_length=2048, truncation_strategy='delete', check_dataset_strategy='none', model_name=[None, None], model_author=[None, None], quant_method=None, quantization_bit=0, hqq_axis=0, hqq_dynamic_config_path=None, bnb_4bit_comp_dtype='bf16', bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, lora_target_modules=['qkv_proj'], lora_rank=8, lora_alpha=32, lora_dropout_p=0.05, lora_bias_trainable='none', lora_modules_to_save=[], lora_dtype='AUTO', lora_lr_ratio=None, use_rslora=False, use_dora=False, init_lora_weights='true', rope_scaling=None, boft_block_size=4, boft_block_num=0, boft_n_butterfly_factor=1, boft_target_modules=['DEFAULT'], boft_dropout=0.0, boft_modules_to_save=[], vera_rank=256, vera_target_modules=['DEFAULT'], vera_projection_prng_key=0, vera_dropout=0.0, vera_d_initial=0.1, vera_modules_to_save=[], adapter_act='gelu', adapter_length=128, use_galore=False, galore_rank=128, galore_target_modules=None, galore_update_proj_gap=50, galore_scale=1.0, galore_proj_type='std', galore_optim_per_parameter=False, galore_with_embedding=False, adalora_target_r=8, adalora_init_r=12, adalora_tinit=0, adalora_tfinal=0, adalora_deltaT=1, adalora_beta1=0.85, adalora_beta2=0.85, adalora_orth_reg_weight=0.5, ia3_target_modules=['DEFAULT'], ia3_feedforward_modules=[], ia3_modules_to_save=[], llamapro_num_new_blocks=4, llamapro_num_groups=None, neftune_noise_alpha=None, neftune_backend='transformers', lisa_activated_layers=0, lisa_step_interval=20, gradient_checkpointing=True, deepspeed=None, batch_size=1, eval_batch_size=1, num_train_epochs=1, max_steps=-1, optim='adamw_torch', adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, learning_rate=0.0001, weight_decay=0.1, gradient_accumulation_steps=16, max_grad_norm=0.5, predict_with_generate=False, lr_scheduler_type='linear', warmup_ratio=0.05, eval_steps=50, save_steps=50, save_only_model=False, save_total_limit=2, logging_steps=5, dataloader_num_workers=1, dataloader_pin_memory=True, dataloader_drop_last=False, push_to_hub=False, hub_model_id=None, hub_token=None, hub_private_repo=False, push_hub_strategy='push_best', test_oom_error=False, disable_tqdm=False, lazy_tokenize=False, preprocess_num_proc=1, use_flash_attn=None, ignore_args_error=False, check_model_is_latest=True, logging_dir='/home/hanlv/workspace/code/research/infodemic/LLM/swift/examples/pytorch/llm/my_inferencing/create_prompt_llm/output/phi3-4b-4k-instruct/v1-20240615-131337/runs', report_to=['tensorboard'], acc_strategy='token', save_on_each_node=True, evaluation_strategy='steps', save_strategy='steps', save_safetensors=True, gpu_memory_fraction=None, include_num_input_tokens_seen=False, local_repo_path=None, custom_register_path=None, custom_dataset_info=None, device_map_config_path=None, max_new_tokens=2048, do_sample=True, temperature=0.3, top_k=20, top_p=0.7, repetition_penalty=1.0, num_beams=1, fsdp='', fsdp_config=None, sequence_parallel_size=1, model_layer_cls_name=None, metric_warmup_step=0, fsdp_num=1, per_device_train_batch_size=None, per_device_eval_batch_size=None, eval_strategy=None, self_cognition_sample=0, train_dataset_mix_ratio=0.0, train_dataset_mix_ds=['ms-bench'], train_dataset_sample=-1, val_dataset_sample=None, safe_serialization=None, only_save_model=None, neftune_alpha=None, deepspeed_config_path=None, model_cache_dir=None, custom_train_dataset_path=[], custom_val_dataset_path=[])\n",
      "[INFO:swift] Global seed set to 42\n",
      "[INFO:swift] Loading the model using model_dir: /home/css/models/Phi-3-mini-4k-instruct\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_count: 1\n",
      "rank: -1, local_rank: -1, world_size: 1, local_world_size: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625503b1a08d48c4b41b42fe0046eaa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] model.max_model_len: 4096\n",
      "[INFO:swift] model_config: Phi3Config {\n",
      "  \"_name_or_path\": \"/home/css/models/Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO:swift] generation_config: GenerationConfig {\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"temperature\": 0.3,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.7\n",
      "}\n",
      "\n",
      "[INFO:swift] lora_target_modules: ['qkv_proj']\n",
      "[INFO:swift] lora_modules_to_save: []\n",
      "[INFO:swift] lora_config: get_wrapped_class.<locals>.PeftWrapper(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/home/css/models/Phi-3-mini-4k-instruct', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=8, target_modules={'qkv_proj'}, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=[], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, lora_dtype=None, lorap_lr_ratio=None, lorap_emb_lr=1e-06)\n",
      "[INFO:swift] [base_model.model.model.embed_tokens.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.o_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.qkv_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.qkv_proj.lora_A.default.weight]: requires_grad=True, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.qkv_proj.lora_B.default.weight]: requires_grad=True, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.mlp.gate_up_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.mlp.down_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.input_layernorm.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.post_attention_layernorm.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.1.self_attn.o_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.1.self_attn.qkv_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.1.self_attn.qkv_proj.lora_A.default.weight]: requires_grad=True, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.1.self_attn.qkv_proj.lora_B.default.weight]: requires_grad=True, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.1.mlp.gate_up_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.1.mlp.down_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.1.input_layernorm.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.1.post_attention_layernorm.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.2.self_attn.o_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.2.self_attn.qkv_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.2.self_attn.qkv_proj.lora_A.default.weight]: requires_grad=True, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] ...\n",
      "[INFO:swift] PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Phi3ForCausalLM(\n",
      "      (model): Phi3Model(\n",
      "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
      "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x Phi3DecoderLayer(\n",
      "            (self_attn): Phi3Attention(\n",
      "              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "              (qkv_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=9216, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): Phi3RotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Phi3MLP(\n",
      "              (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "              (activation_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Phi3RMSNorm()\n",
      "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (post_attention_layernorm): Phi3RMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): Phi3RMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[INFO:swift] PeftModelForCausalLM: 3824.2253M Params (3.1457M Trainable [0.0823%]), 0.0000M Buffers.\n",
      "[INFO:swift] Setting model.config.use_cache: False\n",
      "[INFO:swift] train_dataset: Dataset({\n",
      "    features: ['query', 'response'],\n",
      "    num_rows: 9753\n",
      "})\n",
      "[INFO:swift] val_dataset: None\n",
      "[INFO:swift] system: You are a helpful digital assistant. Please provide safe, ethical and accurate information to the user.\n",
      "[INFO:swift] args.lazy_tokenize: False\n",
      "[INFO:swift] Using num_proc: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91bac6a65dac4244ab6ae3f4b45ef490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9753 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4111 > 4096). Running this sequence through the model will result in indexing errors\n",
      "[INFO:swift] [INPUT_IDS] [1, 32006, 29871, 13, 3492, 526, 263, 8444, 13436, 20255, 29889, 3529, 3867, 9109, 29892, 11314, 936, 322, 16232, 2472, 304, 278, 1404, 29889, 32007, 29871, 13, 32010, 29871, 13, 21140, 340, 338, 263, 315, 4375, 7833, 322, 278, 349, 3960, 1955, 476, 6632, 29956, 20566, 1692, 6942, 411, 372, 29889, 3529, 770, 1598, 278, 315, 4375, 7833, 408, 15676, 470, 17131, 2729, 373, 278, 349, 3960, 1955, 476, 6632, 29956, 20566, 1692, 29889, 960, 278, 2793, 5439, 491, 278, 315, 4375, 7833, 338, 1959, 29892, 769, 770, 1598, 372, 408, 15676, 29936, 565, 278, 2793, 5439, 491, 278, 315, 4375, 7833, 338, 10240, 29892, 769, 770, 1598, 372, 408, 17131, 29889, 13, 13, 13875, 7833, 29901, 319, 12307, 310, 1939, 423, 313, 12803, 310, 5208, 11477, 29892, 13616, 29897, 756, 1063, 3041, 26458, 411, 278, 25082, 485, 22693, 29889, 13, 19858, 362, 2635, 29901, 29871, 29906, 29900, 29906, 29900, 29899, 29900, 29906, 29899, 29900, 29946, 13, 13, 29829, 1955, 476, 6632, 29956, 20566, 1692, 29901, 13, 20350, 29871, 29896, 29901, 13, 19858, 362, 2635, 29901, 29871, 29906, 29900, 29906, 29900, 29899, 29900, 29945, 29899, 29900, 29946, 13, 7030, 29901, 13616, 2994, 265, 485, 22693, 7315, 322, 11733, 3917, 448, 450, 1570, 3088, 10277, 13, 3916, 29901, 13, 13393, 278, 9281, 24469, 322, 11053, 529, 1110, 29958, 974, 829, 1110, 29958, 529, 1110, 29958, 2616, 265, 485, 22693, 829, 1110, 29958, 4251, 29892, 4892, 29879, 322, 325, 5753, 262, 800, 297, 529, 1110, 29958, 5592, 475, 829, 1110, 15513, 13, 20350, 29871, 29906, 29901, 13, 19858, 362, 2635, 29901, 6213, 13, 7030, 29901, 19937, 29899, 29896, 29929, 4251, 491, 5120, 297, 13616, 29871, 29906, 29900, 29906, 29941, 891, 6666, 2079, 13, 3916, 29901, 13, 2887, 310, 29871, 29906, 29900, 29906, 29941, 29892, 529, 1110, 29958, 1552, 829, 1110, 29958, 529, 1110, 29958, 12803, 829, 1110, 29958, 1556, 15201, 491, 1353, 310, 19937, 29899, 29896, 29929, 4251, 297, 529, 1110, 29958, 5592, 475, 829, 1110, 29958, 338, 11732, 6405, 29889, 450, 10432, 529, 1110, 29958, 12803, 829, 1110, 29958, 3203, 15201, 491, 278, 24424, 338, 7747, 6637, 29889, 13, 20350, 29871, 29941, 29901, 13, 19858, 362, 2635, 29901, 29871, 29906, 29900, 29906, 29900, 29899, 29900, 29955, 29899, 29900, 29945, 13, 7030, 29901, 2994, 265, 485, 22693, 29901, 13616, 7275, 267, 1887, 7714, 3204, 297, 5208, 11477, 448, 14129, 10130, 13, 3916, 29901, 13, 29966, 1110, 29958, 29954, 284, 11477, 829, 1110, 29958, 337, 2611, 1078, 3151, 5824, 373, 263, 12180, 284, 4038, 29892, 263, 2462, 1156, 11732, 6405, 527, 4752, 263, 2788, 1887, 7714, 3204, 29889, 13, 20350, 29871, 29946, 29901, 13, 19858, 362, 2635, 29901, 6213, 13, 7030, 29901, 14650, 310, 2994, 265, 485, 22693, 4251, 29901, 5208, 11477, 29892, 13616, 313, 29955, 29945, 29906, 29892, 29941, 29946, 29947, 4251, 29897, 13, 3916, 29901, 13, 7583, 322, 15839, 5849, 310, 19937, 29899, 29896, 29929, 529, 1110, 29958, 2616, 265, 485, 22693, 829, 1110, 29958, 529, 1110, 29958, 262, 20309, 829, 1110, 29958, 3694, 297, 529, 1110, 29958, 29954, 284, 11477, 829, 1110, 29958, 13, 20350, 29871, 29945, 29901, 13, 19858, 362, 2635, 29901, 29871, 29906, 29900, 29906, 29946, 29899, 29900, 29896, 29899, 29896, 29896, 13, 7030, 29901, 1939, 423, 448, 14109, 13, 3916, 29901, 13, 3782, 423, 313, 29954, 284, 8910, 11504, 11173, 362, 29901, 518, 30176, 29876, 30247, 29926, 30628, 2314, 338, 263, 4726, 322, 21511, 297, 278, 28273, 681, 7881, 310, 5208, 11477, 297, 6641, 22741, 13616, 29889, 739, 338, 278, 7483, 310, 278, 419, 23372, 411, 278, 1021, 1024, 29889, 739, 756, 263, 4665, 310, 29871, 29896, 29946, 29892, 29929, 29946, 29955, 19681, 313, 29906, 29900, 29896, 29900, 511, 1641, 24046, 297, 278, 17325, 310, 319, 2994, 29884, 9658, 29892, 777, 29871, 29906, 29900, 7800, 5833, 310, 18135, 316, 422, 2490, 3100, 2978, 278, 13394, 310, 278, 16939, 1030, 8580, 29889, 1939, 423, 471, 263, 266, 1150, 292, 12128, 2011, 2645, 278, 17511, 14253, 319, 2710, 29892, 1641, 472, 393, 931, 278, 4654, 1556, 4100, 4726, 310, 278, 26201, 2200, 310, 18135, 316, 422, 2490, 3100, 29892, 1156, 278, 7483, 322, 278, 4726, 310, 349, 9568, 1490, 336, 29889, 13, 3782, 423, 471, 263, 266, 1150, 292, 12128, 2011, 2645, 278, 17511, 14253, 319, 2710, 29892, 1641, 472, 393, 931, 278, 4654, 1556, 4100, 4726, 310, 278, 26201, 2200, 310, 18135, 316, 422, 2490, 3100, 29892, 1156, 278, 7483, 322, 278, 4726, 310, 349, 9568, 1490, 336, 29889, 3645, 393, 3152, 372, 750, 21634, 263, 18697, 2030, 12616, 29892, 411, 263, 3652, 310, 5683, 5917, 267, 802, 26014, 29889, 450, 376, 29928, 423, 1697, 2803, 3417, 402, 744, 25496, 29908, 313, 12742, 310, 278, 5208, 8910, 16849, 414, 29897, 471, 26301, 297, 19107, 310, 5459, 888, 7740, 309, 743, 316, 323, 11269, 273, 3944, 297, 29871, 29906, 29900, 29900, 29941, 322, 13302, 27161, 8266, 1704, 283, 297, 29871, 29906, 29900, 29900, 29955, 29892, 1716, 6345, 297, 445, 4726, 29889, 13, 13, 1068, 26289, 310, 2672, 19094, 8098, 29901, 1068, 13, 13, 1576, 4944, 2472, 11624, 310, 5320, 12785, 310, 2793, 4475, 304, 19937, 29899, 29896, 29929, 4251, 297, 13616, 29892, 10734, 297, 278, 12786, 310, 5208, 11477, 322, 11732, 6405, 29889, 13, 13, 20350, 29871, 29896, 322, 29871, 29946, 3867, 2498, 2472, 1048, 19937, 29899, 29896, 29929, 4251, 29892, 4892, 29879, 29892, 322, 325, 5753, 262, 800, 297, 13616, 29892, 411, 263, 8569, 373, 5208, 11477, 29889, 13, 13, 20350, 29871, 29906, 5922, 393, 29892, 408, 310, 29871, 29906, 29900, 29906, 29941, 29892, 11732, 6405, 338, 278, 5120, 1556, 15201, 491, 19937, 29899, 29896, 29929, 4251, 297, 13616, 29892, 1550, 7747, 6637, 338, 278, 3203, 15201, 29889, 13, 13, 20350, 29871, 29941, 13676, 373, 263, 1887, 7714, 3204, 297, 5208, 11477, 297, 5468, 29871, 29906, 29900, 29906, 29900, 29892, 1494, 263, 2788, 7714, 3204, 297, 11732, 6405, 29889, 13, 13, 20350, 29871, 29945, 8128, 2498, 2472, 1048, 1939, 423, 29892, 263, 4726, 297, 5208, 11477, 29892, 13616, 29892, 3704, 967, 4955, 29892, 4665, 29892, 322, 16375, 26002, 29889, 13, 13, 1068, 29923, 29894, 5084, 304, 16833, 278, 1959, 2264, 310, 278, 315, 4375, 7833, 29901, 1068, 13, 13, 29933, 1463, 373, 278, 4944, 2472, 29892, 727, 338, 694, 1513, 10757, 304, 9659, 470, 972, 29891, 278, 315, 4375, 7833, 393, 263, 12307, 310, 1939, 423, 313, 12803, 310, 5208, 11477, 29892, 13616, 29897, 756, 1063, 3041, 26458, 411, 278, 25082, 485, 22693, 29889, 2398, 29892, 591, 508, 10115, 777, 8018, 2472, 29901, 13, 13, 29930, 5208, 11477, 338, 263, 5120, 297, 13616, 393, 756, 1063, 15201, 491, 19937, 29899, 29896, 29929, 29892, 408, 3415, 3615, 1133, 491, 10343, 29871, 29896, 29892, 29871, 29941, 29892, 322, 29871, 29946, 29889, 13, 29930, 1670, 505, 1063, 19937, 29899, 29896, 29929, 4251, 297, 5208, 11477, 29892, 411, 263, 1887, 7714, 3204, 527, 4752, 297, 5468, 29871, 29906, 29900, 29906, 29900, 313, 20350, 29871, 29941, 467, 13, 29930, 1939, 423, 338, 263, 4726, 297, 5208, 11477, 29892, 13616, 313, 20350, 29871, 29945, 467, 13, 13, 8809, 488, 591, 2609, 12534, 3149, 263, 2702, 12307, 310, 1939, 423, 1058, 756, 1063, 3041, 26458, 411, 278, 25082, 485, 22693, 29892, 372, 338, 15590, 304, 5251, 393, 29892, 2183, 278, 10122, 310, 19937, 29899, 29896, 29929, 4251, 297, 5208, 11477, 29892, 372, 338, 1950, 393, 4856, 297, 1939, 423, 470, 967, 18830, 10161, 1122, 505, 1063, 3041, 26458, 29889, 13, 13, 1068, 2385, 2450, 310, 278, 315, 4375, 7833, 29901, 1068, 13, 13, 29933, 1463, 373, 278, 3625, 2472, 29892, 306, 723, 770, 1598, 278, 315, 4375, 7833, 408, 3579, 5265, 6059, 16786, 15676, 1068, 29892, 541, 1728, 1513, 10757, 304, 9659, 278, 2702, 1206, 310, 263, 12307, 310, 1939, 423, 1641, 3041, 26458, 411, 278, 25082, 485, 22693, 29889, 32007, 29871, 13, 32001, 29871, 13, 17131, 29889, 32007]\n",
      "[INFO:swift] [INPUT] <s><|system|> \n",
      "You are a helpful digital assistant. Please provide safe, ethical and accurate information to the user.<|end|> \n",
      "<|user|> \n",
      "Below is a CLAIM and the PRIOR KNOWLEDGE associated with it. Please classify the CLAIM as TRUE or FALSE based on the PRIOR KNOWLEDGE. If the content described by the CLAIM is correct, then classify it as TRUE; if the content described by the CLAIM is incorrect, then classify it as FALSE.\n",
      "\n",
      "CLAIM: A neighbor of Noia (region of Galicia, Spain) has been infected with the coronavirus.\n",
      "Publication date: 2020-02-04\n",
      "\n",
      "PRIOR KNOWLEDGE:\n",
      "Information 1:\n",
      "Publication date: 2020-05-04\n",
      "Title: Spain Coronavirus Map and Case Count - The New York Times\n",
      "Content:\n",
      "See the latest charts and maps <strong>of</strong> <strong>coronavirus</strong> cases, deaths and vaccinations in <strong>Spain</strong>.\n",
      "Information 2:\n",
      "Publication date: None\n",
      "Title: COVID-19 cases by region in Spain 2023 | Statista\n",
      "Content:\n",
      "As of 2023, <strong>the</strong> <strong>region</strong> most affected by number of COVID-19 cases in <strong>Spain</strong> is Catalonia. The Spanish <strong>region</strong> least affected by the virus is Ceuta.\n",
      "Information 3:\n",
      "Publication date: 2020-07-05\n",
      "Title: Coronavirus: Spain imposes local lockdown in Galicia - BBC News\n",
      "Content:\n",
      "<strong>Galicia</strong> reinstates curbs on a coastal area, a day after Catalonia imposed a similar local lockdown.\n",
      "Information 4:\n",
      "Publication date: None\n",
      "Title: Development of Coronavirus cases: Galicia, Spain (752,348 cases)\n",
      "Content:\n",
      "Current and historical development of COVID-19 <strong>coronavirus</strong> <strong>infection</strong> numbers in <strong>Galicia</strong>\n",
      "Information 5:\n",
      "Publication date: 2024-01-11\n",
      "Title: Noia - Wikipedia\n",
      "Content:\n",
      "Noia (Galician pronunciation: [ˈnɔjɐ]) is a town and municipality in the autonomous community of Galicia in northwestern Spain. It is the capital of the comarca with the same name. It has a population of 14,947 inhabitants (2010), being situated in the Province of A Coruña, some 20 miles west of Santiago de Compostela near the mouth of the Tambre river. Noia was a thriving commercial port during the Low Middle Ages, being at that time the third most important town of the bishopric of Santiago de Compostela, after the capital and the town of Pontevedra.\n",
      "Noia was a thriving commercial port during the Low Middle Ages, being at that time the third most important town of the bishopric of Santiago de Compostela, after the capital and the town of Pontevedra. From that era it had preserved a notable old quarter, with a series of late Romanesque churches. The \"Dia das Letras Galegas\" (Day of the Galician Writers) was celebrated in honour of Antón Avilés de Taramancos in 2003 and María Mariño Carou in 2007, both born in this town.\n",
      "\n",
      "**Summary of INFORMATION:**\n",
      "\n",
      "The provided information consists of five pieces of content related to COVID-19 cases in Spain, particularly in the regions of Galicia and Catalonia.\n",
      "\n",
      "Information 1 and 4 provide general information about COVID-19 cases, deaths, and vaccinations in Spain, with a focus on Galicia.\n",
      "\n",
      "Information 2 states that, as of 2023, Catalonia is the region most affected by COVID-19 cases in Spain, while Ceuta is the least affected.\n",
      "\n",
      "Information 3 reports on a local lockdown in Galicia in July 2020, following a similar lockdown in Catalonia.\n",
      "\n",
      "Information 5 provides general information about Noia, a town in Galicia, Spain, including its history, population, and cultural significance.\n",
      "\n",
      "**Evidence to judge the correctness of the CLAIM:**\n",
      "\n",
      "Based on the provided information, there is no direct evidence to confirm or deny the CLAIM that a neighbor of Noia (region of Galicia, Spain) has been infected with the coronavirus. However, we can infer some relevant information:\n",
      "\n",
      "* Galicia is a region in Spain that has been affected by COVID-19, as evidenced by Information 1, 3, and 4.\n",
      "* There have been COVID-19 cases in Galicia, with a local lockdown imposed in July 2020 (Information 3).\n",
      "* Noia is a town in Galicia, Spain (Information 5).\n",
      "\n",
      "While we cannot pinpoint a specific neighbor of Noia who has been infected with the coronavirus, it is reasonable to assume that, given the presence of COVID-19 cases in Galicia, it is possible that someone in Noia or its surrounding areas may have been infected.\n",
      "\n",
      "**Classification of the CLAIM:**\n",
      "\n",
      "Based on the available information, I would classify the CLAIM as **LIKELY TRUE**, but without direct evidence to confirm the specific case of a neighbor of Noia being infected with the coronavirus.<|end|> \n",
      "<|assistant|> \n",
      " FALSE.<|end|>\n",
      "[INFO:swift] [LABLES_IDS] [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 17131, 29889, 32007]\n",
      "[INFO:swift] [LABLES] [-100 * 1281]FALSE.<|end|>\n",
      "[INFO:swift] Dataset Token Length: 1612.076775±289.212734, min=762.000000, max=2048.000000, size=4168\n",
      "[INFO:swift] training_args: Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "acc_strategy=token,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "additional_saved_files=[],\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=1,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_steps=50,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=GenerationConfig {\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"temperature\": 0.3,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.7\n",
      "}\n",
      ",\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=16,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/hanlv/workspace/code/research/infodemic/LLM/swift/examples/pytorch/llm/my_inferencing/create_prompt_llm/output/phi3-4b-4k-instruct/v1-20240615-131337/runs,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=0.5,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=loss,\n",
      "metric_warmup_step=0,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/home/hanlv/workspace/code/research/infodemic/LLM/swift/examples/pytorch/llm/my_inferencing/create_prompt_llm/output/phi3-4b-4k-instruct/v1-20240615-131337,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_hub_strategy=push_best,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/hanlv/workspace/code/research/infodemic/LLM/swift/examples/pytorch/llm/my_inferencing/create_prompt_llm/output/phi3-4b-4k-instruct/v1-20240615-131337,\n",
      "save_on_each_node=True,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=50,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "train_dataset_sample=9753,\n",
      "train_sampler_random=True,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.1,\n",
      ")\n",
      "[ERROR:swift] There are error run git command.\n",
      "[INFO:swift] The SftArguments will be saved in: /home/hanlv/workspace/code/research/infodemic/LLM/swift/examples/pytorch/llm/my_inferencing/create_prompt_llm/output/phi3-4b-4k-instruct/v1-20240615-131337/sft_args.json\n",
      "[INFO:swift] The Seq2SeqTrainingArguments will be saved in: /home/hanlv/workspace/code/research/infodemic/LLM/swift/examples/pytorch/llm/my_inferencing/create_prompt_llm/output/phi3-4b-4k-instruct/v1-20240615-131337/training_args.json\n",
      "[INFO:swift] The logging file will be saved in: /home/hanlv/workspace/code/research/infodemic/LLM/swift/examples/pytorch/llm/my_inferencing/create_prompt_llm/output/phi3-4b-4k-instruct/v1-20240615-131337/logging.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6638bbe0984dd3b3c06d33786c17d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.05091524, 'acc': 0.02083333, 'grad_norm': 44.75, 'learning_rate': 7.69e-06, 'memory(GiB)': 10.72, 'train_speed(iter/s)': 0.079698, 'epoch': 0.0, 'global_step': 1}\n",
      "{'loss': 5.9446125, 'acc': 0.0, 'grad_norm': 36.75, 'learning_rate': 3.846e-05, 'memory(GiB)': 10.93, 'train_speed(iter/s)': 0.087253, 'epoch': 0.02, 'global_step': 5}\n",
      "{'loss': 3.72507172, 'acc': 0.02916667, 'grad_norm': 27.0, 'learning_rate': 7.692e-05, 'memory(GiB)': 11.27, 'train_speed(iter/s)': 0.088548, 'epoch': 0.04, 'global_step': 10}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNCCL_IB_DISABLE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mswift\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sft_main, SftArguments, infer_main, InferArguments, deploy_main, DeployArguments\n\u001b[0;32m----> 9\u001b[0m \u001b[43msft_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSftArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mphi3-4b-4k-instruct\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/hanlv/workspace/code/research/infodemic/LLM/swift/examples/pytorch/llm/my_data/with_llama3_info/brave/train_test_split/8:2/train_data1.jsonl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_test_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/css/models/Phi-3-mini-4k-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/code/research/infodemic/LLM/swift/swift/utils/run_utils.py:27\u001b[0m, in \u001b[0;36mget_main.<locals>.x_main\u001b[0;34m(argv, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremaining_argv: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mremaining_argv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mllm_x\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnd time of running main: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS.\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/workspace/code/research/infodemic/LLM/swift/swift/llm/sft.py:307\u001b[0m, in \u001b[0;36mllm_sft\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    305\u001b[0m logging_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39moutput_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogging.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    306\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe logging file will be saved in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogging_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m last_model_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(trainer\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_model_checkpoint\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    309\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_model_checkpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_model_checkpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/code/research/infodemic/LLM/swift/swift/trainers/mixin.py:518\u001b[0m, in \u001b[0;36mSwiftMixin.train\u001b[0;34m(self, resume_from_checkpoint, *args, **kwargs)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resume_from_checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_enabled):\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_from_checkpoint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resume_from_checkpoint)\n\u001b[0;32m--> 518\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resume_from_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_memory \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/transformers/trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/transformers/trainer.py:3238\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3238\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3241\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m~/workspace/code/research/infodemic/LLM/swift/swift/trainers/trainers.py:228\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m    226\u001b[0m     acc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(acc_list, device\u001b[38;5;241m=\u001b[39mpreds\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 228\u001b[0m     acc \u001b[38;5;241m=\u001b[39m (\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mmasked_select(labels, masks))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m acc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_custom_metrics:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "os.environ['NCCL_P2P_DISABLE'] = '1'\n",
    "os.environ['NCCL_IB_DISABLE'] = '1'\n",
    "\n",
    "from swift.llm import sft_main, SftArguments, infer_main, InferArguments, deploy_main, DeployArguments\n",
    "\n",
    "\n",
    "sft_main(SftArguments(\n",
    "    model_type='phi3-4b-4k-instruct', dataset=['/home/hanlv/workspace/code/research/infodemic/LLM/swift/examples/pytorch/llm/my_data/with_llama3_info/brave/train_test_split/8:2/train_data1.jsonl'], dataset_test_ratio=0,\n",
    "    model_id_or_path=\"/home/css/models/Phi-3-mini-4k-instruct\"\n",
    "))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
