{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查每个claim，第一个检索结果如果是 factcheck.afp.com 或者 reuters.com ，\n",
    "然后再核对title是否对应的上claim：\n",
    "找claim第一个单词，再title里找到对应的起始位置，然后终止位置为...\n",
    "查看title是否在claim中出现，如果是，那么FALSE改为TRUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "dirs = [\"../..\", \"../create_prompt_llm\"]\n",
    "for _dir in dirs:\n",
    "    if _dir not in sys.path:\n",
    "        sys.path.append(_dir)\n",
    "import prompt_rag\n",
    "\n",
    "search_engine = \"brave\"\n",
    "\n",
    "def load_train_bak():\n",
    "    with open(\"covmis_data/train.json.bak\", \"r\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def load_train():\n",
    "    with open(\"covmis_data/train.json\", \"r\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def load_type1():\n",
    "    with open(\"output/type_1.json\", \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_type2():\n",
    "    with open(\"output/type_2.json\", \"r\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def save_type1(x):\n",
    "    with open(\"output/type_1.json\", \"w\") as f:\n",
    "        json.dump(x, f, indent=4)\n",
    "\n",
    "def item_exist(x, data):\n",
    "    for item in data:\n",
    "        if item[\"id\"] == x[\"id\"]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def update_data_type1(x, data: list):\n",
    "\n",
    "    data_type2 = load_type2()\n",
    "    if item_exist(x, data_type2):\n",
    "        return\n",
    "    \n",
    "    if not item_exist(x, data):\n",
    "        data.append(x)\n",
    "\n",
    "with open(f\"/home/hanlv/workspace/data/machine_learning/dataset/research/misinformation_dataset/COVMIS-main/data/train_{search_engine}_search.json\", \"r\") as f:\n",
    "    data_search = json.load(f)\n",
    "try:\n",
    "    with open(f\"/home/hanlv/workspace/data/machine_learning/dataset/research/misinformation_dataset/COVMIS-main/data/train_{search_engine}_search_llm.json\", \"r\") as f:\n",
    "        data_search_llm = json.load(f)\n",
    "except:\n",
    "    data_search_llm = [{\n",
    "        \"claim\": i[\"claim\"],\n",
    "        \"claimant\": i[\"claimant\"],\n",
    "        \"label\": i[\"label\"],\n",
    "        \"date\": i[\"date\"],\n",
    "    } for i in data_search]\n",
    "    with open(f\"/home/hanlv/workspace/data/machine_learning/dataset/research/misinformation_dataset/COVMIS-main/data/train_{search_engine}_search_llm.json\", \"w\") as f:\n",
    "        json.dump(data_search_llm, f, indent=4)\n",
    "\n",
    "def save_diff(x, model_name, diff_version):\n",
    "    with open(f\"output/diff_{model_name}_v{diff_version}.json\", \"w\") as f:\n",
    "        json.dump(x, f, indent=4)\n",
    "\n",
    "def load_diff(model_name, diff_version):\n",
    "    try:\n",
    "        with open(f\"output/diff_{model_name}_v{diff_version}.json\", \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except:\n",
    "        data_init = [{\"claim\": i[\"claim\"], \"label\": int(i[\"label\"])} for i in data_search_llm]\n",
    "        with open(f\"output/diff_{model_name}_v{diff_version}.json\", \"w\") as f:\n",
    "            json.dump(data_init, f, indent=4)\n",
    "        return data_init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr Mikovits, an established scientist, revealed that Dr Anthony Fauci was paid to cover up the extent of the problem caused by COVID-19. Also she revealed that SARS-CoV-2 was created in a lab and that influenza vaccines increase COVID-19 mortality by 36%.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = 0\n",
    "for item in data_search:\n",
    "    claim = item[\"claim\"]\n",
    "    title = item[f\"{search_engine}_search_results\"][\"web\"][\"results\"][0][\"title\"]\n",
    "    url = item[f\"{search_engine}_search_results\"][\"web\"][\"results\"][0][\"url\"]\n",
    "    check_list = [\"poynter\"] # \n",
    "\n",
    "    ok = False\n",
    "    for site in check_list:\n",
    "        if site in url:\n",
    "            ok = True\n",
    "            break\n",
    "    if not ok:\n",
    "        continue\n",
    "    cnt += 1\n",
    "\n",
    "    print(claim)\n",
    "    break\n",
    "cnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the given information:\n",
      "\n",
      "Information 1, published on 2020-07-25, is an article from a fact-checking organization called El Sabueso. It verifies a press conference from a group called 'Doctors for the truth' and finds their claims about cures, prevention, and the spread of the virus to be false or out of context.\n",
      "\n",
      "Information 2, published on 2022-02-14, is a Johns Hopkins Medicine article that addresses common myths about COVID-19. However, it does not specifically mention 'Doctors for the truth' or any of their claims.\n",
      "\n",
      "Information 3, with no specific publication date, is an article discussing the role of physicians and researchers in combating misinformation about COVID-19. It highlights their efforts to use blogs, social media, and conversations to guide people towards safe practices.\n",
      "\n",
      "Information 4, published on 2017-10-19, is a Pew Research Center article discussing the future of truth and misinformation online. It features opinions from experts about the potential for reducing misinformation online, but it does not specifically mention COVID-19 or 'Doctors for the truth.'\n",
      "\n",
      "Information 5, published on 2021-09-20, is an article discussing whether doctors spreading COVID-19 misinformation will ever face consequences. It mentions that some top spreaders of misinformation are physicians with active licenses, but it does not specifically mention 'Doctors for the truth.'\n",
      "\n",
      "Based on the available information, the claim is:\n",
      "TRUE\n",
      "\n",
      "Reasoning:\n",
      "The claim, published on 2020-07-25, states that a group called 'Doctors for the truth' shared false or out of context data regarding cures, prevention, and the spread of the virus. Information 1, published on the same day, confirms this claim by fact-checking the press conference and finding the doctors' arguments to be false or without scientific evidence. The other information sources do not contradict this claim and do not provide any evidence that would change the classification.\n"
     ]
    }
   ],
   "source": [
    "s = \"Summary of the given information:\\n\\nInformation 1, published on 2020-07-25, is an article from a fact-checking organization called El Sabueso. It verifies a press conference from a group called 'Doctors for the truth' and finds their claims about cures, prevention, and the spread of the virus to be false or out of context.\\n\\nInformation 2, published on 2022-02-14, is a Johns Hopkins Medicine article that addresses common myths about COVID-19. However, it does not specifically mention 'Doctors for the truth' or any of their claims.\\n\\nInformation 3, with no specific publication date, is an article discussing the role of physicians and researchers in combating misinformation about COVID-19. It highlights their efforts to use blogs, social media, and conversations to guide people towards safe practices.\\n\\nInformation 4, published on 2017-10-19, is a Pew Research Center article discussing the future of truth and misinformation online. It features opinions from experts about the potential for reducing misinformation online, but it does not specifically mention COVID-19 or 'Doctors for the truth.'\\n\\nInformation 5, published on 2021-09-20, is an article discussing whether doctors spreading COVID-19 misinformation will ever face consequences. It mentions that some top spreaders of misinformation are physicians with active licenses, but it does not specifically mention 'Doctors for the truth.'\\n\\nBased on the available information, the claim is:\\nTRUE\\n\\nReasoning:\\nThe claim, published on 2020-07-25, states that a group called 'Doctors for the truth' shared false or out of context data regarding cures, prevention, and the spread of the virus. Information 1, published on the same day, confirms this claim by fact-checking the press conference and finding the doctors' arguments to be false or without scientific evidence. The other information sources do not contradict this claim and do not provide any evidence that would change the classification.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加related url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = load_train()\n",
    "\n",
    "for item in data_train:\n",
    "    related_url = \"\"\n",
    "    article_id = item[\"related_articles\"][-1]\n",
    "    with open(f\"/home/hanlv/workspace/data/machine_learning/dataset/research/misinformation_dataset/COVMIS-main/articles/{article_id}.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    s = \"\"\n",
    "    now = len(lines) - 1\n",
    "    while now >=0 and lines[now].strip == \"\":\n",
    "        now -= 1\n",
    "\n",
    "    s = lines[now]\n",
    "    st = s.find(\"article_url:\")\n",
    "    if st != -1:\n",
    "        related_url = s[st + len(\"article_url:\"):].strip()\n",
    "    item[\"related_url\"] = related_url\n",
    "    \n",
    "\n",
    "# with open(\"covmis_data/train.json\", \"w\") as f:\n",
    "#     json.dump(data_train, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video makes several misleading claims over COVID-19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(609, 154, 763)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_data = load_diff(\"llama3\", 1)\n",
    "data_train = load_train()\n",
    "data_type1 = load_type1()\n",
    "data_type2 = load_type2()\n",
    "\n",
    "claim_dict = {}\n",
    "for item in diff_data:\n",
    "    if item[\"match2\"].lower() == \"no\":\n",
    "        claim_dict[item[\"claim\"].strip()] = 1\n",
    "\n",
    "cnt = 0\n",
    "cnt2 = 0\n",
    "for i, item in enumerate(data_search):\n",
    "    claim = item[\"claim\"]\n",
    "    if int(data_train[i][\"label\"]) == 1:\n",
    "        continue\n",
    "    # ok = False\n",
    "    for search_result_id in range(2):\n",
    "        if len(item[f\"{search_engine}_search_results\"][\"web\"][\"results\"]) == search_result_id:\n",
    "            break\n",
    "        title_bak = item[f\"{search_engine}_search_results\"][\"web\"][\"results\"][search_result_id][\"title\"]\n",
    "        title = title_bak\n",
    "        url = item[f\"{search_engine}_search_results\"][\"web\"][\"results\"][search_result_id][\"url\"]\n",
    "        check_list = [\n",
    "            \"factcheck.afp.com\", \"reuters.com\", \n",
    "            \"africacheck.org\"\n",
    "        ]\n",
    "\n",
    "        ok = False\n",
    "        for site in check_list:\n",
    "            if site in url:\n",
    "                ok = True\n",
    "                break\n",
    "        if not ok:\n",
    "            continue\n",
    "\n",
    "        # cnt += 1\n",
    "        pos_st = title.find(claim.split(' ')[0])\n",
    "        if pos_st == -1:\n",
    "            continue\n",
    "        pos_en = len(title)\n",
    "        if title.endswith(\" ...\"):\n",
    "            pos_en -= 4\n",
    "        elif title.endswith(\"...\"):\n",
    "            pos_en -= 3 \n",
    "\n",
    "        title = title[pos_st : pos_en]\n",
    "        pos_en = len(title)\n",
    "\n",
    "        # [\"| Reuters\", \"| Fact Check\", \"| Fact\", \"|\"]\n",
    "        pos_verical_bar = title.rfind(\"|\")\n",
    "        if pos_verical_bar != -1:\n",
    "            pos_en = pos_verical_bar\n",
    "\n",
    "        if len(title[:pos_en].split()) >= 5 and title[:pos_en].strip() in claim:\n",
    "            if claim_dict.get(claim.strip()) is not None:\n",
    "                cnt += 1\n",
    "                # print(claim)\n",
    "                if data_train[i][\"label\"] == 0 and not item_exist(data_train[i], data_type2):\n",
    "                    data_train[i][\"label\"] = 2\n",
    "                    update_data_type1(data_train[i], data_type1)\n",
    "                    # print(claim)\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                cnt2 += 1\n",
    "                \n",
    "                if data_train[i][\"label\"] == 0:\n",
    "                    \n",
    "                    if not title_bak.lower().startswith(\"false claim\"):\n",
    "                        if (title_bak.lower().startswith(\"fact check:\") and \"reuters.com\" in url) or \\\n",
    "                            (\"africacheck.org\" in url):\n",
    "                            data_train[i][\"label\"] = 2\n",
    "                            update_data_type1(data_train[i], data_type1)\n",
    "                            # print(claim)\n",
    "                        else:\n",
    "                            pass\n",
    "                else:\n",
    "                    pass\n",
    "            break\n",
    "\n",
    "cnt, cnt2, cnt + cnt2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "673"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_type1 = load_type1()\n",
    "data_type2 = load_type2()\n",
    "\n",
    "len(data_type1) + len(data_type2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "703"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save_type1(data_type1)\n",
    "\n",
    "# with open(\"covmis_data/train.json\", \"w\") as f:\n",
    "#     json.dump(data_train, f, indent=4)\n",
    "\n",
    "# len(data_type1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
