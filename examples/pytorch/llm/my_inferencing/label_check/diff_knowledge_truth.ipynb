{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "dirs = [\"../..\", \"../create_prompt_llm\"]\n",
    "for _dir in dirs:\n",
    "    if _dir not in sys.path:\n",
    "        sys.path.append(_dir)\n",
    "import prompt_rag\n",
    "\n",
    "search_engine = \"brave\"\n",
    "\n",
    "\n",
    "with open(f\"/home/hanlv/workspace/data/machine_learning/dataset/research/misinformation_dataset/COVMIS-main/data/train_{search_engine}_search.json\", \"r\") as f:\n",
    "    data_search = json.load(f)\n",
    "try:\n",
    "    with open(f\"/home/hanlv/workspace/data/machine_learning/dataset/research/misinformation_dataset/COVMIS-main/data/train_{search_engine}_search_llm.json\", \"r\") as f:\n",
    "        data_search_llm = json.load(f)\n",
    "except:\n",
    "    data_search_llm = [{\n",
    "        \"claim\": i[\"claim\"],\n",
    "        \"claimant\": i[\"claimant\"],\n",
    "        \"label\": i[\"label\"],\n",
    "        \"date\": i[\"date\"],\n",
    "    } for i in data_search]\n",
    "    with open(f\"/home/hanlv/workspace/data/machine_learning/dataset/research/misinformation_dataset/COVMIS-main/data/train_{search_engine}_search_llm.json\", \"w\") as f:\n",
    "        json.dump(data_search_llm, f, indent=4)\n",
    "\n",
    "def save_diff(x, model_name, diff_version):\n",
    "    with open(f\"output/diff_{model_name}_v{diff_version}.json\", \"w\") as f:\n",
    "        json.dump(x, f, indent=4)\n",
    "\n",
    "def load_diff(model_name, diff_version):\n",
    "    try:\n",
    "        with open(f\"output/diff_{model_name}_v{diff_version}.json\", \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except:\n",
    "        data_init = [{\"claim\": i[\"claim\"], \"label\": int(i[\"label\"])} for i in data_search_llm]\n",
    "        with open(f\"output/diff_{model_name}_v{diff_version}.json\", \"w\") as f:\n",
    "            json.dump(data_init, f, indent=4)\n",
    "        return data_init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:33:58,973 - modelscope - INFO - PyTorch version 2.3.0 Found.\n",
      "2024-05-10 12:33:58,975 - modelscope - INFO - Loading ast index from /home/hanlv/.cache/modelscope/ast_indexer\n",
      "2024-05-10 12:33:59,001 - modelscope - INFO - Loading done! Current index file version is 1.14.0, with md5 2a8987246d4b67f321effea600434525 and a total number of 976 components indexed\n",
      "[INFO:swift] Successfully registered `/home/hanlv/workspace/code/research/infodemic/LLM/swift/swift/llm/data/dataset_info.json`\n",
      "[INFO:swift] Loading the model using model_dir: /home/css/models/Mixtral-8x7B-Instruct-v0.1-GPTQ-int4\n",
      "[INFO:swift] Setting torch_dtype: torch.float16\n",
      "[INFO:swift] model_config: MixtralConfig {\n",
      "  \"_name_or_path\": \"/home/css/models/Mixtral-8x7B-Instruct-v0.1-GPTQ-int4\",\n",
      "  \"architectures\": [\n",
      "    \"MixtralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mixtral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_experts_per_tok\": 2,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_local_experts\": 8,\n",
      "  \"output_router_logits\": false,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"bits\": 4,\n",
      "    \"damp_percent\": 0.1,\n",
      "    \"desc_act\": true,\n",
      "    \"group_size\": 32,\n",
      "    \"model_file_base_name\": \"model\",\n",
      "    \"model_name_or_path\": null,\n",
      "    \"modules_in_block_to_quantize\": [\n",
      "      [\n",
      "        \"self_attn.k_proj\",\n",
      "        \"self_attn.v_proj\",\n",
      "        \"self_attn.q_proj\"\n",
      "      ],\n",
      "      [\n",
      "        \"self_attn.o_proj\"\n",
      "      ],\n",
      "      [\n",
      "        \"block_sparse_moe.experts.0.w1\",\n",
      "        \"block_sparse_moe.experts.0.w2\",\n",
      "        \"block_sparse_moe.experts.0.w3\"\n",
      "      ],\n",
      "      [\n",
      "        \"block_sparse_moe.experts.1.w1\",\n",
      "        \"block_sparse_moe.experts.1.w2\",\n",
      "        \"block_sparse_moe.experts.1.w3\"\n",
      "      ],\n",
      "      [\n",
      "        \"block_sparse_moe.experts.2.w1\",\n",
      "        \"block_sparse_moe.experts.2.w2\",\n",
      "        \"block_sparse_moe.experts.2.w3\"\n",
      "      ],\n",
      "      [\n",
      "        \"block_sparse_moe.experts.3.w1\",\n",
      "        \"block_sparse_moe.experts.3.w2\",\n",
      "        \"block_sparse_moe.experts.3.w3\"\n",
      "      ],\n",
      "      [\n",
      "        \"block_sparse_moe.experts.4.w1\",\n",
      "        \"block_sparse_moe.experts.4.w2\",\n",
      "        \"block_sparse_moe.experts.4.w3\"\n",
      "      ],\n",
      "      [\n",
      "        \"block_sparse_moe.experts.5.w1\",\n",
      "        \"block_sparse_moe.experts.5.w2\",\n",
      "        \"block_sparse_moe.experts.5.w3\"\n",
      "      ],\n",
      "      [\n",
      "        \"block_sparse_moe.experts.6.w1\",\n",
      "        \"block_sparse_moe.experts.6.w2\",\n",
      "        \"block_sparse_moe.experts.6.w3\"\n",
      "      ],\n",
      "      [\n",
      "        \"block_sparse_moe.experts.7.w1\",\n",
      "        \"block_sparse_moe.experts.7.w2\",\n",
      "        \"block_sparse_moe.experts.7.w3\"\n",
      "      ]\n",
      "    ],\n",
      "    \"quant_method\": \"gptq\",\n",
      "    \"sym\": true,\n",
      "    \"true_sequential\": true\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"router_aux_loss_coef\": 0.02,\n",
      "  \"router_jitter_noise\": 0.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-10 12:33:59 config.py:1086] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 05-10 12:33:59 config.py:177] The model is convertible to Marlin format. Using Marlin kernel.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:34:02,736\tINFO worker.py:1749 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-10 12:34:03 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='/home/css/models/Mixtral-8x7B-Instruct-v0.1-GPTQ-int4', speculative_config=None, tokenizer='/home/css/models/Mixtral-8x7B-Instruct-v0.1-GPTQ-int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=42, served_model_name=/home/css/models/Mixtral-8x7B-Instruct-v0.1-GPTQ-int4)\n",
      "INFO 05-10 12:34:08 utils.py:660] Found nccl from library /home/hanlv/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=2574827)\u001b[0m INFO 05-10 12:34:08 utils.py:660] Found nccl from library /home/hanlv/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-10 12:34:08 selector.py:81] Cannot use FlashAttention-2 backend because the flash_attn package is not found. Please install it for better performance.\n",
      "INFO 05-10 12:34:08 selector.py:32] Using XFormers backend.\n",
      "\u001b[36m(RayWorkerWrapper pid=2574827)\u001b[0m INFO 05-10 12:34:09 selector.py:81] Cannot use FlashAttention-2 backend because the flash_attn package is not found. Please install it for better performance.\n",
      "\u001b[36m(RayWorkerWrapper pid=2574827)\u001b[0m INFO 05-10 12:34:09 selector.py:32] Using XFormers backend.\n",
      "INFO 05-10 12:34:11 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=2574827)\u001b[0m INFO 05-10 12:34:11 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "INFO 05-10 12:34:12 utils.py:132] reading GPU P2P access cache from /home/hanlv/.config/vllm/gpu_p2p_access_cache_for_1,2.json\n",
      "WARNING 05-10 12:34:12 custom_all_reduce.py:74] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[36m(RayWorkerWrapper pid=2574827)\u001b[0m INFO 05-10 12:34:12 utils.py:132] reading GPU P2P access cache from /home/hanlv/.config/vllm/gpu_p2p_access_cache_for_1,2.json\n",
      "\u001b[36m(RayWorkerWrapper pid=2574827)\u001b[0m WARNING 05-10 12:34:12 custom_all_reduce.py:74] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[36m(RayWorkerWrapper pid=2574827)\u001b[0m INFO 05-10 12:34:16 model_runner.py:175] Loading model weights took 12.4631 GB\n",
      "INFO 05-10 12:34:16 model_runner.py:175] Loading model weights took 12.4631 GB\n",
      "INFO 05-10 12:34:29 distributed_gpu_executor.py:45] # GPU blocks: 2735, # CPU blocks: 4096\n",
      "INFO 05-10 12:34:30 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-10 12:34:30 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerWrapper pid=2574827)\u001b[0m INFO 05-10 12:34:31 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerWrapper pid=2574827)\u001b[0m INFO 05-10 12:34:31 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerWrapper pid=2574827)\u001b[0m INFO 05-10 12:34:36 model_runner.py:1017] Graph capturing finished in 5 secs.\n",
      "INFO 05-10 12:34:36 model_runner.py:1017] Graph capturing finished in 6 secs.\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2'\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:256'\n",
    "\n",
    "from swift.llm import (\n",
    "    ModelType, get_vllm_engine, get_default_template_type,\n",
    "    get_template, inference_vllm, VllmGenerationConfig\n",
    ")\n",
    "from custom import CustomModelType, CustomTemplateType\n",
    "\n",
    "model_type = CustomModelType.mixtral_moe_7b_instruct_gptq_int4\n",
    "# model_type = ModelType.llama3_70b_instruct_int4\n",
    "\n",
    "llm_engine = get_vllm_engine(\n",
    "    model_type, \n",
    "    torch_dtype=torch.float16,  # 检查正确的数据类型！！！！\n",
    "    tensor_parallel_size=2,\n",
    "    # max_model_len=4096,\n",
    "    # gpu_memory_utilization=0.95,\n",
    "    engine_kwargs = {\n",
    "        # \"enforce_eager\": True,\n",
    "        \"max_num_seqs\": 32,\n",
    "        \"seed\": 42,\n",
    "    }\n",
    ")\n",
    "\n",
    "template_type = get_default_template_type(model_type)\n",
    "template = get_template(template_type, llm_engine.hf_tokenizer)\n",
    "\n",
    "generation_config = VllmGenerationConfig(\n",
    "    max_new_tokens=4096,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "get_resp_list = lambda request_list : inference_vllm(\n",
    "    llm_engine, template, request_list, \n",
    "    generation_config=generation_config, \n",
    "    use_tqdm=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先用llama3根据检索到的内容给出每个claim的结论\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_for_diff_conclusions(\n",
    "        claim, claim_date, search_engine, search_results,\n",
    "        model_name=\"llama3\", K=5, ids=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    为了验证先验知识和label的差异性，先对检索到的信息进行总结\n",
    "    \"\"\"\n",
    "    claim = claim.strip()\n",
    "    if model_name == \"llama3\":\n",
    "        pre = \"Below is a claim and some information searched online. These pieces of Information are relevant to the claim. This claim and all information include their respective publication dates and contents. To classify the claim more accurately (if the content described by the claim is correct, it will be classified as true; if the content described by the claim is incorrect, it will be classified as false), please judge the correctness of the claim based on the available information and your knowledge. In reasoning, it is necessary to consider the sequential relationship between the date of publication of the claim and the date of publication of the information. Just provide the final conclusion without explaining the reasoning process. \\n\\n\"\n",
    "    elif model_name == \"mixtral\":\n",
    "        pre = \"Below is a claim and some information searched online. These pieces of Information are relevant to the claim. This claim and all information include their respective publication dates and contents. To classify the claim more accurately (if the content described by the claim is correct, it will be classified as true; if the content described by the claim is incorrect, it will be classified as false), please judge the correctness of the claim based on the available information and your knowledge. In reasoning, it is necessary to consider the sequential relationship between the date of publication of the claim and the date of publication of the information. Just provide the brief final conclusion without explaining the reasoning process. \\n\\n\"\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Select model name in [\\\"llama3\\\"].\")\n",
    "    \n",
    "    if search_engine == \"brave\":\n",
    "        if ids is None:\n",
    "            ids = slice(0, K)\n",
    "        snippet = prompt_rag.get_brave_snippet(search_results, ids=ids)\n",
    "    else:\n",
    "        raise Exception(\"Select search engine in [\\\"brave\\\"].\")\n",
    "    \n",
    "    text = \"Claim:\" + prompt_rag.get_claim_with_date(claim, claim_date) +'\\n\\n'\n",
    "    info = \"Information:\\n\" + snippet\n",
    "    return pre + text + info\n",
    "\n",
    "def update_conclusions(\n",
    "        model_name, diff_version, get_resp_list, search_engine, data_search, K=5):\n",
    "    \n",
    "    data_diff = load_diff(model_name, diff_version)\n",
    "\n",
    "    len_data_diff = 0\n",
    "    for i, item in enumerate(data_diff):\n",
    "        if item.get(\"conclusion\") is None:\n",
    "            len_data_diff = i\n",
    "            break\n",
    "\n",
    "    prompt_list = []\n",
    "    for i in range(len_data_diff, len(data_diff)):\n",
    "        item = data_search[i]\n",
    "        ids = None\n",
    "        prompt = get_prompt_for_diff_conclusions(\n",
    "            item[\"claim\"], item[\"date\"], \n",
    "            search_engine, item[f\"{search_engine}_search_results\"], \n",
    "            model_name, K=K, ids=ids\n",
    "        )\n",
    "        prompt_list.append(prompt)\n",
    "    request_list = [{'query': prompt} for prompt in prompt_list]\n",
    "    resp_list = get_resp_list(request_list)\n",
    "    resp_list = [i[\"response\"] for i in resp_list]\n",
    "\n",
    "    for i in range(len_data_diff, len(data_diff)):\n",
    "        data_diff[i][f\"conclusion\"] = resp_list[i - len_data_diff].strip()\n",
    "\n",
    "    save_diff(data_diff, model_name, diff_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"llama3\" # llama3\n",
    "diff_version = 1\n",
    "\n",
    "# update_conclusions(model_name, diff_version, get_resp_list, search_engine, data_search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_diff(model_name, diff_version, get_resp_list):\n",
    "\n",
    "    data_diff = load_diff(model_name, diff_version)\n",
    "\n",
    "    len_data_diff = 0\n",
    "    for i, item in enumerate(data_diff):\n",
    "        if item.get(\"match\") is None:\n",
    "            len_data_diff = i\n",
    "            break\n",
    "\n",
    "    prompt_list = []\n",
    "    for i in range(len_data_diff, len(data_diff)):\n",
    "\n",
    "        item = data_diff[i]\n",
    "        claim = item[\"claim\"]\n",
    "        label = int(item[\"label\"])\n",
    "        if not isinstance(label, int):\n",
    "            raise Exception(f\"Label 不是 int 类型。\\n{claim}\")\n",
    "        \n",
    "        if label == 1:\n",
    "            prompt_list.append(\"Do not output.\")\n",
    "            continue\n",
    "\n",
    "        answer = item[\"conclusion\"]\n",
    "\n",
    "        if label == 2:\n",
    "            ground_truth = f'The claim({claim}) is true.'\n",
    "        elif label == 0:\n",
    "            ground_truth = f'The claim({claim}) is false.'\n",
    "        else:\n",
    "            # label为1不做计算，ground_truth是什么无所谓\n",
    "            pass\n",
    "        \n",
    "        if model_name == \"llama3\":\n",
    "            prompt = f\"Does the answer given below match the ground truth? If it does, please answer \\\"Yes\\\"; otherwise, answer \\\"No\\\".\\nGround truth: {ground_truth}\\nAnswer:\\n{answer}\"\n",
    "        elif model_name == \"mixtral\":\n",
    "            prompt = f\"Does the answer given below match the ground truth? If it does, please only answer \\\"Yes\\\"; otherwise, only answer \\\"No\\\". And don't explain the reason. \\nGround truth: {ground_truth}\\nAnswer:\\n{answer}\"\n",
    "        else:\n",
    "            raise Exception()\n",
    "        prompt_list.append(prompt)\n",
    "    request_list = [{'query': prompt} for prompt in prompt_list]\n",
    "    resp_list = get_resp_list(request_list)\n",
    "    resp_list = [i[\"response\"] for i in resp_list]\n",
    "\n",
    "    for i in range(len_data_diff, len(data_diff)):\n",
    "        item = data_diff[i]\n",
    "        claim = item[\"claim\"]\n",
    "        label = int(item[\"label\"])\n",
    "        if not isinstance(label, int):\n",
    "            raise Exception(f\"Label 不是 int 类型。\\n{claim}\")\n",
    "        \n",
    "        if label == 1:\n",
    "            data_diff[i][f\"match\"] = 'None'\n",
    "        else:\n",
    "            data_diff[i][f\"match\"] = resp_list[i - len_data_diff].strip()\n",
    "\n",
    "    save_diff(data_diff, model_name, diff_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"llama3\" # llama3\n",
    "diff_version = 1\n",
    "\n",
    "# update_diff(model_name, diff_version, get_resp_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "手动判定match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_v2 = \"match2\"\n",
    "model_name = \"llama3\" # llama3\n",
    "diff_version = 1\n",
    "data_diff = load_diff(model_name, diff_version)\n",
    "\n",
    "cnt_false = 0\n",
    "cnt_true = 0\n",
    "\n",
    "for i in range(len(data_diff)):\n",
    "    item = data_diff[i]\n",
    "    claim = item[\"claim\"]\n",
    "    label = int(item[\"label\"])\n",
    "    if not isinstance(label, int):\n",
    "        raise Exception(f\"Label 不是 int 类型。\\n{claim}\")\n",
    "    # print(item)\n",
    "    if \"true\" in item[\"conclusion\"].lower():\n",
    "        pred = 2\n",
    "    elif \"false\" in item[\"conclusion\"].lower() or \"unverified\" in item[\"conclusion\"].lower()\\\n",
    "        or \"unverifiable\" in item[\"conclusion\"].lower():\n",
    "        pred = 0\n",
    "    else:\n",
    "        pred = 0\n",
    "    if label == 1:\n",
    "        data_diff[i][match_v2] = 'None'\n",
    "    else:\n",
    "        if pred == label:\n",
    "            data_diff[i][match_v2] = \"Yes\"\n",
    "        else:\n",
    "            data_diff[i][match_v2] = \"No\"\n",
    "cnt_true, cnt_false\n",
    "# save_diff(data_diff, model_name, diff_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0, 'conclusion': 'Based on the available information, I classify the claim as UNVERIFIED.', 'match': 'No', 'match2': 'Yes'}\n",
      "{'label': 2, 'conclusion': 'TRUE', 'match': 'No', 'match2': 'Yes'}\n",
      "{'label': 0, 'conclusion': 'Based on the available information, I conclude that the claim is UNVERIFIED.', 'match': 'No', 'match2': 'Yes'}\n",
      "{'label': 0, 'conclusion': 'Based on the available information, I conclude that the claim is FALSE.', 'match': 'No', 'match2': 'Yes'}\n",
      "{'label': 0, 'conclusion': 'Based on the available information, I conclude that the claim is PARTLY FALSE.', 'match': 'No', 'match2': 'Yes'}\n",
      "{'label': 0, 'conclusion': 'Based on the available information, I conclude that the claim is FALSE.', 'match': 'No', 'match2': 'Yes'}\n",
      "{'label': 0, 'conclusion': 'Based on the available information, I conclude that the claim is FALSE.', 'match': 'No', 'match2': 'Yes'}\n",
      "{'label': 0, 'conclusion': 'Based on the available information, I conclude that the claim \"WHO confirmed COVID-19 as airborne\" with a publication date of 2020-03-22 is FALSE.\\n\\nThe earliest information provided, Information 2, dated 2020-07-09, indicates that WHO did not confirm COVID-19 as airborne at that time. Instead, it stated that COVID-19 virus is primarily transmitted between people through respiratory droplets and contact routes, and airborne transmission was not reported.\\n\\nIt wasn\\'t until later, as shown in Information 1, dated 2021-05-09, that WHO finally admitted that Coronavirus is airborne. This admission came more than a year after the claim\\'s publication date.', 'match': 'No', 'match2': 'Yes'}\n",
      "{'label': 0, 'conclusion': 'Based on the available information, I conclude that the claim is UNVERIFIED.', 'match': 'No', 'match2': 'Yes'}\n",
      "{'label': 2, 'conclusion': 'TRUE', 'match': 'No', 'match2': 'Yes'}\n",
      "{'label': 0, 'conclusion': 'Based on the available information, I conclude that the claim is FALSE.', 'match': 'No', 'match2': 'Yes'}\n",
      "{'label': 0, 'conclusion': 'Based on the available information, I classify the claim as UNVERIFIED.', 'match': 'No', 'match2': 'Yes'}\n",
      "{'label': 0, 'conclusion': 'Based on the available information, I conclude that the claim is:\\n\\n**UNVERIFIED**\\n\\nThe claim states that two persons with coronavirus escaped from a hospital in Bogota, but the provided information does not specifically confirm or deny this incident. While there are reports of coronavirus patients escaping from hospitals in other locations, there is no direct evidence to support the claim about Bogota.', 'match': 'No', 'match2': 'Yes'}\n",
      "{'label': 0, 'conclusion': 'Based on the provided information, I conclude that the claim is: UNCLASSIFIABLE', 'match': 'No', 'match2': 'Yes'}\n",
      "{'label': 2, 'conclusion': 'Based on the available information, I conclude that the claim \"Claims that living at high altitudes protects against COVID-19 ‘speculative\\'\" is TRUE.', 'match': 'No', 'match2': 'Yes'}\n",
      "{'label': 0, 'conclusion': 'Based on the available information, I conclude that the claim is: PARTLY FALSE', 'match': 'No', 'match2': 'Yes'}\n",
      "{'label': 2, 'conclusion': 'Based on the available information, I conclude that the claim \"COVID-19 Lockdown Increases Child Abuse Risk\" is **PARTIALLY TRUE**.\\n\\nWhile some studies and experts suggest that the COVID-19 pandemic and lockdown measures may increase the risk of child abuse due to increased stress, isolation, and economic hardship, others found mixed or contradictory results. Some studies reported an increase in child maltreatment, while others found a decrease or no significant change.\\n\\nThe information provided highlights the complexity of the issue, with various factors contributing to the risk of child abuse, including parental stress, economic hardship, and social isolation. However, the evidence is not conclusive, and more research is needed to fully understand the impact of the COVID-19 pandemic on child abuse risk.', 'match': 'No', 'match2': 'Yes'}\n",
      "{'label': 0, 'conclusion': 'Based on the available information, I classify the claim as: UNVERIFIED', 'match': 'No', 'match2': 'Yes'}\n",
      "{'label': 0, 'conclusion': 'Based on the available information, I conclude that the claim \"WHO confirmed COVID-19 as airborne\" with a publication date of 2020-03-22 is FALSE.\\n\\nThe earliest information provided, Information 2, dated 2020-07-09, indicates that WHO did not confirm COVID-19 as airborne at that time. Instead, it stated that COVID-19 virus is primarily transmitted between people through respiratory droplets and contact routes, and airborne transmission was not reported.\\n\\nIt wasn\\'t until later, as shown in Information 1, dated 2021-05-09, that WHO finally admitted that Coronavirus is airborne. This admission came more than a year after the claim\\'s publication date.', 'match': 'No', 'match2': 'Yes'}\n",
      "{'label': 0, 'conclusion': \"Based on the available information, I conclude that the claim is:\\n\\n**UNVERIFIED**\\n\\nThe claim mentions a person fainting in Cali's bus terminal due to COVID-19, but none of the provided information specifically confirms or denies this incident. While the information does discuss COVID-19 cases, hospitalizations, and deaths in California, it does not provide direct evidence to support or refute the claim.\", 'match': 'No', 'match2': 'Yes'}\n",
      "{'label': 0, 'conclusion': 'Based on the available information, I conclude that the claim \"COVID-19 is not true and there is no evidence of a medical pandemic\" is FALSE.', 'match': 'Yes', 'match2': 'No'}\n",
      "{'label': 0, 'conclusion': 'Based on the available information, I conclude that the claim is UNVERIFIED.', 'match': 'No', 'match2': 'Yes'}\n",
      "{'label': 0, 'conclusion': 'Based on the available information, I conclude that the claim is:\\n\\n**UNVERIFIABLE**\\n\\nThe claim does not provide enough information to be verified or falsified based on the available online information.', 'match': 'No', 'match2': 'Yes'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查match和match2冲突的部分\n",
    "# 最终生成match_final\n",
    "\n",
    "data_diff = load_diff(model_name, diff_version)\n",
    "cnt = 0\n",
    "for i in range(len(data_diff)):\n",
    "    item = data_diff[i]\n",
    "    claim = item[\"claim\"]\n",
    "    label = int(item[\"label\"])\n",
    "    if  item[\"match2\"] != item[\"match\"]:\n",
    "        if label == 0 and item['conclusion'] == 'TRUE' and item[\"match2\"] == 'No':\n",
    "            continue\n",
    "        item.pop(\"claim\")\n",
    "        print(item)\n",
    "cnt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
