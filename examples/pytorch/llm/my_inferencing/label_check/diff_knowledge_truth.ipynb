{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "dirs = [\"../..\", \"../create_prompt_llm\"]\n",
    "for _dir in dirs:\n",
    "    if _dir not in sys.path:\n",
    "        sys.path.append(_dir)\n",
    "import prompt_rag\n",
    "\n",
    "search_engine = \"brave\"\n",
    "\n",
    "\n",
    "with open(f\"/home/hanlv/workspace/data/machine_learning/dataset/research/misinformation_dataset/COVMIS-main/data/train_{search_engine}_search.json\", \"r\") as f:\n",
    "    data_search = json.load(f)\n",
    "try:\n",
    "    with open(f\"/home/hanlv/workspace/data/machine_learning/dataset/research/misinformation_dataset/COVMIS-main/data/train_{search_engine}_search_llm.json\", \"r\") as f:\n",
    "        data_search_llm = json.load(f)\n",
    "except:\n",
    "    data_search_llm = [{\n",
    "        \"claim\": i[\"claim\"],\n",
    "        \"claimant\": i[\"claimant\"],\n",
    "        \"label\": i[\"label\"],\n",
    "        \"date\": i[\"date\"],\n",
    "    } for i in data_search]\n",
    "    with open(f\"/home/hanlv/workspace/data/machine_learning/dataset/research/misinformation_dataset/COVMIS-main/data/train_{search_engine}_search_llm.json\", \"w\") as f:\n",
    "        json.dump(data_search_llm, f, indent=4)\n",
    "\n",
    "def save_diff(x, model_name, diff_version):\n",
    "    with open(f\"output/diff_{model_name}_v{diff_version}.json\", \"w\") as f:\n",
    "        json.dump(x, f, indent=4)\n",
    "\n",
    "def load_diff(model_name, diff_version):\n",
    "    try:\n",
    "        with open(f\"output/diff_{model_name}_v{diff_version}.json\", \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except:\n",
    "        data_init = [{\"claim\": i[\"claim\"], \"label\": int(i[\"label\"])} for i in data_search_llm]\n",
    "        with open(f\"output/diff_{model_name}_v{diff_version}.json\", \"w\") as f:\n",
    "            json.dump(data_init, f, indent=4)\n",
    "        return data_init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-01 18:32:22,286 - modelscope - INFO - PyTorch version 2.2.1 Found.\n",
      "2024-05-01 18:32:22,289 - modelscope - INFO - Loading ast index from /home/hanlv/.cache/modelscope/ast_indexer\n",
      "2024-05-01 18:32:22,380 - modelscope - INFO - Loading done! Current index file version is 1.14.0, with md5 543353d790c7c1e5afd7a4dcd3609c68 and a total number of 976 components indexed\n",
      "[INFO:swift] Loading the model using model_dir: /home/css/models/Meta-Llama-3-70B-Instruct-GPTQ-Int4\n",
      "[INFO:swift] Setting torch_dtype: torch.float16\n",
      "[INFO:swift] model_config: LlamaConfig {\n",
      "  \"_name_or_path\": \"/home/css/models/Meta-Llama-3-70B-Instruct-GPTQ-Int4\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 8192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 28672,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_hidden_layers\": 80,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"bits\": 4,\n",
      "    \"damp_percent\": 0.1,\n",
      "    \"desc_act\": false,\n",
      "    \"group_size\": 128,\n",
      "    \"modules_in_block_to_quantize\": null,\n",
      "    \"quant_method\": \"gptq\",\n",
      "    \"sym\": true,\n",
      "    \"true_sequential\": true\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-01 18:32:23 config.py:169] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-01 18:32:26,692\tINFO worker.py:1749 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-01 18:32:27 llm_engine.py:98] Initializing an LLM engine (v0.4.1) with config: model='/home/css/models/Meta-Llama-3-70B-Instruct-GPTQ-Int4', speculative_config=None, tokenizer='/home/css/models/Meta-Llama-3-70B-Instruct-GPTQ-Int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-01 18:32:32 utils.py:608] Found nccl from library /home/hanlv/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=97107)\u001b[0m INFO 05-01 18:32:32 utils.py:608] Found nccl from library /home/hanlv/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-01 18:32:32 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.\n",
      "INFO 05-01 18:32:32 selector.py:33] Using XFormers backend.\n",
      "\u001b[36m(RayWorkerWrapper pid=97107)\u001b[0m INFO 05-01 18:32:33 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.\n",
      "\u001b[36m(RayWorkerWrapper pid=97107)\u001b[0m INFO 05-01 18:32:33 selector.py:33] Using XFormers backend.\n",
      "INFO 05-01 18:32:35 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=97107)\u001b[0m INFO 05-01 18:32:35 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "INFO 05-01 18:32:37 utils.py:129] reading GPU P2P access cache from /home/hanlv/.config/vllm/gpu_p2p_access_cache_for_1,2.json\n",
      "WARNING 05-01 18:32:37 custom_all_reduce.py:74] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[36m(RayWorkerWrapper pid=97107)\u001b[0m INFO 05-01 18:32:37 utils.py:129] reading GPU P2P access cache from /home/hanlv/.config/vllm/gpu_p2p_access_cache_for_1,2.json\n",
      "\u001b[36m(RayWorkerWrapper pid=97107)\u001b[0m WARNING 05-01 18:32:37 custom_all_reduce.py:74] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 05-01 18:32:52 model_runner.py:173] Loading model weights took 18.5585 GB\n",
      "\u001b[36m(RayWorkerWrapper pid=97107)\u001b[0m INFO 05-01 18:32:54 model_runner.py:173] Loading model weights took 18.5585 GB\n",
      "INFO 05-01 18:32:57 ray_gpu_executor.py:217] # GPU blocks: 556, # CPU blocks: 1638\n",
      "INFO 05-01 18:32:59 model_runner.py:976] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-01 18:32:59 model_runner.py:980] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerWrapper pid=97107)\u001b[0m INFO 05-01 18:33:00 model_runner.py:976] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerWrapper pid=97107)\u001b[0m INFO 05-01 18:33:00 model_runner.py:980] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerWrapper pid=97107)\u001b[0m INFO 05-01 18:33:04 model_runner.py:1057] Graph capturing finished in 5 secs.\n",
      "INFO 05-01 18:33:05 model_runner.py:1057] Graph capturing finished in 6 secs.\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2'\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:256'\n",
    "\n",
    "from swift.llm import (\n",
    "    ModelType, get_vllm_engine, get_default_template_type,\n",
    "    get_template, inference_vllm, VllmGenerationConfig\n",
    ")\n",
    "from custom import CustomModelType, CustomTemplateType\n",
    "\n",
    "model_type = CustomModelType.llama_3_70b_instruct_gptq_int4\n",
    "# model_type = ModelType.llama3_70b_instruct_int4\n",
    "\n",
    "llm_engine = get_vllm_engine(\n",
    "    model_type, \n",
    "    # torch_dtype=torch.float16,  # 检查正确的数据类型！！！！\n",
    "    tensor_parallel_size=2,\n",
    "    max_model_len=4096,\n",
    "    # gpu_memory_utilization=0.95,\n",
    "    engine_kwargs = {\n",
    "        # \"enforce_eager\": True,\n",
    "        \"max_num_seqs\": 32,\n",
    "        \"seed\": 42,\n",
    "    }\n",
    ")\n",
    "\n",
    "template_type = get_default_template_type(model_type)\n",
    "template = get_template(template_type, llm_engine.hf_tokenizer)\n",
    "\n",
    "generation_config = VllmGenerationConfig(\n",
    "    max_new_tokens=4096,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "get_resp_list = lambda request_list : inference_vllm(\n",
    "    llm_engine, template, request_list, \n",
    "    generation_config=generation_config, \n",
    "    use_tqdm=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先用llama3根据检索到的内容给出每个claim的结论\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_for_diff_conclusions(\n",
    "        claim, claim_date, search_engine, search_results,\n",
    "        model_name=\"llama3\", K=5, ids=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    为了验证先验知识和label的差异性，先对检索到的信息进行总结\n",
    "    \"\"\"\n",
    "    claim = claim.strip()\n",
    "    if model_name == \"llama3\":\n",
    "        pre = \"Below is a claim and some information searched online. These pieces of Information are relevant to the claim. This claim and all information include their respective publication dates and contents. To classify the claim more accurately (if the content described by the claim is correct, it will be classified as true; if the content described by the claim is incorrect, it will be classified as false), please judge the correctness of the claim based on the available information and your knowledge. In reasoning, it is necessary to consider the sequential relationship between the date of publication of the claim and the date of publication of the information. Just provide the final conclusion without explaining the reasoning process. \\n\\n\"\n",
    "    else:\n",
    "        raise Exception(\"Select model name in [\\\"llama3\\\"].\")\n",
    "    \n",
    "    if search_engine == \"brave\":\n",
    "        if ids is None:\n",
    "            ids = slice(0, K)\n",
    "        snippet = prompt_rag.get_brave_snippet(search_results, ids=ids)\n",
    "    else:\n",
    "        raise Exception(\"Select search engine in [\\\"brave\\\"].\")\n",
    "    \n",
    "    text = \"Claim:\" + prompt_rag.get_claim_with_date(claim, claim_date) +'\\n\\n'\n",
    "    info = \"Information:\\n\" + snippet\n",
    "    return pre + text + info\n",
    "\n",
    "def update_conclusions(\n",
    "        model_name, diff_version, get_resp_list, search_engine, data_search, K=5):\n",
    "    \n",
    "    data_diff = load_diff(model_name, diff_version)\n",
    "\n",
    "    len_data_diff = 0\n",
    "    for i, item in enumerate(data_diff):\n",
    "        if item.get(\"conclusion\") is None:\n",
    "            len_data_diff = i\n",
    "            break\n",
    "\n",
    "    prompt_list = []\n",
    "    for i in range(len_data_diff, len(data_diff)):\n",
    "        item = data_search[i]\n",
    "        ids = None\n",
    "        prompt = get_prompt_for_diff_conclusions(\n",
    "            item[\"claim\"], item[\"date\"], \n",
    "            search_engine, item[f\"{search_engine}_search_results\"], \n",
    "            model_name, K=K, ids=ids\n",
    "        )\n",
    "        prompt_list.append(prompt)\n",
    "    request_list = [{'query': prompt} for prompt in prompt_list]\n",
    "    resp_list = get_resp_list(request_list)\n",
    "    resp_list = [i[\"response\"] for i in resp_list]\n",
    "\n",
    "    for i in range(len_data_diff, len(data_diff)):\n",
    "        data_diff[i][f\"conclusion\"] = resp_list[i - len_data_diff].strip()\n",
    "\n",
    "    save_diff(data_diff, model_name, diff_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"llama3\"\n",
    "diff_version = 1\n",
    "\n",
    "# update_conclusions(model_name, diff_version, get_resp_list, search_engine, data_search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_diff(model_name, diff_version, get_resp_list):\n",
    "\n",
    "    data_diff = load_diff(model_name, diff_version)\n",
    "\n",
    "    len_data_diff = 0\n",
    "    for i, item in enumerate(data_diff):\n",
    "        if item.get(\"match\") is None:\n",
    "            len_data_diff = i\n",
    "            break\n",
    "\n",
    "    prompt_list = []\n",
    "    for i in range(len_data_diff, len(data_diff)):\n",
    "\n",
    "        item = data_diff[i]\n",
    "        claim = item[\"claim\"]\n",
    "        label = int(item[\"label\"])\n",
    "        if not isinstance(label, int):\n",
    "            raise Exception(f\"Label 不是 int 类型。\\n{claim}\")\n",
    "        \n",
    "        if label == 1:\n",
    "            prompt_list.append(\"Do not output.\")\n",
    "            continue\n",
    "\n",
    "        answer = item[\"conclusion\"]\n",
    "\n",
    "        if label == 2:\n",
    "            ground_truth = f'The claim({claim}) is true.'\n",
    "        elif label == 0:\n",
    "            ground_truth = f'The claim({claim}) is false.'\n",
    "        else:\n",
    "            # label为1不做计算，ground_truth是什么无所谓\n",
    "            pass\n",
    "\n",
    "        prompt = f\"Does the answer given below match the ground truth? If it does, please answer \\\"Yes\\\"; otherwise, answer \\\"No\\\".\\nGround truth: {ground_truth}\\nAnswer:\\n{answer}\"\n",
    "        \n",
    "        prompt_list.append(prompt)\n",
    "    request_list = [{'query': prompt} for prompt in prompt_list]\n",
    "    resp_list = get_resp_list(request_list)\n",
    "    resp_list = [i[\"response\"] for i in resp_list]\n",
    "\n",
    "    for i in range(len_data_diff, len(data_diff)):\n",
    "        item = data_diff[i]\n",
    "        claim = item[\"claim\"]\n",
    "        label = int(item[\"label\"])\n",
    "        if not isinstance(label, int):\n",
    "            raise Exception(f\"Label 不是 int 类型。\\n{claim}\")\n",
    "        \n",
    "        if label == 1:\n",
    "            data_diff[i][f\"match\"] = 'None'\n",
    "        else:\n",
    "            data_diff[i][f\"match\"] = resp_list[i - len_data_diff].strip()\n",
    "\n",
    "    save_diff(data_diff, model_name, diff_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14384/14384 [15:36<00:00, 15.36it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"llama3\"\n",
    "diff_version = 1\n",
    "\n",
    "# update_diff(model_name, diff_version, get_resp_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
