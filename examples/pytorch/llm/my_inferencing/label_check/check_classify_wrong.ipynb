{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "search_engine = \"brave\"\n",
    "model_name = 'llama3'\n",
    "\n",
    "with open(f\"/home/hanlv/workspace/data/machine_learning/dataset/research/misinformation_dataset/COVMIS-2024/train_{search_engine}_search.json\", \"r\") as f:\n",
    "    data_search = json.load(f)\n",
    "with open(\"covmis_data/train.json\", \"r\") as f:\n",
    "    data_train = json.load(f)\n",
    "\n",
    "def load_diff(model_name, diff_version):\n",
    "    with open(f\"output/diff_{model_name}_v{diff_version}.json\", \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_train():\n",
    "    with open(\"covmis_data/train.json\", \"r\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def item_exist(x, data):\n",
    "    for item in data:\n",
    "        if item[\"id\"] == x[\"id\"]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def load_type1():\n",
    "    with open(\"output/type_1.json\", \"r\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def load_type2():\n",
    "    with open(\"output/type_2.json\", \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_type1(x):\n",
    "    with open(\"output/type_1.json\", \"w\") as f:\n",
    "        json.dump(x, f, indent=4)\n",
    "        \n",
    "def save_type2(x):\n",
    "    with open(\"output/type_2.json\", \"w\") as f:\n",
    "        json.dump(x, f, indent=4)\n",
    "\n",
    "def save_excel(file_name, ids, claims, labels, related_urls, preds=None, human_checks=None):\n",
    "\n",
    "    if preds:\n",
    "        df = pd.DataFrame(columns=[\"id\", \"claim\", \"label\", \"related_url\", \"prediction\"])\n",
    "        # data ={'claim': claims, 'label':labels, \"related_url\":related_urls, 'prediction':preds}\n",
    "        for i, row in enumerate(claims):\n",
    "            df.loc[i] = [ids[i], claims[i], labels[i], related_urls[i], preds[i]]\n",
    "    elif human_checks:\n",
    "        df = pd.DataFrame(columns=[\"id\", \"claim\", \"label\", \"related_url\", \"human_check\"])\n",
    "        # data ={'claim': claims, 'label':labels, \"related_url\":related_urls, 'human_check':human_checks}\n",
    "        for i, row in enumerate(claims):\n",
    "            df.loc[i] = [ids[i], claims[i], labels[i], related_urls[i], human_checks[i]]\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=[\"id\", \"claim\", \"label\", \"related_url\"])\n",
    "        # data ={'claim': claims, 'label':labels, \"related_url\":related_urls}\n",
    "        for i, row in enumerate(claims):\n",
    "            df.loc[i] = [ids[i], claims[i], labels[i], related_urls[i]]\n",
    "    df.to_excel(f'{file_name}.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../test_metric_single_llm/with_mixtral_info/brave/data1-split=0.5:9.5-ratio=1.0/lora-r=2/openchat-3.5-0106-lr=2e-4.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m wrong_claims_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../test_metric_single_llm/with_mixtral_info/brave/data1-split=0.5:9.5-ratio=1.0/lora-r=2/openchat-3.5-0106-lr=2e-4.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(wrong_claims_dir) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     wrong_claims \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m      5\u001b[0m wrong_labels \u001b[38;5;241m=\u001b[39m [s[s\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel: \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel: \u001b[39m\u001b[38;5;124m\"\u001b[39m): s\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m wrong_claims]\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../test_metric_single_llm/with_mixtral_info/brave/data1-split=0.5:9.5-ratio=1.0/lora-r=2/openchat-3.5-0106-lr=2e-4.txt'"
     ]
    }
   ],
   "source": [
    "wrong_claims_dir = \"../test_metric_single_llm/with_mixtral_info/brave/data1-split=0.5:9.5-ratio=1.0/lora-r=2/openchat-3.5-0106-lr=2e-4.txt\"\n",
    "with open(wrong_claims_dir) as f:\n",
    "    wrong_claims = f.readlines()[2:]\n",
    "\n",
    "wrong_labels = [s[s.find(\"Label: \") + len(\"Label: \"): s.find(\". Pred\")].strip() for s in wrong_claims]\n",
    "wrong_preds = [s[s.find(\"Pred: \") + len(\"Pred: \"):s.find(\". Claim\")].strip() for s in wrong_claims]\n",
    "wrong_claims = [s[s.find(\"Claim: \") + len(\"Claim: \"):].strip() for s in wrong_claims]\n",
    "\n",
    "len(wrong_claims)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型分类错误的样例中不包含在先验知识与label不符的样例中的情况：如果这些样例均是label本身就正确，那么“先验知识与label不符”的策略可行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_version = 1\n",
    "wrong_claims_diff = []\n",
    "data_diff = load_diff(\"llama3\", diff_version)\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "check_claim_list = []\n",
    "check_preds_list = []\n",
    "check_labels_list = []\n",
    "\n",
    "for item in data_diff:\n",
    "    if item[\"match2\"].lower() == \"no\":\n",
    "        wrong_claims_diff.append(item[\"claim\"])\n",
    "\n",
    "\n",
    "for i, claim in enumerate(wrong_claims):\n",
    "    if claim not in wrong_claims_diff: # 检查不在diff中的是不是都是对的\n",
    "        check_claim_list.append(claim)\n",
    "        check_preds_list.append(wrong_preds[i])\n",
    "        check_labels_list.append(wrong_labels[i])\n",
    "\n",
    "# save_excel(\"check_claim\", check_claim_list, check_labels_list, check_preds_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先验知识与label不符的样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_version = 1\n",
    "wrong_claims_diff = []\n",
    "labels_diff = []\n",
    "related_urls_diff = []\n",
    "ids_diff = []\n",
    "# human_checks = []\n",
    "data_diff = load_diff(\"llama3\", diff_version)\n",
    "data_type1 = load_type1()\n",
    "data_type2 = load_type2()\n",
    "false_claims_trump = pd.read_csv('wapo_trumpclaims_export-012021.csv')\n",
    "claims = list(false_claims_trump.loc[:, \"claim\"])\n",
    "\n",
    "for i, item in enumerate(data_diff):\n",
    "    if item[\"match2\"].lower() != \"no\":\n",
    "        continue\n",
    "    if item_exist(item, data_type1) or item_exist(item, data_type2):\n",
    "        continue\n",
    "\n",
    "    label = int(item[\"label\"])\n",
    "    need_pass = False\n",
    "\n",
    "    for search_result_id in range(2):\n",
    "        if len(data_search[i][f\"{search_engine}_search_results\"][\"web\"][\"results\"]) == search_result_id:\n",
    "            break\n",
    "        title_bak = data_search[i][f\"{search_engine}_search_results\"][\"web\"][\"results\"][search_result_id][\"title\"]\n",
    "        title = title_bak\n",
    "        url = data_search[i][f\"{search_engine}_search_results\"][\"web\"][\"results\"][search_result_id][\"url\"]\n",
    "        check_list = [\"poynter.org\"] # \n",
    "\n",
    "        ok = False\n",
    "        for site in check_list:\n",
    "            if site in url:\n",
    "                ok = True\n",
    "                break\n",
    "        if not ok:\n",
    "            continue\n",
    "\n",
    "        pos_st = title.find(item[\"claim\"].split(' ')[0])\n",
    "        if pos_st == -1:\n",
    "            continue\n",
    "        pos_en = len(title)\n",
    "        if title.endswith(\" ...\"):\n",
    "            pos_en -= 4\n",
    "        elif title.endswith(\"...\"):\n",
    "            pos_en -= 3\n",
    "\n",
    "        title = title[pos_st : pos_en]\n",
    "        pos_en = len(title)\n",
    "\n",
    "        #  - Poynter\n",
    "        end_list = [\"- Poynter\", \"-\", ]\n",
    "        for s_end in end_list:\n",
    "            if title.endswith(s_end):\n",
    "                pos_en -= len(s_end)\n",
    "        \n",
    "        if len(title.split()) >= 5 and title.strip() in item[\"claim\"] and title_bak.lower().startswith(\"false:\"):\n",
    "            if label == 0:\n",
    "                need_pass = True\n",
    "                break\n",
    "    if need_pass:\n",
    "        # print(item[\"claim\"])\n",
    "        continue\n",
    "    \n",
    "    wrong_claims_diff.append(item[\"claim\"])\n",
    "    related_urls_diff.append(data_train[i][\"related_url\"])\n",
    "    ids_diff.append(item[\"id\"])\n",
    "\n",
    "    if label == 0:\n",
    "        label = \"FALSE\"\n",
    "    elif label == 2:\n",
    "        label = \"TRUE\"\n",
    "    else:\n",
    "        raise Exception()\n",
    "    labels_diff.append(label)\n",
    "\n",
    "    # human_check = \"\"\n",
    "    # for x in claims:\n",
    "    #     if (\"Donald Trump\" in data_search[i][\"claimant\"] or \"Donald J. Trump\" in data_search[i][\"claimant\"]) and item[\"claim\"] in x: # 还是要检查下是不是川普说的，不然一些不完整的都会算进去\n",
    "    #         human_check = \"FALSE\"\n",
    "    #         break\n",
    "\n",
    "    # human_checks.append(human_check)\n",
    "    \n",
    "# 属于false claims 的标注\n",
    "# save_excel(\"diff_llama3_v99\", ids_diff, wrong_claims_diff, labels_diff, related_urls_diff, human_checks=human_checks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1517"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = [\n",
    "    'politifact.com/factchecks', 'snopes.com/fact-check', 'factcheck.afp', 'africacheck', \n",
    "    'polygraph.info', \n",
    "    # 'reuters',  # \n",
    "    # 'washingtonpost', # \n",
    "    # 'google fact check', \n",
    "    'poynter',\n",
    "\n",
    "    \"factcheck\"\n",
    "]\n",
    "\n",
    "pending_ids = []\n",
    "pending_labels = []\n",
    "pending_preds = []\n",
    "\n",
    "def get_ans(result):\n",
    "    fact_check_list = [\"fact check\", \"factcheck\", \"fact-check\"]\n",
    "    for i in fact_check_list:\n",
    "        if i in result[\"title\"].lower():\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            for site in websites:\n",
    "                if site in result[\"url\"].lower():\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "for c, claim in enumerate(wrong_claims):\n",
    "    for i in range(len(data_search)):\n",
    "        if claim.strip() in data_search[i][\"claim\"].strip():\n",
    "            results = data_search[i][f\"{search_engine}_search_results\"][\"web\"][\"results\"]\n",
    "            ok = False\n",
    "            for result in results:\n",
    "                if get_ans(result):\n",
    "                    ok = True\n",
    "                    break\n",
    "            if not ok:\n",
    "                pending_ids.append(i)\n",
    "                pending_labels.append(wrong_labels[c])\n",
    "                pending_preds.append(wrong_preds[c])\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pending_claims = [data_search[i][\"claim\"] for i in pending_ids]\n",
    "len(pending_claims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = pd.read_excel(\"diff_llama3_v8.xlsx\", \n",
    "                   # usecols=[\"claim\", \"label\", \"human check\", \"new claim\", \"evidence_url\"]\n",
    "                   )\n",
    "\n",
    "def update_item(data, evidence_url, index):\n",
    "    flg = False\n",
    "    for item in data:\n",
    "        if item[\"id\"] == index:\n",
    "            flg = True\n",
    "            if isinstance(evidence_url, str):\n",
    "                item[\"evidence_url\"] = evidence_url\n",
    "            break\n",
    "    # if not flg:\n",
    "    #     print(index)\n",
    "data_train = load_train()\n",
    "data_type1 = load_type1()\n",
    "data_type2 = load_type2()\n",
    "ids = df[\"id\"].tolist()\n",
    "evidences = df[\"evidence_url\"].tolist()\n",
    "\n",
    "for i, index in enumerate(ids):\n",
    "    update_item(data_train, evidences[i], index)\n",
    "    update_item(data_type1, evidences[i], index)\n",
    "    update_item(data_type2, evidences[i], index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_type1(data_type1)\n",
    "# save_type2(data_type2)\n",
    "\n",
    "# with open(\"covmis_data/train.json\", \"w\") as f:\n",
    "#     json.dump(data_train, f, indent=4)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
