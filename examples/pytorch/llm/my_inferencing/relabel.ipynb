{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Successfully registered `/home/hanlv/workspace/code/research/infodemic/LLM/swift/swift/llm/data/dataset_info.json`\n",
      "[INFO:swift] Loading the model using model_dir: /home/css/models/Meta-Llama-3-8B-Instruct\n",
      "[INFO:swift] Setting torch_dtype: torch.bfloat16\n",
      "[INFO:swift] model_config: LlamaConfig {\n",
      "  \"_name_or_path\": \"/home/css/models/Meta-Llama-3-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a593967ad04f57910597e8305ad92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] model.max_model_len: 8192\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "dirs = [\"..\", \"/home/hanlv/workspace/code/research/infodemic/LLM/swift/examples/pytorch/llm/my_inferencing/create_prompt_llm\"]\n",
    "for _dir in dirs:\n",
    "    if _dir not in sys.path:\n",
    "        sys.path.append(_dir)\n",
    "\n",
    "from swift.llm import (\n",
    "    get_model_tokenizer, get_template, inference, \n",
    ")\n",
    "from swift.tuners import Swift\n",
    "from custom import CustomModelType, CustomTemplateType\n",
    "import covmis, liar2\n",
    "import prompt_rag\n",
    "\n",
    "ckpt_dir_cvomis = \"/home/hanlv/workspace/code/research/infodemic/LLM/swift/examples/pytorch/llm/output/covmis/Llama-3-8B-Instruct/with_llama3_info/brave/data1-split=8:2-ratio=1.0/dora-r=8/lr=9e-5-20240626-01:48:13/checkpoint-609\"\n",
    "ckpt_dir_liar2 = \"/home/hanlv/workspace/code/research/infodemic/LLM/swift/examples/pytorch/llm/output/liar2/Llama-3-8B-Instruct/with_llama3_info/brave/data1.2-split=8:1:1-ratio=1.0-epochs=1/dora-r=8/lr=1.5e-4-20240723-10:07:49/checkpoint-611\"\n",
    "\n",
    "ckpt_dir = ckpt_dir_liar2\n",
    "\n",
    "with open(f\"{ckpt_dir}/sft_args.json\", \"r\") as f:\n",
    "    sft_args = json.load(f)\n",
    "\n",
    "def get_model_template():\n",
    "    model_type, template_type = sft_args[\"model_type\"], sft_args[\"template_type\"]\n",
    "    model, tokenizer = get_model_tokenizer(\n",
    "        model_type, model_kwargs={'device_map': 'auto'},\n",
    "        # model_dir=sft_args[\"model_cache_dir\"],\n",
    "        use_flash_attn=sft_args[\"use_flash_attn\"]\n",
    "    )\n",
    "    model = Swift.from_pretrained(model, ckpt_dir, inference_mode=True)\n",
    "    if sft_args[\"sft_type\"] == 'adalora':\n",
    "        model = model.to(model.dtype)\n",
    "    model.generation_config.max_new_tokens = 512\n",
    "    # model.generation_config.temperature = None\n",
    "    model.generation_config.do_sample = False\n",
    "\n",
    "    template = get_template(template_type, tokenizer)\n",
    "\n",
    "    return model, template\n",
    "\n",
    "model, template = get_model_template()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_convert_liar2 = {'pants-fire': 0, 'false': 1, 'barely-true': 2, 'half-true': 3, 'mostly-true': 4, 'true': 5}\n",
    "\n",
    "search_engine = \"brave\"\n",
    "\n",
    "dataset = \"liar2\" # liar2, covmis\n",
    "data_type = \"valid\" # train, test, valid\n",
    "\n",
    "if dataset == \"covmis\":\n",
    "    data = covmis.load_train()\n",
    "    data_search =  covmis.load_train_search(search_engine=search_engine)\n",
    "    data_search_llm = covmis.load_train_llm(search_engine=search_engine)\n",
    "    claim_key = 'claim'\n",
    "    claimant_key = 'None'\n",
    "    LABEL_TRUE = 2\n",
    "    LABEL_FALSE = 0\n",
    "    true_labels_original = [LABEL_TRUE]\n",
    "    false_labels_original = [LABEL_FALSE]\n",
    "\n",
    "    save_data = lambda data: covmis.save_train(data)\n",
    "    save_search = lambda data: covmis.save_train_search(\n",
    "        data, search_engine=search_engine)\n",
    "    save_search_llm = lambda data: covmis.save_train_llm(\n",
    "        data, search_engine=search_engine)\n",
    "elif dataset == \"liar2\":\n",
    "    data = liar2.load_data(data_type)\n",
    "    data_search = liar2.load_data_search(data_type, search_engine)\n",
    "    data_search_llm = liar2.load_data_llm(data_type, search_engine)\n",
    "    claim_key = 'statement'\n",
    "    claimant_key = 'None'\n",
    "    true_labels_original = [\n",
    "        label_convert_liar2['true'], \n",
    "        # label_convert_liar2['mostly-true'],\n",
    "        # label_convert_liar2['half-true']\n",
    "    ]\n",
    "    \n",
    "    false_labels_original = [\n",
    "        # label_convert_liar2['barely-true'], \n",
    "        label_convert_liar2['false'],\n",
    "        label_convert_liar2['pants-fire']\n",
    "    ]\n",
    "\n",
    "    save_data = lambda data: liar2.save_data(data, data_type)\n",
    "    save_search = lambda data: liar2.save_data_search(\n",
    "        data, data_type, search_engine)\n",
    "    save_search_llm = lambda data: liar2.save_data_llm(\n",
    "        data, data_type, search_engine)\n",
    "else:\n",
    "    raise Exception(\"数据集错误\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CVOMIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cafa8cfd611c4bd7b7c3d7ab909d5ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2297 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanlv/miniconda3/envs/swift/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/hanlv/miniconda3/envs/swift/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "covmis_dir = \"/home/hanlv/workspace/data/machine_learning/dataset/research/misinformation_dataset/COVMIS-2024/data.json\"\n",
    "\n",
    "# with open(f\"/home/hanlv/workspace/data/machine_learning/dataset/research/misinformation_dataset/COVMIS-2024/data2.json\", \"w\") as f:\n",
    "#             json.dump(data_covmis, f, indent=4)\n",
    "\n",
    "K = 5\n",
    "prior_knowledge_version = \"1\"\n",
    "model_name = \"llama3\"\n",
    "\n",
    "for i in tqdm(range(len(data_search_llm))):\n",
    "    item = data_search_llm[i]\n",
    "\n",
    "    if data[i][\"id\"] != item[\"id\"]:\n",
    "        raise Exception(\"data 与 data_search_llm 的 id 不匹配！\")\n",
    "    \n",
    "    if int(data[i][\"label\"]) not in (true_labels_original + false_labels_original):\n",
    "        prompt = prompt_rag.get_prompt_with_prior_knowledge(\n",
    "            data[i][claim_key], \n",
    "            search_engine,\n",
    "            data_search[i][f\"{search_engine}_search_results\"], \n",
    "            item[f\"prior_knowledge_{model_name}_v{prior_knowledge_version}_K={K}\"], \n",
    "            K=K,\n",
    "            claim_date=data[i][\"date\"],\n",
    "            # claimant=data[i].get(claimant_key), # data_version 为 *.1 需使用\n",
    "            justification=data[i].get('justification'),\n",
    "            known_info=True, \n",
    "            rag_info=True,\n",
    "            justify_info=False,\n",
    "            ids=None\n",
    "        )\n",
    "        pred_raw = inference(model, template, prompt)[0].strip()\n",
    "\n",
    "        if pred_raw.startswith(\"TRUE\"):\n",
    "            data[i][\"label2\"] = true_labels_original[0]\n",
    "        elif pred_raw.startswith(\"FALSE\"):\n",
    "            data[i][\"label2\"] = false_labels_original[0]\n",
    "        else:\n",
    "            raise Exception(f\"Error label: {pred_raw}\")\n",
    "\n",
    "        # dict_list.append({\"query\": prompt, \"response\": label})\n",
    "    else:\n",
    "        if data[i][\"label\"] in true_labels_original:\n",
    "            data[i][\"label2\"] = true_labels_original[0]\n",
    "        elif data[i][\"label\"] in false_labels_original:\n",
    "            data[i][\"label2\"] = false_labels_original[0]\n",
    "        else:\n",
    "            raise Exception()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_data(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
