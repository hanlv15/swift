{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:18:44,417 - modelscope - INFO - PyTorch version 2.3.0 Found.\n",
      "2024-05-10 12:18:44,420 - modelscope - INFO - Loading ast index from /home/hanlv/.cache/modelscope/ast_indexer\n",
      "2024-05-10 12:18:44,449 - modelscope - INFO - Loading done! Current index file version is 1.14.0, with md5 2a8987246d4b67f321effea600434525 and a total number of 976 components indexed\n",
      "[INFO:swift] Successfully registered `/home/hanlv/workspace/code/research/infodemic/LLM/swift/swift/llm/data/dataset_info.json`\n",
      "[INFO:swift] Loading the model using model_dir: /home/css/models/Meta-Llama-3-70B-Instruct-GPTQ-Int4\n",
      "[INFO:swift] Setting torch_dtype: torch.float16\n",
      "[INFO:swift] model_config: LlamaConfig {\n",
      "  \"_name_or_path\": \"/home/css/models/Meta-Llama-3-70B-Instruct-GPTQ-Int4\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 8192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 28672,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_hidden_layers\": 80,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"bits\": 4,\n",
      "    \"damp_percent\": 0.1,\n",
      "    \"desc_act\": false,\n",
      "    \"group_size\": 128,\n",
      "    \"modules_in_block_to_quantize\": null,\n",
      "    \"quant_method\": \"gptq\",\n",
      "    \"sym\": true,\n",
      "    \"true_sequential\": true\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-10 12:18:45 config.py:177] The model is convertible to Marlin format. Using Marlin kernel.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:18:47,400\tINFO worker.py:1749 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-10 12:18:48 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='/home/css/models/Meta-Llama-3-70B-Instruct-GPTQ-Int4', speculative_config=None, tokenizer='/home/css/models/Meta-Llama-3-70B-Instruct-GPTQ-Int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=42, served_model_name=/home/css/models/Meta-Llama-3-70B-Instruct-GPTQ-Int4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-10 12:18:53 utils.py:660] Found nccl from library /home/hanlv/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=2553718)\u001b[0m INFO 05-10 12:18:53 utils.py:660] Found nccl from library /home/hanlv/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-10 12:18:53 selector.py:81] Cannot use FlashAttention-2 backend because the flash_attn package is not found. Please install it for better performance.\n",
      "INFO 05-10 12:18:53 selector.py:32] Using XFormers backend.\n",
      "\u001b[36m(RayWorkerWrapper pid=2553718)\u001b[0m INFO 05-10 12:18:54 selector.py:81] Cannot use FlashAttention-2 backend because the flash_attn package is not found. Please install it for better performance.\n",
      "\u001b[36m(RayWorkerWrapper pid=2553718)\u001b[0m INFO 05-10 12:18:54 selector.py:32] Using XFormers backend.\n",
      "INFO 05-10 12:18:55 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=2553718)\u001b[0m INFO 05-10 12:18:55 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "INFO 05-10 12:18:56 utils.py:132] reading GPU P2P access cache from /home/hanlv/.config/vllm/gpu_p2p_access_cache_for_1,2.json\n",
      "WARNING 05-10 12:18:56 custom_all_reduce.py:74] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[36m(RayWorkerWrapper pid=2553718)\u001b[0m INFO 05-10 12:18:56 utils.py:132] reading GPU P2P access cache from /home/hanlv/.config/vllm/gpu_p2p_access_cache_for_1,2.json\n",
      "\u001b[36m(RayWorkerWrapper pid=2553718)\u001b[0m WARNING 05-10 12:18:56 custom_all_reduce.py:74] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 05-10 12:19:06 model_runner.py:175] Loading model weights took 18.4481 GB\n",
      "\u001b[36m(RayWorkerWrapper pid=2553718)\u001b[0m INFO 05-10 12:19:18 model_runner.py:175] Loading model weights took 18.4481 GB\n",
      "INFO 05-10 12:19:22 distributed_gpu_executor.py:45] # GPU blocks: 671, # CPU blocks: 1638\n",
      "INFO 05-10 12:19:24 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-10 12:19:24 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerWrapper pid=2553718)\u001b[0m INFO 05-10 12:19:25 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerWrapper pid=2553718)\u001b[0m INFO 05-10 12:19:25 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerWrapper pid=2553718)\u001b[0m INFO 05-10 12:19:27 model_runner.py:1017] Graph capturing finished in 2 secs.\n",
      "INFO 05-10 12:19:27 model_runner.py:1017] Graph capturing finished in 3 secs.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2'\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
    "dirs = [\"../..\"]\n",
    "for _dir in dirs:\n",
    "    if _dir not in sys.path:\n",
    "        sys.path.append(_dir)\n",
    "\n",
    "from swift.llm import (\n",
    "    ModelType, get_vllm_engine, get_default_template_type,\n",
    "    get_template, inference_vllm, VllmGenerationConfig\n",
    ")\n",
    "from custom import CustomModelType, CustomTemplateType\n",
    "\n",
    "model_type = CustomModelType.llama_3_70b_instruct_gptq_int4  # mixtral_moe_7b_instruct_gptq_int4\n",
    "# model_type = ModelType.llama3_70b_instruct_int4\n",
    "\n",
    "llm_engine = get_vllm_engine(\n",
    "    model_type, \n",
    "    # torch_dtype=torch.float16,  # 检查正确的数据类型！！！！\n",
    "    tensor_parallel_size=2,\n",
    "    max_model_len=4096,\n",
    "    # gpu_memory_utilization=0.95,\n",
    "    engine_kwargs = {\n",
    "        # \"enforce_eager\": True,\n",
    "        \"max_num_seqs\": 16,\n",
    "        \"seed\": 42,\n",
    "    }\n",
    ")\n",
    "\n",
    "template_type = get_default_template_type(model_type)\n",
    "template = get_template(template_type, llm_engine.hf_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:48<07:13, 48.12s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m      1\u001b[0m generation_config \u001b[38;5;241m=\u001b[39m VllmGenerationConfig(\n\u001b[1;32m      2\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m,\n\u001b[1;32m      3\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m request_list \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# {'query': 'Below is a CLAIM and some INFORMATION searched online. These pieces of INFORMATION are relevant to the CLAIM. This CLAIM and all INFORMATION include their respective publication dates and contents. To classify the CLAIM more accurately (if the content described by the CLAIM is correct, it will be classified as TRUE; if the content described by the CLAIM is incorrect, it will be classified as FALSE), please first provide a detailed summary of the given INFORMATION. Then analyze, reason, and provide reasonable evidence to judge the correctness of the CLAIM based on the available information and your knowledge.\\n\\nCLAIM:\\nPublication date: 2020-08-13\\nContent: Illinois Strengthens Face Mask Rules in Businesses\\n\\nINFORMATION:\\nInformation 1:\\nPublication date: 2013-08-06\\nTitle: Illinois Strengthens Face Mask Rules in Businesses - WebMD\\nContent:\\nCOVID-19 is a new type of coronavirus that causes mild to severe cases. Here’s a quick guide on how to spot symptoms, risk factors, prevent spread of the disease, and find out what to do if you think you have it.\\nInformation 2:\\nPublication date: None\\nTitle: press-release\\nContent:\\nIllinoisans can resume activities without wearing a mask indoors on February 28th except where required by federal, state, local, tribal, or territorial laws, rules, and regulations, including local business and workplace guidance. Federal requirements, in effect through at least March 18, include all transportation systems such as airports, planes, trains, and buses. \"Preparing to repeal statewide masking mandates at the end of the month is aggressive and optimistic but reasonable,\" said Dr.\\n\"Broad mandates are not about individuals. They are put in place to help protect communities, businesses, and healthcare access. Repealing the mask mandate allows people to choose the mitigation layers that are best for them and I have no doubt that many should and will choose to keep mask rules.\"\\nIf these trends continue — and we expect them to —then on Monday, February 28th, we will lift the indoor mask requirement for the State of Illinois,\" said Governor JB Pritzker. \"I want to be clear: Many local jurisdictions, businesses and organizations have their own mask requirements and other mitigations that must be respected.\\nSeth Trueger, a Northwestern emergency physician who is also immunocompromised. \"Masking has helped slow the spread even in the face of omicron\\'s transmissibility. We can and must use this time to further increase vaccination uptake & outreach, especially among children and other populations with low vaccination rates, so when the next wave comes, we will be even better prepared.\"\\nInformation 3:\\nPublication date: None\\nTitle: Mask and Vaccine Requirements FAQ\\'s\\nContent:\\nOn September 3, 2021, the Governor signed Executive Order 21-22 which requires all individuals over the age of 2 and who can medically tolerate a face covering to wear a face covering when in indoor public places. The Executive Order also requires health care workers, school personnel, higher education personnel and students, and employees and contractors of state-owned or operated congregate facilities to be fully vaccinated, as described in the Order\\nInformation 4:\\nPublication date: None\\nTitle: FAQ for Businesses Concerning Use of Face-Coverings During COVID-19\\nContent:\\nA: Businesses reserve the right to refuse service to persons unable to comply with the requirement to wear a face covering, but they are required to provide a reasonable accommodation if it does not cause an undue hardship. Businesses are encouraged to inform customers there are exceptions to the requirement that all individuals must wear a mask.\\nThese frequently asked questions are to provide guidance regarding the application of the face covering requirement in Executive Order 2021-10 for businesses and other places of public accommodation subject to Article 5 of the Illinois Human Rights Act, 775 ILCS 5/. A: A face covering is a mask or cloth face covering that covers your nose and mouth.\\nExceptions may be made for individuals with medical conditions or disabilities that prevent them from safely wearing a face covering. For more information, refer to the questions on reasonable accommodations. A: Masks still must be worn by everyone on planes, buses, trains, and other forms of public transportation; in transportation hubs, such as airports and train and bus stations; in health care settings; and in congregate facilities, such as correctional facilities and homeless shelters.\\nThese frequently asked questions are to provide guidance regarding the application of the face covering requirement in Executive Order 2021-10 for businesses and other places of public accommodation subject to Article 5 of the Illinois Human Rights Act, 775 ILCS 5/. When Face Coverings are Required Q: What does it mean to wear a face covering?\\nInformation 5:\\nPublication date: 2020-04-22\\nTitle: Facing Your Face Mask Duties – A List of Statewide Orders | Littler ...\\nContent:\\n********************************************************NOTE: Given the reduction in activity on this topic,THIS POST WILL NO LONGER BE UPDATED, as of November 10, 2022.********************************************************Governors and public health officials across the country implemented stringent mitigation measures to help contain the spread of COVID-19.\\n'},\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwho are you?\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m     13\u001b[0m ] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m---> 15\u001b[0m resp_list \u001b[38;5;241m=\u001b[39m \u001b[43minference_vllm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_engine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# verbose=True, prompt_prefix=\"\", output_prefix=\"\"\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m request, resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(request_list, resp_list):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mrequest[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/code/research/infodemic/LLM/swift/swift/llm/utils/vllm_utils.py:339\u001b[0m, in \u001b[0;36minference_vllm\u001b[0;34m(llm_engine, template, request_list, generation_config, lora_request, use_tqdm, verbose, prompt_prefix, output_prefix, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m llm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m--> 339\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/engine/llm_engine.py:600\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    598\u001b[0m     output \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 600\u001b[0m request_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_model_outputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduled_seq_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignored_seq_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Log stats.\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_log_stats(scheduler_outputs, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/engine/llm_engine.py:516\u001b[0m, in \u001b[0;36mLLMEngine._process_model_outputs\u001b[0;34m(self, output, scheduled_seq_groups, ignored_seq_groups, seq_group_metadata_list)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_processor\u001b[38;5;241m.\u001b[39mprocess_prompt_logprob(seq_group, outputs)\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m seq_group_meta\u001b[38;5;241m.\u001b[39mdo_sample:\n\u001b[0;32m--> 516\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# Free the finished sequence groups.\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mfree_finished_seq_groups()\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/engine/output_processor/single_step.py:56\u001b[0m, in \u001b[0;36mSingleStepOutputProcessor.process_outputs\u001b[0;34m(self, sequence_group, outputs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Append all new tokens to sequences in the sequence group. Fork any\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03msurviving beam candidates; free any unsurviving ones.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03mInvokes detokenizer to detokenize new tokens, and also marks sequences\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03mas finished if they meet stop conditions.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(outputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     55\u001b[0m         ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not support multiple outputs per step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_sequence_group_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/engine/output_processor/single_step.py:117\u001b[0m, in \u001b[0;36mSingleStepOutputProcessor._process_sequence_group_outputs\u001b[0;34m(self, seq_group, outputs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq, _ \u001b[38;5;129;01min\u001b[39;00m child_seqs:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m seq_group\u001b[38;5;241m.\u001b[39msampling_params\u001b[38;5;241m.\u001b[39mdetokenize \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenizer:\n\u001b[0;32m--> 117\u001b[0m         new_char_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_sequence_inplace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m         new_char_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/transformers_utils/detokenizer.py:116\u001b[0m, in \u001b[0;36mDetokenizer.decode_sequence_inplace\u001b[0;34m(self, seq, prms)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seq\u001b[38;5;241m.\u001b[39mtokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     (seq\u001b[38;5;241m.\u001b[39mtokens, seq\u001b[38;5;241m.\u001b[39mprefix_offset,\n\u001b[1;32m    109\u001b[0m      seq\u001b[38;5;241m.\u001b[39mread_offset) \u001b[38;5;241m=\u001b[39m convert_prompt_ids_to_tokens(\n\u001b[1;32m    110\u001b[0m          tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m    111\u001b[0m          prompt_ids\u001b[38;5;241m=\u001b[39mall_input_ids[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    112\u001b[0m          skip_special_tokens\u001b[38;5;241m=\u001b[39mprms\u001b[38;5;241m.\u001b[39mskip_special_tokens,\n\u001b[1;32m    113\u001b[0m      )\n\u001b[1;32m    115\u001b[0m (new_tokens, new_decoded_token_text, prefix_offset,\n\u001b[0;32m--> 116\u001b[0m  read_offset) \u001b[38;5;241m=\u001b[39m \u001b[43mdetokenize_incrementally\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m     \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m     \u001b[49m\u001b[43mall_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m     \u001b[49m\u001b[43mprev_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m     \u001b[49m\u001b[43mprefix_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefix_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m     \u001b[49m\u001b[43mread_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m     \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m     \u001b[49m\u001b[43mspaces_between_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspaces_between_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Decode logprobs\u001b[39;00m\n\u001b[1;32m    127\u001b[0m logprobs \u001b[38;5;241m=\u001b[39m seq\u001b[38;5;241m.\u001b[39moutput_logprobs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/vllm/transformers_utils/detokenizer.py:269\u001b[0m, in \u001b[0;36mdetokenize_incrementally\u001b[0;34m(tokenizer, all_input_ids, prev_tokens, prefix_offset, read_offset, skip_special_tokens, spaces_between_special_tokens)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m prev_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# If the new token id is out of bounds, return an empty string.\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_token_id \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokenizer):\n\u001b[1;32m    270\u001b[0m     new_tokens \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# Put new_token_id in a list so skip_special_tokens is respected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:251\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    248\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m    Size of the full vocabulary with the added tokens.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocab_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwith_added_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generation_config = VllmGenerationConfig(\n",
    "    max_new_tokens=2048,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "request_list = [\n",
    "    # {'query': 'Below is a CLAIM and some INFORMATION searched online. These pieces of INFORMATION are relevant to the CLAIM. This CLAIM and all INFORMATION include their respective publication dates and contents. To classify the CLAIM more accurately (if the content described by the CLAIM is correct, it will be classified as TRUE; if the content described by the CLAIM is incorrect, it will be classified as FALSE), please first provide a detailed summary of the given INFORMATION. Then analyze, reason, and provide reasonable evidence to judge the correctness of the CLAIM based on the available information and your knowledge.\\n\\nCLAIM:\\nPublication date: 2020-08-13\\nContent: Illinois Strengthens Face Mask Rules in Businesses\\n\\nINFORMATION:\\nInformation 1:\\nPublication date: 2013-08-06\\nTitle: Illinois Strengthens Face Mask Rules in Businesses - WebMD\\nContent:\\nCOVID-19 is a new type of coronavirus that causes mild to severe cases. Here’s a quick guide on how to spot symptoms, risk factors, prevent spread of the disease, and find out what to do if you think you have it.\\nInformation 2:\\nPublication date: None\\nTitle: press-release\\nContent:\\nIllinoisans can resume activities without wearing a mask indoors on February 28th except where required by federal, state, local, tribal, or territorial laws, rules, and regulations, including local business and workplace guidance. Federal requirements, in effect through at least March 18, include all transportation systems such as airports, planes, trains, and buses. \"Preparing to repeal statewide masking mandates at the end of the month is aggressive and optimistic but reasonable,\" said Dr.\\n\"Broad mandates are not about individuals. They are put in place to help protect communities, businesses, and healthcare access. Repealing the mask mandate allows people to choose the mitigation layers that are best for them and I have no doubt that many should and will choose to keep mask rules.\"\\nIf these trends continue — and we expect them to —then on Monday, February 28th, we will lift the indoor mask requirement for the State of Illinois,\" said Governor JB Pritzker. \"I want to be clear: Many local jurisdictions, businesses and organizations have their own mask requirements and other mitigations that must be respected.\\nSeth Trueger, a Northwestern emergency physician who is also immunocompromised. \"Masking has helped slow the spread even in the face of omicron\\'s transmissibility. We can and must use this time to further increase vaccination uptake & outreach, especially among children and other populations with low vaccination rates, so when the next wave comes, we will be even better prepared.\"\\nInformation 3:\\nPublication date: None\\nTitle: Mask and Vaccine Requirements FAQ\\'s\\nContent:\\nOn September 3, 2021, the Governor signed Executive Order 21-22 which requires all individuals over the age of 2 and who can medically tolerate a face covering to wear a face covering when in indoor public places. The Executive Order also requires health care workers, school personnel, higher education personnel and students, and employees and contractors of state-owned or operated congregate facilities to be fully vaccinated, as described in the Order\\nInformation 4:\\nPublication date: None\\nTitle: FAQ for Businesses Concerning Use of Face-Coverings During COVID-19\\nContent:\\nA: Businesses reserve the right to refuse service to persons unable to comply with the requirement to wear a face covering, but they are required to provide a reasonable accommodation if it does not cause an undue hardship. Businesses are encouraged to inform customers there are exceptions to the requirement that all individuals must wear a mask.\\nThese frequently asked questions are to provide guidance regarding the application of the face covering requirement in Executive Order 2021-10 for businesses and other places of public accommodation subject to Article 5 of the Illinois Human Rights Act, 775 ILCS 5/. A: A face covering is a mask or cloth face covering that covers your nose and mouth.\\nExceptions may be made for individuals with medical conditions or disabilities that prevent them from safely wearing a face covering. For more information, refer to the questions on reasonable accommodations. A: Masks still must be worn by everyone on planes, buses, trains, and other forms of public transportation; in transportation hubs, such as airports and train and bus stations; in health care settings; and in congregate facilities, such as correctional facilities and homeless shelters.\\nThese frequently asked questions are to provide guidance regarding the application of the face covering requirement in Executive Order 2021-10 for businesses and other places of public accommodation subject to Article 5 of the Illinois Human Rights Act, 775 ILCS 5/. When Face Coverings are Required Q: What does it mean to wear a face covering?\\nInformation 5:\\nPublication date: 2020-04-22\\nTitle: Facing Your Face Mask Duties – A List of Statewide Orders | Littler ...\\nContent:\\n********************************************************NOTE: Given the reduction in activity on this topic,THIS POST WILL NO LONGER BE UPDATED, as of November 10, 2022.********************************************************Governors and public health officials across the country implemented stringent mitigation measures to help contain the spread of COVID-19.\\n'},\n",
    "    \n",
    "    # {'query': 'Does the conclusion in the answer given below match the ground truth? If it does, please answer \"Yes\" in the first line; otherwise, answer \"No\" in the first line. Please provide a reason on the next line.\\nGround truth: The claim(Illinois Strengthens Face Mask Rules in Businesses) is true.\\nAnswer:True'},\n",
    "    \n",
    "    # {'query': 'Below is a claim and some information searched online. These pieces of Information are relevant to the claim. This claim and all information include their respective publication dates and contents. To classify the claim more accurately (if the content described by the claim is correct, it will be classified as true; if the content described by the claim is incorrect, it will be classified as false), please judge the correctness of the claim based on the available information and your knowledge. In reasoning, it is necessary to consider the sequential relationship between the date of publication of the claim and the date of publication of the information. Just provide the final conclusion without explaining the reasoning process. \\n\\nClaim:\\nPublication date: 2021-01-18\\nContent: 13 Israelis suffer FACIAL PARALYSIS after taking Pfizer Covid jab\\n\\nInformation:\\nInformation 1:\\nPublication date: 2021-01-17\\nTitle: 13 People Develop Facial Paralysis After Receiving Covid-19 Vaccine ...\\nContent:\\nAccording to WION, the Israeli Health Ministry has indicated that the number of people suffering from such side effects after being inoculated for Covid-19 could be higher. “For at least 28 hours I walked around with it (facial paralysis),” one person told Ynet adding that he recovered after that. This comes just days after 23 people died in Norway after receiving Pfizer’s Covid-19 jab.\\nAs many as 13 people in Israel have suffered from facial paralysis after being administered the Covid-19 vaccine, reports claimed. According to WION, the Israeli Health Ministry has indicated that the number of people suffering from such side effects after being inoculated for Covid-19 could be higher.\\nOne of the Covid-19 vaccine recipient’s face was paralysed for 28 hours, reports claimed\\nInformation 2:\\nPublication date: 2021-01-17\\nTitle: 13 People in Israel Suffer From Facial Paralysis After Taking ...\\nContent:\\nJerusalem: At least 13 people in Israel have suffered mild facial paralysis as a side-effect post Coronavirus vaccination, WION reported. The Health Ministry said that the actual number of people suffering from such side effects could be higher than reported. Experts are now apprehensive of giving the second dose of the shot to these people, even though the health ministry has insisted on giving the jab once the paralysis heals.\\nExperts are now apprehensive of giving the second dose of the shot to these people, even though the health ministry has insisted on giving the jab once the paralysis heals. “For at least 28 hours I walked around with it (facial paralysis),” one person told Ynet. “I can’t say it was completely gone afterwards, but other than that I had no other pains, except a minor pain where the injection was but there was nothing beyond that.” · Israel began its Covid-19 vaccination drive on December 20, 2020.\\nThe Health Ministry said that the actual number of people suffering from such side effects could be higher than reported. Experts are now apprehensive of giving the second dose of the shot to these people, even though the health ministry has insisted on giving the jab once the paralysis heals. “For at least 28 hours I walked around with it (facial paralysis),” one person told Ynet.\\n“For at least 28 hours I walked around with it (facial paralysis),” one person told Ynet. “I can’t say it was completely gone afterwards, but other than that I had no other pains, except a minor pain where the injection was but there was nothing beyond that.” · Israel began its Covid-19 vaccination drive on December 20, 2020. About 72 per cent of those aged 60 and over have already been vaccinated. In an similar incident which was reported last month, four volunteers who were given Pfizer vaccine shots during the trial stage had developed Bell’s Palsy in the UK.\\nInformation 3:\\nPublication date: 2021-01-19\\nTitle: 13 Israelis suffer FACIAL PARALYSIS after taking Pfizer Covid jab, ...\\nContent:\\nAt least 13 Israelis have experienced facial paralysis after being administered the Pfizer Covid-19 vaccine, a month after the US Food and Drug Administration reported similar issues but said they weren’t linked to the jab. ADDITIONAL CONTEXT: Over two million Israelis have been vaccinated against Covid-19 to date, and there has been no evidence the paralysis is linked to the vaccination. The French nonprofit Health Feedback has suggested that the number of facial paralysis cases among the vaccinated “is lower than what would be expected in the general population” and that “incidental illnesses are expected to occur at a certain rate in the general population.”\\nFor a handful of Israelis, however, the initiative has led to some unexpected health scares. At least 13 people have reported mild facial paralysis after receiving the Pfizer/BioNTech jab, Israeli outlet Ynet reported, citing the Health Ministry, adding that officials believe the number of such cases could be higher.\\nAt least 13 people have reported mild facial paralysis after receiving the Pfizer/BioNTech jab, Israeli outlet Ynet reported, citing the Health Ministry, adding that officials believe the number of such cases could be higher.  · “For at least 28 hours I walked around with it [facial paralysis],” one person who had the side effect told Ynet.\\nGalia Rahav, director of the Infectious Diseases Unit at Sheba Medical Center, who said she did not feel “comfortable” with administering the second dose to someone who had received the first jab and subsequently suffered from paralysis.  · “No one knows if this is connected to the vaccine or not. That\\'s why I would refrain from giving a second dose to someone who suffered from paralysis after the first dose,” she told the outlet.  · Last month the FDA disclosed that Bell’s palsy, a form of temporary facial paralysis, was reported by four participants during phase three trials of the Pfizer vaccine.\\nInformation 4:\\nPublication date: None\\nTitle: FarsNews Agency 13 Israelis Suffer Facial Paralysis After Taking ...\\nContent:\\nTEHRAN (FNA)- At least 13 Israelis have experienced facial paralysis after being administered the Pfizer COVID-19 vaccine, a month after the US Food and Drug Administration reported similar issues but said they weren’t linked to the jab. Israel has been hailed for its speedy and efficient mass inoculation program, which has vaccinated a staggering 20 percent of the country’s population since the drive began at the end of December. For a handful of Israelis, however, the initiative has led to some unexpected health scares. At least 13 people have reported mild facial paralysis after receiving the Pfizer/BioNTech jab, Israeli outlet Ynet reported, citing the Health Ministry, adding that officials believe the number of such cases could be higher.\\nFor a handful of Israelis, however, the initiative has led to some unexpected health scares. At least 13 people have reported mild facial paralysis after receiving the Pfizer/BioNTech jab, Israeli outlet Ynet reported, citing the Health Ministry, adding that officials believe the number of such cases could be higher.\\nAt least 13 people have reported mild facial paralysis after receiving the Pfizer/BioNTech jab, Israeli outlet Ynet reported, citing the Health Ministry, adding that officials believe the number of such cases could be higher. “For at least 28 hours I walked around with it [facial paralysis],” one person who had the side effect told Ynet, adding, “I can\\'t say it was completely gone afterwards, but other than that I had no other pains, except a minor pain where the injection was, but there was nothing beyond that.”\\nGalia Rahav, director of the Infectious Diseases Unit at Sheba Medical Center, who said she did not feel “comfortable” with administering the second dose to someone who had received the first jab and subsequently suffered from paralysis. “No one knows if this is connected to the vaccine or not. That\\'s why I would refrain from giving a second dose to someone who suffered from paralysis after the first dose,” she told the outlet. Last month the FDA disclosed that Bell’s palsy, a form of temporary facial paralysis, was reported by four participants during phase three trials of the Pfizer vaccine.\\nInformation 5:\\nPublication date: None\\nTitle: COVID-19 vaccine: 13 out of 2 mil. Israelis suffer facial paralysis ...\\nContent:\\nThat\\'s why I would refrain from giving a second dose to someone who suffered from paralysis after the first dose.\" The Health Ministry stated, however, that the second dose should of course be only provided if and when the paralysis passes, Ynet reported. ... More than two million Israelis have received the Pfizer coronavirus vaccine and the majority have experienced no side effects or mild side effects, according to the Health Ministry.\\nThe Health Ministry stated, however, that the second dose should of course be only provided if and when the paralysis passes, Ynet reported. ... More than two million Israelis have received the Pfizer coronavirus vaccine and the majority have experienced no side effects or mild side effects, according to the Health Ministry.  · This article was updated and corrected for accuracy on January 20. ... var cont = `Stay Informed As the war against Hamas unfolds, our unwavering newsroom remains committed to covering Israel\\'s most profound crisis.\\n'}\n",
    "    {'query': 'who are you?'}\n",
    "] * 10\n",
    "\n",
    "resp_list = inference_vllm(\n",
    "    llm_engine, template, request_list, generation_config=generation_config, \n",
    "    use_tqdm=True,\n",
    "    # verbose=True, prompt_prefix=\"\", output_prefix=\"\"\n",
    ")\n",
    "\n",
    "for request, resp in zip(request_list, resp_list):\n",
    "    print(f\"query:\\n{request['query']}\")\n",
    "    print()\n",
    "    print(f\"response:\\n{resp['response']}\")\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
