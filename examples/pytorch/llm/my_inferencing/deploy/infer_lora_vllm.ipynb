{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 20:40:12,647 - modelscope - INFO - PyTorch version 2.1.2 Found.\n",
      "2024-03-24 20:40:12,649 - modelscope - INFO - Loading ast index from /home/hanlv/.cache/modelscope/ast_indexer\n",
      "2024-03-24 20:40:12,678 - modelscope - INFO - Loading done! Current index file version is 1.13.1, with md5 ad87f4a589251d86333765b92ab59e41 and a total number of 972 components indexed\n",
      "[INFO:swift] Loading the model using model_dir: /home/css/models/openchat-3.5-0106\n",
      "[INFO:swift] Setting torch_dtype: torch.bfloat16\n",
      "[INFO:swift] model_config: MistralConfig {\n",
      "  \"_name_or_path\": \"/home/css/models/openchat-3.5-0106\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.39.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32002\n",
      "}\n",
      "\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 20:40:13 llm_engine.py:87] Initializing an LLM engine with config: model='/home/css/models/openchat-3.5-0106', tokenizer='/home/css/models/openchat-3.5-0106', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 20:40:18 llm_engine.py:357] # GPU blocks: 3307, # CPU blocks: 2048\n",
      "INFO 03-24 20:40:19 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-24 20:40:19 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-24 20:40:22 model_runner.py:756] Graph capturing finished in 3 secs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:256'\n",
    "\n",
    "dirs = [\"../..\"]\n",
    "for _dir in dirs:\n",
    "    if _dir not in sys.path:\n",
    "        sys.path.append(_dir)\n",
    "\n",
    "from swift.llm import (\n",
    "    ModelType, get_vllm_engine, get_default_template_type,\n",
    "    get_template, inference_vllm, VllmGenerationConfig, LoRARequest\n",
    ")\n",
    "from custom import CustomModelType, CustomTemplateType\n",
    "\n",
    "lora_checkpoint = '/home/hanlv/workspace/code/research/infodemic/LLM/swift/examples/pytorch/llm/output/openchat_3.5/with_solar_info/brave/data1-split=8:2-ratio=1.0/dora-r=3/lr=1e-4-20240324-20:38:21/checkpoint-1'\n",
    "lora_request = LoRARequest('default-lora', 1, lora_checkpoint)\n",
    "\n",
    "model_type = CustomModelType.openchat_35\n",
    "llm_engine = get_vllm_engine(\n",
    "    model_type,\n",
    "    tensor_parallel_size=2,\n",
    "    seed=42,\n",
    "    enable_lora=True,\n",
    "    max_loras=1, \n",
    "    max_lora_rank=16,\n",
    "    engine_kwargs={\"max_num_seqs\": 128},\n",
    ")\n",
    "\n",
    "template_type = get_default_template_type(model_type)\n",
    "template = get_template(template_type, llm_engine.hf_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Loading lora /home/hanlv/workspace/code/research/infodemic/LLM/swift/examples/pytorch/llm/output/openchat_3.5/with_solar_info/brave/data1-split=8:2-ratio=1.0/dora-r=3/lr=1e-4-20240324-20:38:21/checkpoint-1 failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.10/site-packages/vllm/lora/worker_manager.py:139\u001b[0m, in \u001b[0;36mWorkerLoRAManager._load_lora\u001b[0;34m(self, lora_request)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m     lora \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lora_model_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_local_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_local_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlora_model_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_int_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_embedding_padding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_extra_vocab_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_padding_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_padding_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.10/site-packages/vllm/lora/models.py:214\u001b[0m, in \u001b[0;36mLoRAModel.from_local_checkpoint\u001b[0;34m(cls, lora_dir, lora_model_id, device, dtype, target_embedding_padding, embedding_modules, embedding_padding_modules)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlora_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt contain tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    216\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: /home/hanlv/workspace/code/research/infodemic/LLM/swift/examples/pytorch/llm/output/openchat_3.5/with_solar_info/brave/data1-split=8:2-ratio=1.0/dora-r=3/lr=1e-4-20240324-20:38:21/checkpoint-1 doesn't contain tensors",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m request_list \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m: i[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m]} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     14\u001b[0m request_list \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhow are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m}]\n\u001b[0;32m---> 16\u001b[0m resp_list \u001b[38;5;241m=\u001b[39m \u001b[43minference_vllm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_engine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# verbose=True, prompt_prefix=\"\", output_prefix=\"\"\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# for request, resp in zip(request_list, resp_list):\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#     print(f\"query: {request['query']}\")\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#     print()\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#     print(f\"response: {resp['response']}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/code/research/infodemic/LLM/swift/swift/llm/utils/vllm_utils.py:369\u001b[0m, in \u001b[0;36minference_vllm\u001b[0;34m(llm_engine, template, request_list, generation_config, lora_request, use_tqdm, verbose, prompt_prefix, output_prefix, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m llm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m--> 369\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.10/site-packages/vllm/engine/llm_engine.py:838\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    834\u001b[0m seq_group_metadata_list, scheduler_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mschedule()\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scheduler_outputs\u001b[38;5;241m.\u001b[39mis_empty():\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;66;03m# Execute the model.\u001b[39;00m\n\u001b[0;32m--> 838\u001b[0m     all_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexecute_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdriver_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq_group_metadata_list\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks_to_swap_in\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_swap_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks_to_swap_out\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_swap_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks_to_copy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_ray_compiled_dag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUSE_RAY_COMPILED_DAG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;66;03m# Only the driver worker returns the sampling results.\u001b[39;00m\n\u001b[1;32m    849\u001b[0m     output \u001b[38;5;241m=\u001b[39m all_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.10/site-packages/vllm/engine/llm_engine.py:1041\u001b[0m, in \u001b[0;36mLLMEngine._run_workers\u001b[0;34m(self, method, driver_args, driver_kwargs, max_concurrent_workers, use_ray_compiled_dag, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     driver_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;66;03m# Start the driver worker after all the ray workers.\u001b[39;00m\n\u001b[0;32m-> 1041\u001b[0m driver_worker_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdriver_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdriver_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;66;03m# Get the results of the ray workers.\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers:\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.10/site-packages/vllm/worker/worker.py:223\u001b[0m, in \u001b[0;36mWorker.execute_model\u001b[0;34m(self, seq_group_metadata_list, blocks_to_swap_in, blocks_to_swap_out, blocks_to_copy)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_seq_groups \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m--> 223\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.10/site-packages/vllm/worker/model_runner.py:574\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, seq_group_metadata_list, kv_caches)\u001b[0m\n\u001b[1;32m    569\u001b[0m (input_tokens, input_positions, input_metadata, sampling_metadata,\n\u001b[1;32m    570\u001b[0m  lora_requests,\n\u001b[1;32m    571\u001b[0m  lora_mapping) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_input_tensors(seq_group_metadata_list)\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_config:\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_active_loras\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_requests\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Execute the model.\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_metadata\u001b[38;5;241m.\u001b[39muse_cuda_graph:\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.10/site-packages/vllm/worker/model_runner.py:660\u001b[0m, in \u001b[0;36mModelRunner.set_active_loras\u001b[0;34m(self, lora_requests, lora_mapping)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_manager:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoRA is not enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 660\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_active_loras\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_requests\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_mapping\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.10/site-packages/vllm/lora/worker_manager.py:112\u001b[0m, in \u001b[0;36mWorkerLoRAManager.set_active_loras\u001b[0;34m(self, lora_requests, lora_mapping)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_active_loras\u001b[39m(\u001b[38;5;28mself\u001b[39m, lora_requests: List[LoRARequest],\n\u001b[1;32m    111\u001b[0m                      lora_mapping: LoRAMapping) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_loras\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_requests\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lora_manager\u001b[38;5;241m.\u001b[39mset_lora_mapping(lora_mapping)\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.10/site-packages/vllm/lora/worker_manager.py:224\u001b[0m, in \u001b[0;36mLRUCacheWorkerLoRAManager._apply_loras\u001b[0;34m(self, lora_requests)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of requested LoRAs (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(loras_map)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is greater \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthan the number of GPU LoRA slots \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lora_manager\u001b[38;5;241m.\u001b[39mlora_slots\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lora \u001b[38;5;129;01min\u001b[39;00m loras_map\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.10/site-packages/vllm/lora/worker_manager.py:231\u001b[0m, in \u001b[0;36mLRUCacheWorkerLoRAManager.add_lora\u001b[0;34m(self, lora_request)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lora_manager) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lora_manager\u001b[38;5;241m.\u001b[39mcapacity:\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lora_manager\u001b[38;5;241m.\u001b[39mremove_oldest_lora()\n\u001b[0;32m--> 231\u001b[0m     lora \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lora_manager\u001b[38;5;241m.\u001b[39madd_lora(lora)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# If the lora is already loaded, just touch it to\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# update its position in the caches\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/swift/lib/python3.10/site-packages/vllm/lora/worker_manager.py:150\u001b[0m, in \u001b[0;36mWorkerLoRAManager._load_lora\u001b[0;34m(self, lora_request)\u001b[0m\n\u001b[1;32m    139\u001b[0m     lora \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lora_model_cls\u001b[38;5;241m.\u001b[39mfrom_local_checkpoint(\n\u001b[1;32m    140\u001b[0m         lora_request\u001b[38;5;241m.\u001b[39mlora_local_path,\n\u001b[1;32m    141\u001b[0m         lora_model_id\u001b[38;5;241m=\u001b[39mlora_request\u001b[38;5;241m.\u001b[39mlora_int_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m         embedding_padding_modules\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_padding_modules,\n\u001b[1;32m    148\u001b[0m     )\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading lora \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlora_request\u001b[38;5;241m.\u001b[39mlora_local_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lora\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_config\u001b[38;5;241m.\u001b[39mmax_lora_rank:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoRA rank \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlora\u001b[38;5;241m.\u001b[39mrank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is greater than max_lora_rank \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_config\u001b[38;5;241m.\u001b[39mmax_lora_rank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Loading lora /home/hanlv/workspace/code/research/infodemic/LLM/swift/examples/pytorch/llm/output/openchat_3.5/with_solar_info/brave/data1-split=8:2-ratio=1.0/dora-r=3/lr=1e-4-20240324-20:38:21/checkpoint-1 failed"
     ]
    }
   ],
   "source": [
    "generation_config = VllmGenerationConfig(\n",
    "    max_new_tokens=512,\n",
    "    temperature=0,\n",
    ")\n",
    "import jsonlines\n",
    "data = []\n",
    "with jsonlines.open(\n",
    "f\"../../my_data/with_solar_info/brave/train_test_split/8:2/test_data1.jsonl\", 'r') as f:\n",
    "    for item in f.iter():\n",
    "        data.append(item)\n",
    "\n",
    "\n",
    "request_list = [{'query': i[\"query\"]} for i in data]\n",
    "request_list = [{'query': \"how are you?\"}]\n",
    "\n",
    "resp_list = inference_vllm(\n",
    "    llm_engine, template, request_list, \n",
    "    generation_config=generation_config, \n",
    "    lora_request=lora_request,\n",
    "    use_tqdm=True,\n",
    "    # verbose=True, prompt_prefix=\"\", output_prefix=\"\"\n",
    ")\n",
    "\n",
    "# for request, resp in zip(request_list, resp_list):\n",
    "#     print(f\"query: {request['query']}\")\n",
    "#     print()\n",
    "#     print(f\"response: {resp['response']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
