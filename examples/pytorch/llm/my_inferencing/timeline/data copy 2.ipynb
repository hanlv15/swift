{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Successfully registered `/home/hanlv/workspace/code/research/infodemic/LLM/swift/swift/llm/data/dataset_info.json`\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import jsonlines\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "dirs = [\"../..\", \"..\"]\n",
    "for _dir in dirs:\n",
    "    if _dir not in sys.path:\n",
    "        sys.path.append(_dir)\n",
    "\n",
    "from swift.llm import (\n",
    "    get_model_tokenizer, get_template, get_vllm_engine, \n",
    "    inference_vllm, inference, VllmGenerationConfig, LoRARequest\n",
    ")\n",
    "from swift.tuners import Swift\n",
    "from custom import CustomModelType, CustomTemplateType\n",
    "import evaluation\n",
    "\n",
    "\n",
    "def get_labels_preds(ckpt_dir, data_dir, data_type):\n",
    "\n",
    "    with open(f\"{ckpt_dir}/sft_args.json\", \"r\") as f:\n",
    "        sft_args = json.load(f)\n",
    "\n",
    "\n",
    "    with jsonlines.open(f\"{ckpt_dir}/../logging.jsonl\", 'r') as f:\n",
    "        for item in f.iter():\n",
    "            training_result = item\n",
    "            train_loss = training_result.get(\"train_loss\")\n",
    "            if train_loss is not None:\n",
    "                break\n",
    "\n",
    "    def get_engine_config_request(ckpt_dir):\n",
    "        # 检查checkpoint是否为peft格式\n",
    "        if os.path.exists(ckpt_dir + '/default'):\n",
    "            raise Exception(\"只支持peft格式！\")\n",
    "            # if not os.path.exists(ckpt_dir + '-peft'):\n",
    "            #     subprocess.run([\"python\", \"../llm_export.py\", \"--ckpt\", ckpt_dir, \"--to_peft_format\", \"true\"])\n",
    "            # ckpt_dir = ckpt_dir + '-peft'\n",
    "\n",
    "\n",
    "        lora_request = LoRARequest('default-lora', 1, ckpt_dir)\n",
    "\n",
    "        model_type, template_type = sft_args[\"model_type\"], sft_args[\"template_type\"]\n",
    "        vllm_engine = get_vllm_engine(\n",
    "            model_type, \n",
    "            tensor_parallel_size=1,\n",
    "            max_model_len=4096,\n",
    "            enable_lora=True,\n",
    "            max_loras=1, \n",
    "            max_lora_rank=8,\n",
    "            engine_kwargs={\n",
    "                \"max_num_seqs\": 128,\n",
    "                \"seed\": 42,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        template = get_template(template_type, vllm_engine.hf_tokenizer)\n",
    "        generation_config = VllmGenerationConfig(\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            temperature=0,\n",
    "        )\n",
    "        return vllm_engine, template, generation_config, lora_request\n",
    "\n",
    "    def get_model_template():\n",
    "        model_type, template_type = sft_args[\"model_type\"], sft_args[\"template_type\"]\n",
    "        model, tokenizer = get_model_tokenizer(\n",
    "            model_type, model_kwargs={'device_map': 'auto'},\n",
    "            # model_dir=sft_args[\"model_cache_dir\"],\n",
    "            use_flash_attn=sft_args[\"use_flash_attn\"]\n",
    "        )\n",
    "        model = Swift.from_pretrained(model, ckpt_dir, inference_mode=True)\n",
    "        if sft_args[\"sft_type\"] == 'adalora':\n",
    "            model = model.to(model.dtype)\n",
    "        model.generation_config.max_new_tokens = 512\n",
    "        # model.generation_config.temperature = None\n",
    "        model.generation_config.do_sample = False\n",
    "\n",
    "        template = get_template(template_type, tokenizer)\n",
    "\n",
    "        return model, template\n",
    "\n",
    "    labels, preds = evaluation.cal_metric_single_llm(\n",
    "        (get_model_template, get_engine_config_request), \n",
    "        (inference, inference_vllm), \n",
    "        sft_args, ckpt_dir, train_loss, save=True, use_vllm=False, \n",
    "        data_dir=data_dir, data_type=data_type,\n",
    "    )\n",
    "\n",
    "    return labels, preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Loading the model using model_dir: /home/css/models/Meta-Llama-3-8B-Instruct\n",
      "[INFO:swift] Setting torch_dtype: torch.bfloat16\n",
      "[INFO:swift] model_config: LlamaConfig {\n",
      "  \"_name_or_path\": \"/home/css/models/Meta-Llama-3-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1150fc66ab54a89a08cbf5353d85ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] model.max_model_len: 8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3-8B-Instruct sft_type=dora lr=9e-5 train_loss=0.07930025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cnt_wrong=0:   0%|          | 0/1220 [00:00<?, ?it/s]                     /home/hanlv/miniconda3/envs/swift/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/hanlv/miniconda3/envs/swift/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "cnt_wrong=23: 100%|██████████| 1220/1220 [17:17<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"model\": \"Meta-Llama-3-8B-Instruct\",\n",
      "    \"train_test_split\": \"8:2\",\n",
      "    \"train_ratio\": \"1.0\",\n",
      "    \"train_loss\": 0.07930025,\n",
      "    \"lr\": \"9e-5\",\n",
      "    \"search_date\": \"2024-09-10\",\n",
      "    \"ACC\": 0.9811475409836066,\n",
      "    \"F1\": 0.9740204188767674,\n",
      "    \"Precision\": 0.985259172512592,\n",
      "    \"Recall\": 0.9639130434782609\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "search_dates = [\n",
    "\n",
    "    \"2024-09-10\",\n",
    "\n",
    "]\n",
    "\n",
    "ckpt_dir = \"/home/hanlv/workspace/code/research/infodemic/LLM/swift/examples/pytorch/llm/output/covmis/Llama-3-8B-Instruct/with_llama3_info/brave/data1-split=8:2-ratio=1.0/dora-r=8/lr=9e-5-20240626-01:48:13/checkpoint-609\"\n",
    "data_type = \"test\" # test, valid\n",
    "\n",
    "for search_date in search_dates:\n",
    "    if os.path.exists(f\"labels/{search_date}.json\"):\n",
    "        continue\n",
    "    data_dir = f\"/home/hanlv/workspace/code/research/infodemic/LLM/swift/examples/pytorch/llm/my_data/covmis/with_llama3_info/brave/train_valid_split/8:2/timeline_data1/test_data1_{search_date}.jsonl\"\n",
    "\n",
    "    labels, preds = get_labels_preds(ckpt_dir, data_dir, data_type)\n",
    "\n",
    "    with open(f'labels/{search_date}.json', 'w') as f:\n",
    "        json.dump(labels, f, indent=4)\n",
    "    with open(f'preds/{search_date}.json', 'w') as f:\n",
    "        json.dump(preds, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
